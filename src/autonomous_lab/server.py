#!/usr/bin/env python3
"""
MCP Feedback Enhanced ä¼ºæœå™¨ä¸»è¦æ¨¡çµ„

æ­¤æ¨¡çµ„æä¾› MCP (Model Context Protocol) çš„å¢å¼·å›é¥‹æ”¶é›†åŠŸèƒ½ï¼Œ
æ”¯æ´æ™ºèƒ½ç’°å¢ƒæª¢æ¸¬ï¼Œè‡ªå‹•ä½¿ç”¨ Web UI ä»‹é¢ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- MCP å·¥å…·å¯¦ç¾
- ä»‹é¢é¸æ“‡ï¼ˆWeb UIï¼‰
- ç’°å¢ƒæª¢æ¸¬ (SSH Remote, WSL, Local)
- åœ‹éš›åŒ–æ”¯æ´
- åœ–ç‰‡è™•ç†èˆ‡ä¸Šå‚³
- å‘½ä»¤åŸ·è¡Œèˆ‡çµæœå±•ç¤º
- å°ˆæ¡ˆç›®éŒ„ç®¡ç†

ä¸»è¦ MCP å·¥å…·ï¼š
- interactive_feedback: æ”¶é›†ç”¨æˆ¶äº’å‹•å›é¥‹
- get_system_info: ç²å–ç³»çµ±ç’°å¢ƒè³‡è¨Š

ä½œè€…: FÃ¡bio Ferreira (åŸä½œè€…)
å¢å¼·: Minidoracat (Web UI, åœ–ç‰‡æ”¯æ´, ç’°å¢ƒæª¢æ¸¬)
é‡æ§‹: æ¨¡å¡ŠåŒ–è¨­è¨ˆ
"""

import base64
import io
import json
import os
import sys
from typing import Annotated, Any

from fastmcp import FastMCP
from fastmcp.utilities.types import Image as MCPImage
from mcp.types import TextContent
from pydantic import Field

# å°å…¥çµ±ä¸€çš„èª¿è©¦åŠŸèƒ½
from .debug import server_debug_log as debug_log

# å°å…¥å¤šèªç³»æ”¯æ´
# å°å…¥éŒ¯èª¤è™•ç†æ¡†æ¶
from .utils.error_handler import ErrorHandler, ErrorType

# å°å…¥è³‡æºç®¡ç†å™¨
from .utils.resource_manager import create_temp_file


# ===== ç·¨ç¢¼åˆå§‹åŒ– =====
def init_encoding():
    """åˆå§‹åŒ–ç·¨ç¢¼è¨­ç½®ï¼Œç¢ºä¿æ­£ç¢ºè™•ç†ä¸­æ–‡å­—ç¬¦"""
    try:
        # Windows ç‰¹æ®Šè™•ç†
        if sys.platform == "win32":
            import msvcrt

            # è¨­ç½®ç‚ºäºŒé€²åˆ¶æ¨¡å¼
            msvcrt.setmode(sys.stdin.fileno(), os.O_BINARY)
            msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)

            # é‡æ–°åŒ…è£ç‚º UTF-8 æ–‡æœ¬æµï¼Œä¸¦ç¦ç”¨ç·©è¡
            # ä¿®å¾© union-attr éŒ¯èª¤ - å®‰å…¨ç²å– buffer æˆ– detach
            stdin_buffer = getattr(sys.stdin, "buffer", None)
            if stdin_buffer is None and hasattr(sys.stdin, "detach"):
                stdin_buffer = sys.stdin.detach()

            stdout_buffer = getattr(sys.stdout, "buffer", None)
            if stdout_buffer is None and hasattr(sys.stdout, "detach"):
                stdout_buffer = sys.stdout.detach()

            sys.stdin = io.TextIOWrapper(
                stdin_buffer, encoding="utf-8", errors="replace", newline=None
            )
            sys.stdout = io.TextIOWrapper(
                stdout_buffer,
                encoding="utf-8",
                errors="replace",
                newline="",
                write_through=True,  # é—œéµï¼šç¦ç”¨å¯«å…¥ç·©è¡
            )
        else:
            # é Windows ç³»çµ±çš„æ¨™æº–è¨­ç½®
            if hasattr(sys.stdout, "reconfigure"):
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stdin, "reconfigure"):
                sys.stdin.reconfigure(encoding="utf-8", errors="replace")

        # è¨­ç½® stderr ç·¨ç¢¼ï¼ˆç”¨æ–¼èª¿è©¦è¨Šæ¯ï¼‰
        if hasattr(sys.stderr, "reconfigure"):
            sys.stderr.reconfigure(encoding="utf-8", errors="replace")

        return True
    except Exception:
        # å¦‚æœç·¨ç¢¼è¨­ç½®å¤±æ•—ï¼Œå˜—è©¦åŸºæœ¬è¨­ç½®
        try:
            if hasattr(sys.stdout, "reconfigure"):
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stdin, "reconfigure"):
                sys.stdin.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stderr, "reconfigure"):
                sys.stderr.reconfigure(encoding="utf-8", errors="replace")
        except:
            pass
        return False


# åˆå§‹åŒ–ç·¨ç¢¼ï¼ˆåœ¨å°å…¥æ™‚å°±åŸ·è¡Œï¼‰
_encoding_initialized = init_encoding()

# ===== å¸¸æ•¸å®šç¾© =====
SERVER_NAME = "Autonomous Lab MCP"
SSH_ENV_VARS = ["SSH_CONNECTION", "SSH_CLIENT", "SSH_TTY"]
REMOTE_ENV_VARS = ["REMOTE_CONTAINERS", "CODESPACES"]


# åˆå§‹åŒ– MCP æœå‹™å™¨
from . import __version__


# ç¢ºä¿ log_level è¨­å®šç‚ºæ­£ç¢ºçš„å¤§å¯«æ ¼å¼
fastmcp_settings = {}

# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ä¸¦è¨­å®šæ­£ç¢ºçš„ log_level
env_log_level = os.getenv("FASTMCP_LOG_LEVEL", "").upper()
if env_log_level in ("DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"):
    fastmcp_settings["log_level"] = env_log_level
else:
    # é è¨­ä½¿ç”¨ INFO ç­‰ç´š
    fastmcp_settings["log_level"] = "INFO"

mcp: Any = FastMCP(SERVER_NAME)


# ===== å·¥å…·å‡½æ•¸ =====
def is_wsl_environment() -> bool:
    """
    æª¢æ¸¬æ˜¯å¦åœ¨ WSL (Windows Subsystem for Linux) ç’°å¢ƒä¸­é‹è¡Œ

    Returns:
        bool: True è¡¨ç¤º WSL ç’°å¢ƒï¼ŒFalse è¡¨ç¤ºå…¶ä»–ç’°å¢ƒ
    """
    try:
        # æª¢æŸ¥ /proc/version æ–‡ä»¶æ˜¯å¦åŒ…å« WSL æ¨™è­˜
        if os.path.exists("/proc/version"):
            with open("/proc/version") as f:
                version_info = f.read().lower()
                if "microsoft" in version_info or "wsl" in version_info:
                    debug_log("åµæ¸¬åˆ° WSL ç’°å¢ƒï¼ˆé€šé /proc/versionï¼‰")
                    return True

        # æª¢æŸ¥ WSL ç›¸é—œç’°å¢ƒè®Šæ•¸
        wsl_env_vars = ["WSL_DISTRO_NAME", "WSL_INTEROP", "WSLENV"]
        for env_var in wsl_env_vars:
            if os.getenv(env_var):
                debug_log(f"åµæ¸¬åˆ° WSL ç’°å¢ƒè®Šæ•¸: {env_var}")
                return True

        # æª¢æŸ¥æ˜¯å¦å­˜åœ¨ WSL ç‰¹æœ‰çš„è·¯å¾‘
        wsl_paths = ["/mnt/c", "/mnt/d", "/proc/sys/fs/binfmt_misc/WSLInterop"]
        for path in wsl_paths:
            if os.path.exists(path):
                debug_log(f"åµæ¸¬åˆ° WSL ç‰¹æœ‰è·¯å¾‘: {path}")
                return True

    except Exception as e:
        debug_log(f"WSL æª¢æ¸¬éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

    return False


def is_remote_environment() -> bool:
    """
    æª¢æ¸¬æ˜¯å¦åœ¨é ç«¯ç’°å¢ƒä¸­é‹è¡Œ

    Returns:
        bool: True è¡¨ç¤ºé ç«¯ç’°å¢ƒï¼ŒFalse è¡¨ç¤ºæœ¬åœ°ç’°å¢ƒ
    """
    # WSL ä¸æ‡‰è¢«è¦–ç‚ºé ç«¯ç’°å¢ƒï¼Œå› ç‚ºå®ƒå¯ä»¥è¨ªå• Windows ç€è¦½å™¨
    if is_wsl_environment():
        debug_log("WSL ç’°å¢ƒä¸è¢«è¦–ç‚ºé ç«¯ç’°å¢ƒ")
        return False

    # æª¢æŸ¥ SSH é€£ç·šæŒ‡æ¨™
    for env_var in SSH_ENV_VARS:
        if os.getenv(env_var):
            debug_log(f"åµæ¸¬åˆ° SSH ç’°å¢ƒè®Šæ•¸: {env_var}")
            return True

    # æª¢æŸ¥é ç«¯é–‹ç™¼ç’°å¢ƒ
    for env_var in REMOTE_ENV_VARS:
        if os.getenv(env_var):
            debug_log(f"åµæ¸¬åˆ°é ç«¯é–‹ç™¼ç’°å¢ƒ: {env_var}")
            return True

    # æª¢æŸ¥ Docker å®¹å™¨
    if os.path.exists("/.dockerenv"):
        debug_log("åµæ¸¬åˆ° Docker å®¹å™¨ç’°å¢ƒ")
        return True

    # Windows é ç«¯æ¡Œé¢æª¢æŸ¥
    if sys.platform == "win32":
        session_name = os.getenv("SESSIONNAME", "")
        if session_name and "RDP" in session_name:
            debug_log(f"åµæ¸¬åˆ° Windows é ç«¯æ¡Œé¢: {session_name}")
            return True

    # Linux ç„¡é¡¯ç¤ºç’°å¢ƒæª¢æŸ¥ï¼ˆä½†æ’é™¤ WSLï¼‰
    if (
        sys.platform.startswith("linux")
        and not os.getenv("DISPLAY")
        and not is_wsl_environment()
    ):
        debug_log("åµæ¸¬åˆ° Linux ç„¡é¡¯ç¤ºç’°å¢ƒ")
        return True

    return False


def save_feedback_to_file(feedback_data: dict, file_path: str | None = None) -> str:
    """
    å°‡å›é¥‹è³‡æ–™å„²å­˜åˆ° JSON æ–‡ä»¶

    Args:
        feedback_data: å›é¥‹è³‡æ–™å­—å…¸
        file_path: å„²å­˜è·¯å¾‘ï¼Œè‹¥ç‚º None å‰‡è‡ªå‹•ç”¢ç”Ÿè‡¨æ™‚æ–‡ä»¶

    Returns:
        str: å„²å­˜çš„æ–‡ä»¶è·¯å¾‘
    """
    if file_path is None:
        # ä½¿ç”¨è³‡æºç®¡ç†å™¨å‰µå»ºè‡¨æ™‚æ–‡ä»¶
        file_path = create_temp_file(suffix=".json", prefix="feedback_")

    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

    # è¤‡è£½æ•¸æ“šä»¥é¿å…ä¿®æ”¹åŸå§‹æ•¸æ“š
    json_data = feedback_data.copy()

    # è™•ç†åœ–ç‰‡æ•¸æ“šï¼šå°‡ bytes è½‰æ›ç‚º base64 å­—ç¬¦ä¸²ä»¥ä¾¿ JSON åºåˆ—åŒ–
    if "images" in json_data and isinstance(json_data["images"], list):
        processed_images = []
        for img in json_data["images"]:
            if isinstance(img, dict) and "data" in img:
                processed_img = img.copy()
                # å¦‚æœ data æ˜¯ bytesï¼Œè½‰æ›ç‚º base64 å­—ç¬¦ä¸²
                if isinstance(img["data"], bytes):
                    processed_img["data"] = base64.b64encode(img["data"]).decode(
                        "utf-8"
                    )
                    processed_img["data_type"] = "base64"
                processed_images.append(processed_img)
            else:
                processed_images.append(img)
        json_data["images"] = processed_images

    # å„²å­˜è³‡æ–™
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    debug_log(f"å›é¥‹è³‡æ–™å·²å„²å­˜è‡³: {file_path}")
    return file_path


def create_feedback_text(feedback_data: dict) -> str:
    """
    å»ºç«‹æ ¼å¼åŒ–çš„å›é¥‹æ–‡å­—

    Args:
        feedback_data: å›é¥‹è³‡æ–™å­—å…¸

    Returns:
        str: æ ¼å¼åŒ–å¾Œçš„å›é¥‹æ–‡å­—
    """
    text_parts = []

    # åŸºæœ¬å›é¥‹å…§å®¹
    if feedback_data.get("interactive_feedback"):
        text_parts.append(f"=== ç”¨æˆ¶å›é¥‹ ===\n{feedback_data['interactive_feedback']}")

    # å‘½ä»¤åŸ·è¡Œæ—¥èªŒ
    if feedback_data.get("command_logs"):
        text_parts.append(f"=== å‘½ä»¤åŸ·è¡Œæ—¥èªŒ ===\n{feedback_data['command_logs']}")

    # åœ–ç‰‡é™„ä»¶æ¦‚è¦
    if feedback_data.get("images"):
        images = feedback_data["images"]
        text_parts.append(f"=== åœ–ç‰‡é™„ä»¶æ¦‚è¦ ===\nç”¨æˆ¶æä¾›äº† {len(images)} å¼µåœ–ç‰‡ï¼š")

        for i, img in enumerate(images, 1):
            size = img.get("size", 0)
            name = img.get("name", "unknown")

            # æ™ºèƒ½å–®ä½é¡¯ç¤º
            if size < 1024:
                size_str = f"{size} B"
            elif size < 1024 * 1024:
                size_kb = size / 1024
                size_str = f"{size_kb:.1f} KB"
            else:
                size_mb = size / (1024 * 1024)
                size_str = f"{size_mb:.1f} MB"

            img_info = f"  {i}. {name} ({size_str})"

            # ç‚ºæé«˜å…¼å®¹æ€§ï¼Œæ·»åŠ  base64 é è¦½ä¿¡æ¯
            if img.get("data"):
                try:
                    if isinstance(img["data"], bytes):
                        img_base64 = base64.b64encode(img["data"]).decode("utf-8")
                    elif isinstance(img["data"], str):
                        img_base64 = img["data"]
                    else:
                        img_base64 = None

                    if img_base64:
                        # åªé¡¯ç¤ºå‰50å€‹å­—ç¬¦çš„é è¦½
                        preview = (
                            img_base64[:50] + "..."
                            if len(img_base64) > 50
                            else img_base64
                        )
                        img_info += f"\n     Base64 é è¦½: {preview}"
                        img_info += f"\n     å®Œæ•´ Base64 é•·åº¦: {len(img_base64)} å­—ç¬¦"

                        # å¦‚æœ AI åŠ©æ‰‹ä¸æ”¯æ´ MCP åœ–ç‰‡ï¼Œå¯ä»¥æä¾›å®Œæ•´ base64
                        debug_log(f"åœ–ç‰‡ {i} Base64 å·²æº–å‚™ï¼Œé•·åº¦: {len(img_base64)}")

                        # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨ Base64 è©³ç´°æ¨¡å¼ï¼ˆå¾ UI è¨­å®šä¸­ç²å–ï¼‰
                        include_full_base64 = feedback_data.get("settings", {}).get(
                            "enable_base64_detail", False
                        )

                        if include_full_base64:
                            # æ ¹æ“šæª”æ¡ˆåæ¨æ–· MIME é¡å‹
                            file_name = img.get("name", "image.png")
                            if file_name.lower().endswith((".jpg", ".jpeg")):
                                mime_type = "image/jpeg"
                            elif file_name.lower().endswith(".gif"):
                                mime_type = "image/gif"
                            elif file_name.lower().endswith(".webp"):
                                mime_type = "image/webp"
                            else:
                                mime_type = "image/png"

                            img_info += f"\n     å®Œæ•´ Base64: data:{mime_type};base64,{img_base64}"

                except Exception as e:
                    debug_log(f"åœ–ç‰‡ {i} Base64 è™•ç†å¤±æ•—: {e}")

            text_parts.append(img_info)

        # æ·»åŠ å…¼å®¹æ€§èªªæ˜
        text_parts.append(
            "\nğŸ’¡ æ³¨æ„ï¼šå¦‚æœ AI åŠ©æ‰‹ç„¡æ³•é¡¯ç¤ºåœ–ç‰‡ï¼Œåœ–ç‰‡æ•¸æ“šå·²åŒ…å«åœ¨ä¸Šè¿° Base64 ä¿¡æ¯ä¸­ã€‚"
        )

    return "\n\n".join(text_parts) if text_parts else "ç”¨æˆ¶æœªæä¾›ä»»ä½•å›é¥‹å…§å®¹ã€‚"


def process_images(images_data: list[dict]) -> list[MCPImage]:
    """
    è™•ç†åœ–ç‰‡è³‡æ–™ï¼Œè½‰æ›ç‚º MCP åœ–ç‰‡å°è±¡

    Args:
        images_data: åœ–ç‰‡è³‡æ–™åˆ—è¡¨

    Returns:
        List[MCPImage]: MCP åœ–ç‰‡å°è±¡åˆ—è¡¨
    """
    mcp_images = []

    for i, img in enumerate(images_data, 1):
        try:
            if not img.get("data"):
                debug_log(f"åœ–ç‰‡ {i} æ²’æœ‰è³‡æ–™ï¼Œè·³é")
                continue

            # æª¢æŸ¥æ•¸æ“šé¡å‹ä¸¦ç›¸æ‡‰è™•ç†
            if isinstance(img["data"], bytes):
                # å¦‚æœæ˜¯åŸå§‹ bytes æ•¸æ“šï¼Œç›´æ¥ä½¿ç”¨
                image_bytes = img["data"]
                debug_log(
                    f"åœ–ç‰‡ {i} ä½¿ç”¨åŸå§‹ bytes æ•¸æ“šï¼Œå¤§å°: {len(image_bytes)} bytes"
                )
            elif isinstance(img["data"], str):
                # å¦‚æœæ˜¯ base64 å­—ç¬¦ä¸²ï¼Œé€²è¡Œè§£ç¢¼
                image_bytes = base64.b64decode(img["data"])
                debug_log(f"åœ–ç‰‡ {i} å¾ base64 è§£ç¢¼ï¼Œå¤§å°: {len(image_bytes)} bytes")
            else:
                debug_log(f"åœ–ç‰‡ {i} æ•¸æ“šé¡å‹ä¸æ”¯æ´: {type(img['data'])}")
                continue

            if len(image_bytes) == 0:
                debug_log(f"åœ–ç‰‡ {i} æ•¸æ“šç‚ºç©ºï¼Œè·³é")
                continue

            # æ ¹æ“šæ–‡ä»¶åæ¨æ–·æ ¼å¼
            file_name = img.get("name", "image.png")
            if file_name.lower().endswith((".jpg", ".jpeg")):
                image_format = "jpeg"
            elif file_name.lower().endswith(".gif"):
                image_format = "gif"
            else:
                image_format = "png"  # é»˜èªä½¿ç”¨ PNG

            # å‰µå»º MCPImage å°è±¡
            mcp_image = MCPImage(data=image_bytes, format=image_format)
            mcp_images.append(mcp_image)

            debug_log(f"åœ–ç‰‡ {i} ({file_name}) è™•ç†æˆåŠŸï¼Œæ ¼å¼: {image_format}")

        except Exception as e:
            # ä½¿ç”¨çµ±ä¸€éŒ¯èª¤è™•ç†ï¼ˆä¸å½±éŸ¿ JSON RPCï¼‰
            error_id = ErrorHandler.log_error_with_context(
                e,
                context={"operation": "åœ–ç‰‡è™•ç†", "image_index": i},
                error_type=ErrorType.FILE_IO,
            )
            debug_log(f"åœ–ç‰‡ {i} è™•ç†å¤±æ•— [éŒ¯èª¤ID: {error_id}]: {e}")

    debug_log(f"å…±è™•ç† {len(mcp_images)} å¼µåœ–ç‰‡")
    return mcp_images


# ===== MCP å·¥å…·å®šç¾© =====
@mcp.tool()
async def lab_meeting(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
    summary: Annotated[
        str, Field(description="Summary of AI work completed for user review")
    ] = "Completed the requested task.",
    timeout: Annotated[int, Field(description="Pause duration in seconds (default: 15)")] = 15,
) -> str:
    """Pause briefly between lab turns to show the monitoring UI and collect optional feedback.

    This opens/refreshes the game-style lab monitoring UI in the browser.
    It pauses for a short period (default 15s) during which the user can
    optionally type feedback through the lab UI's feedback panel.

    Call this after EVERY autolab_record call during normal lab turns.

    LOOP PROTOCOL (normal turns):
      autolab_next -> (act as role) -> autolab_record -> lab_meeting -> autolab_next -> ...

    DO NOT use this for editorial submission waits. Use autolab_editorial instead.

    After this tool returns, ALWAYS call autolab_next to continue.
    NEVER stop the loop.

    Args:
        project_directory: Project directory path
        summary: Summary of AI work completed (displayed in the status bar)
        timeout: How long to pause for user feedback (seconds, default 15)

    Returns:
        str: Any user feedback collected, or a message to continue the loop
    """
    import asyncio

    try:
        if not os.path.exists(project_directory):
            project_directory = os.getcwd()
        project_directory = os.path.abspath(project_directory)

        # Ensure the game-style lab UI is open
        _ensure_lab_ui(project_directory)

        # Clear any stale feedback from the previous turn
        try:
            from .lab.state import load_state, save_state
            state = load_state(project_directory)
            old_feedback = state.get("user_feedback", "")
            if old_feedback:
                state["user_feedback"] = ""
                save_state(project_directory, state)
        except Exception:
            old_feedback = ""

        debug_log(f"lab_meeting: pausing {timeout}s for optional feedback")

        # Poll for user feedback during the pause window
        poll_interval = 3  # seconds
        elapsed = 0
        user_feedback = ""
        while elapsed < timeout:
            await asyncio.sleep(min(poll_interval, timeout - elapsed))
            elapsed += poll_interval
            try:
                state = load_state(project_directory)
                fb = state.get("user_feedback", "").strip()
                if fb:
                    user_feedback = fb
                    # Clear the feedback so the next turn starts fresh
                    state["user_feedback"] = ""
                    save_state(project_directory, state)
                    break  # Got feedback, no need to wait longer
            except Exception:
                continue

        if user_feedback:
            debug_log(f"lab_meeting: got user feedback: {user_feedback[:80]}")
            return (
                f"User feedback received:\n{user_feedback}\n\n"
                f"Incorporate this feedback into the next turn.\n"
                f"Continue the lab loop: call autolab_next now."
            )

        debug_log("lab_meeting: no feedback, continuing")
        return "No user feedback. Continue the lab loop: call autolab_next now."

    except Exception as e:
        debug_log(f"lab_meeting error: {e}")
        return f"Lab meeting error (non-fatal): {e}\n\nContinue the lab loop: call autolab_next now."


@mcp.tool()
def get_system_info() -> str:
    """
    ç²å–ç³»çµ±ç’°å¢ƒè³‡è¨Š

    Returns:
        str: JSON æ ¼å¼çš„ç³»çµ±è³‡è¨Š
    """
    is_remote = is_remote_environment()
    is_wsl = is_wsl_environment()

    system_info = {
        "å¹³å°": sys.platform,
        "Python ç‰ˆæœ¬": sys.version.split()[0],
        "WSL ç’°å¢ƒ": is_wsl,
        "é ç«¯ç’°å¢ƒ": is_remote,
        "ä»‹é¢é¡å‹": "Web UI",
        "ç’°å¢ƒè®Šæ•¸": {
            "SSH_CONNECTION": os.getenv("SSH_CONNECTION"),
            "SSH_CLIENT": os.getenv("SSH_CLIENT"),
            "DISPLAY": os.getenv("DISPLAY"),
            "VSCODE_INJECTION": os.getenv("VSCODE_INJECTION"),
            "SESSIONNAME": os.getenv("SESSIONNAME"),
            "WSL_DISTRO_NAME": os.getenv("WSL_DISTRO_NAME"),
            "WSL_INTEROP": os.getenv("WSL_INTEROP"),
            "WSLENV": os.getenv("WSLENV"),
        },
    }

    return json.dumps(system_info, ensure_ascii=False, indent=2)


# ===== Autonomous Lab MCP Tools =====

# Mandatory loop instruction for NORMAL turns (PI/Trainee/Reviewer)
_LOOP_INSTRUCTION = (
    "\n\n---\n"
    "MANDATORY NEXT STEPS (you MUST do both, in order):\n"
    "1. Call lab_meeting NOW with a brief summary of what just happened.\n"
    "2. After lab_meeting returns, call autolab_next to continue the lab session.\n"
    "NEVER stop or end your turn. The lab loop runs until the user explicitly says 'stop'."
)

# Editorial instruction for submission / waiting-for-editor scenarios
_EDITORIAL_INSTRUCTION = (
    "\n\n---\n"
    "MANDATORY NEXT STEP:\n"
    "Call autolab_editorial NOW. It will wait (with no timeout) for the Editor's action.\n"
    "When it returns, follow its instructions to continue.\n"
    "NEVER stop or end your turn."
)


_lab_browser_opened: set[str] = set()  # tracks which project dirs already have a browser tab


def _ensure_lab_ui(project_directory: str, force_open: bool = False) -> None:
    """Ensure the monitoring web UI is running and points to this project.

    Only opens a new browser tab ONCE per project per MCP session.
    Subsequent calls just update lab_project_dir without spawning tabs.
    Set force_open=True to open the browser even if already opened (e.g., autolab_resume).
    """
    try:
        import urllib.parse

        from .web.main import get_web_ui_manager

        manager = get_web_ui_manager()
        manager.lab_project_dir = project_directory

        # Start the server if not running
        if manager.server_thread is None or not manager.server_thread.is_alive():
            import time
            manager.start_server()
            time.sleep(1)

        # Only open browser once per project (or on force_open)
        if project_directory not in _lab_browser_opened or force_open:
            encoded = urllib.parse.quote(project_directory, safe="")
            lab_url = f"{manager.get_server_url()}/lab?project={encoded}"
            manager.open_browser(lab_url)
            _lab_browser_opened.add(project_directory)
            debug_log(f"Lab UI browser opened for {project_directory}")
        else:
            debug_log("Lab UI already open, skipping browser launch")
    except Exception as e:
        debug_log(f"Lab UI ensure failed (non-fatal): {e}")


@mcp.tool()
async def autolab_init(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
    idea: Annotated[str, Field(description="Research project idea and context")] = "",
    title: Annotated[str, Field(description="Paper title")] = "TITLE",
    force_new: Annotated[bool, Field(description="Force fresh start even if project exists")] = False,
) -> str:
    """Initialize an Autonomous Lab project.

    Creates .autolab/ directory with state, profiles, and meeting log.
    Creates paper/ directory with LaTeX template.
    Creates working directories: data/, scripts/, figures/, results/.
    Opens the game-style monitoring UI in the browser.

    If an existing project is found, returns a warning and suggests
    calling autolab_resume instead. Set force_new=True to overwrite.

    Args:
        project_directory: Path to the project directory
        idea: The research idea, context, and any preliminary data description
        title: Working title for the paper
        force_new: If True, overwrite existing project. Default False.

    Returns:
        str: Confirmation message with instructions to call autolab_next
    """
    from .lab.latex_template import create_paper_structure
    from .lab.state import init_project

    project_directory = os.path.abspath(project_directory)

    if not idea.strip():
        return "ERROR: 'idea' parameter is required. Provide the research idea and context."

    # Check if an existing project is present â€” suggest resume instead of overwriting
    autolab_dir = os.path.join(project_directory, ".autolab")
    state_file = os.path.join(autolab_dir, "state.json")
    if os.path.exists(state_file) and not force_new:
        try:
            with open(state_file, "r") as f:
                existing = json.load(f)
            iteration = existing.get("iteration", 0)
            status = existing.get("status", "active")
            role = existing.get("next_role", "pi")
            progress = existing.get("progress", 0)
            return (
                f"WARNING: An existing Autonomous Lab project was found in {project_directory}\n\n"
                f"  Iteration: {iteration}, Status: {status}, Next role: {role}, Progress: {progress}%\n\n"
                f"To RESUME the existing project, call autolab_resume(project_directory='{project_directory}').\n"
                f"To START FRESH (this will overwrite the existing project), call autolab_init again "
                f"with force_new=True.\n\n"
                f"Do NOT reinitialize unless the user explicitly asks for a fresh start."
            )
        except Exception:
            pass  # Corrupted state â€” safe to reinitialize

    try:
        state = init_project(project_directory, idea)
        create_paper_structure(project_directory, title)

        # Start the monitoring web UI (non-blocking)
        lab_url = ""
        try:
            _ensure_lab_ui(project_directory)
            from .web.main import get_web_ui_manager
            lab_url = f"{get_web_ui_manager().get_server_url()}/lab"
        except Exception as e:
            debug_log(f"Failed to start lab UI: {e}")
            lab_url = "(failed to start)"

        return (
            f"Autonomous Lab initialized in {project_directory}\n\n"
            f"Created:\n"
            f"  .autolab/ -- state, profiles, meeting log\n"
            f"  paper/ -- LaTeX template ({title})\n"
            f"  scripts/, figures/, results/ -- working directories\n\n"
            f"State: iteration={state['iteration']}, next_role={state['next_role']}\n"
            f"Monitoring UI: {lab_url}"
            + _LOOP_INSTRUCTION
        )
    except Exception as e:
        return f"ERROR initializing project: {e}"


@mcp.tool()
async def autolab_resume(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
) -> str:
    """Resume an interrupted Autonomous Lab session.

    Call this when the AI agent was disconnected, the process crashed,
    or a new conversation is started for an existing project.

    This tool:
    1. Loads the full saved state (iteration, role, status, progress, editorial)
    2. Reads the last several meeting log entries to reconstruct context
    3. Lists all existing files (scripts, figures, paper sections)
    4. Opens the monitoring UI
    5. Returns a comprehensive briefing so the AI can continue seamlessly

    After this returns, follow the MANDATORY NEXT STEPS to re-enter the loop.

    Args:
        project_directory: Path to the project directory with .autolab/

    Returns:
        str: Full recovery briefing + instructions to continue
    """
    from .lab.state import (
        get_editorial,
        get_meeting_summaries,
        get_paper_progress,
        get_recent_meetings,
        load_idea,
        load_state,
        scan_project_files,
    )

    project_directory = os.path.abspath(project_directory)

    # Verify the project exists
    autolab_dir = os.path.join(project_directory, ".autolab")
    if not os.path.exists(autolab_dir):
        return (
            f"ERROR: No Autonomous Lab project found in {project_directory}\n"
            f"(.autolab/ directory does not exist)\n\n"
            f"To start a new project, call autolab_init instead."
        )

    try:
        state = load_state(project_directory)
    except Exception as e:
        return f"ERROR loading state: {e}"

    # Open the monitoring UI (force open since this is an explicit resume)
    _ensure_lab_ui(project_directory, force_open=True)

    # Gather context
    idea = load_idea(project_directory)
    iteration = state.get("iteration", 0)
    role = state.get("next_role", "pi")
    status = state.get("status", "active")
    progress = state.get("progress", 0)
    experts = state.get("experts", [])
    editorial = state.get("editorial", {})
    created_at = state.get("created_at", "unknown")

    # Recent meeting history (more than normal â€” 5 entries for recovery context)
    recent_meetings = get_recent_meetings(project_directory, n=5)
    summaries = get_meeting_summaries(project_directory)
    file_listings = scan_project_files(project_directory)
    paper_progress = get_paper_progress(project_directory)

    # Build file listing summary
    file_summary_parts = []
    for category, files in file_listings.items():
        if files:
            file_summary_parts.append(f"  {category}/: {len(files)} file(s) â€” {', '.join(files[:5])}")
    file_summary = "\n".join(file_summary_parts) if file_summary_parts else "  (no files yet)"

    # Build paper progress summary
    paper_parts = []
    for section, info in paper_progress.items():
        word_count = info.get("words", 0)
        paper_parts.append(f"  {section}: {word_count} words" if word_count > 0 else f"  {section}: not written")
    paper_summary = "\n".join(paper_parts) if paper_parts else "  (no paper sections yet)"

    # Build editorial status
    editorial_phase = editorial.get("phase", "none")
    editorial_summary = ""
    if editorial_phase != "none":
        editorial_summary = (
            f"\n\nEditorial Status:\n"
            f"  Phase: {editorial_phase}\n"
            f"  Round: {editorial.get('round', 1)}\n"
            f"  Decision: {editorial.get('decision', 'pending')}\n"
        )
        if editorial.get("reviewers"):
            rev_names = [r.get("name", "?") for r in editorial["reviewers"]]
            editorial_summary += f"  Reviewers: {', '.join(rev_names)}\n"
        if editorial.get("reviews"):
            done_reviews = list(editorial["reviews"].keys())
            editorial_summary += f"  Reviews completed: {', '.join(done_reviews)}\n"

    # Expert consultants
    expert_summary = ""
    if experts:
        expert_summary = "\n\nActive Consultants:\n"
        for ex in experts:
            expert_summary += f"  - {ex.get('name', '?')} ({ex.get('role', '?')})\n"

    # Determine the right instruction based on current state
    if status in ("submitted_to_editor", "reviews_complete"):
        next_instruction = _EDITORIAL_INSTRUCTION
    else:
        next_instruction = _LOOP_INSTRUCTION

    briefing = (
        f"[AUTOLAB] SESSION RECOVERED â€” Resuming from saved state\n"
        f"{'=' * 60}\n\n"
        f"Project: {project_directory}\n"
        f"Started: {created_at}\n"
        f"Iteration: {iteration}\n"
        f"Status: {status}\n"
        f"Progress: {progress}%\n"
        f"Next role: {role}\n"
        f"{editorial_summary}"
        f"{expert_summary}\n\n"
        f"Project Idea:\n{idea[:500]}{'...' if len(idea) > 500 else ''}\n\n"
        f"Files:\n{file_summary}\n\n"
        f"Paper Progress:\n{paper_summary}\n\n"
        f"Recent Meeting History (last 5 turns):\n"
        f"{'â”€' * 40}\n"
        f"{recent_meetings if recent_meetings.strip() else '(no meeting history yet)'}\n"
        f"{'â”€' * 40}\n\n"
        f"Meeting Summaries:\n"
        f"{summaries[:1000] if summaries.strip() else '(none yet)'}\n\n"
        f"{'=' * 60}\n"
        f"Recovery complete. The monitoring UI should be open.\n"
        f"Continue the lab from where it left off."
        f"{next_instruction}"
    )

    return briefing


@mcp.tool()
async def autolab_next(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
) -> str:
    """Get the next role prompt for the Autonomous Lab session.

    Reads the current state to determine whose turn it is (PI or Trainee),
    assembles context (idea, profiles, meeting history, file listings),
    and returns a detailed role prompt.

    The AI should then follow the returned prompt exactly, acting as
    the specified role (PI or Trainee).

    MANDATORY LOOP PROTOCOL â€” the lab NEVER stops:

    Normal turns:
      autolab_next -> (act as role) -> autolab_record -> lab_meeting -> autolab_next -> ...

    Editorial submission:
      autolab_record(status='ready_for_review') -> autolab_editorial (blocks) -> follow instructions

    After EVERY autolab_record call, follow the MANDATORY NEXT STEPS in the return message.
    NEVER stop. NEVER end your turn.

    Args:
        project_directory: Path to the project directory

    Returns:
        str: The role prompt to follow (MUST act on it and continue the loop)
    """
    from .lab.prompts import (
        build_pi_prompt,
        build_reviewer_prompt,
        build_revision_prompt,
        build_submission_prompt,
        build_trainee_prompt,
    )
    from .lab.state import (
        get_editorial,
        get_meeting_summaries,
        get_paper_progress,
        get_recent_meetings,
        load_idea,
        load_profile,
        load_state,
        scan_project_files,
    )

    project_directory = os.path.abspath(project_directory)

    # Ensure the monitoring UI is visible on every new turn
    _ensure_lab_ui(project_directory)

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    idea = load_idea(project_directory)
    role = state["next_role"]
    iteration = state["iteration"]
    user_feedback = state.get("user_feedback", "")
    meeting_history = get_recent_meetings(project_directory, n=3)
    summaries = get_meeting_summaries(project_directory)
    file_listings = scan_project_files(project_directory)
    editorial = state.get("editorial", {})

    # --- Editorial workflow routing ---

    # 1. PI is preparing submission (ready_for_review)
    if state.get("status") == "ready_for_review" and editorial.get("phase", "none") == "none":
        paper_progress = get_paper_progress(project_directory)
        prompt = build_submission_prompt(paper_progress, file_listings, meeting_history)
        return (
            f"[AUTOLAB] SUBMISSION MODE -- Iteration {iteration}\n\n"
            f"{prompt}\n\n"
            f"After completing your submission, call autolab_record, then follow the MANDATORY NEXT STEPS."
        )

    # 2. Waiting for editor (submitted / reviews_complete)
    if state.get("status") in ("submitted_to_editor", "reviews_complete"):
        phase = editorial.get("phase", "none")
        if phase == "submitted":
            return (
                f"[AUTOLAB] AWAITING EDITOR -- Iteration {iteration}\n\n"
                f"The manuscript has been submitted to the Editor.\n"
                f"The Editor (user) will decide: Desk Reject or Invite Reviewers."
                + _EDITORIAL_INSTRUCTION
            )
        if phase == "reviews_complete":
            return (
                f"[AUTOLAB] AWAITING EDITORIAL DECISION -- Iteration {iteration}\n\n"
                f"All reviewer reports are in.\n"
                f"The Editor (user) will decide: Accept / Minor Revision / Major Revision / Reject."
                + _EDITORIAL_INSTRUCTION
            )
        # Catch-all for transitional editorial states
        return (
            f"[AUTOLAB] EDITORIAL IN PROGRESS -- Iteration {iteration}\n\n"
            f"Editorial phase: {phase}, Status: {state.get('status')}\n"
            f"The editorial workflow is in progress."
            + _EDITORIAL_INSTRUCTION
        )

    # 3. Reviewer turn
    if role.startswith("reviewer_"):
        paper_progress = get_paper_progress(project_directory)
        reviewers = editorial.get("reviewers", [])
        reviewer = next((r for r in reviewers if r["id"] == role), None)
        if not reviewer:
            return f"ERROR: Reviewer {role} not found in editorial state."

        cover_letter = editorial.get("cover_letter", "")
        round_num = editorial.get("round", 1)
        prompt = build_reviewer_prompt(
            reviewer=reviewer,
            paper_progress=paper_progress,
            file_listings=file_listings,
            cover_letter=cover_letter,
            round_number=round_num,
        )
        return (
            f"[AUTOLAB] REVIEWER TURN -- {reviewer['name']} ({reviewer['role']})\n"
            f"Iteration: {iteration}, Round: {round_num}\n\n"
            f"{prompt}\n\n"
            f"After completing your review, call autolab_record, then follow the MANDATORY NEXT STEPS."
        )

    # 4. Paper accepted â€” PI wraps up
    if state.get("status") == "accepted":
        return (
            f"[AUTOLAB] PAPER ACCEPTED -- Iteration {iteration}\n\n"
            f"Congratulations! The manuscript has been accepted by the Editor.\n"
            f"As PI, write a brief acknowledgment, finalize the paper, and celebrate.\n"
            f"If there is nothing left to do, inform the user the project is complete."
            + _LOOP_INSTRUCTION
        )

    # 5. PI handling revision
    if state.get("status") in ("revision_requested", "rejected"):
        profile = load_profile(project_directory, "pi")
        reviews = editorial.get("reviews", {})
        decision = editorial.get("decision", "")
        decision_fb = editorial.get("decision_feedback", "")
        round_num = editorial.get("round", 1)
        prompt = build_revision_prompt(
            idea=idea,
            profile=profile,
            reviews=reviews,
            editorial_decision=decision,
            editorial_feedback=decision_fb,
            meeting_history=meeting_history,
            file_listings=file_listings,
            round_number=round_num,
        )
        return (
            f"[AUTOLAB] REVISION MODE -- Round {round_num}\n"
            f"Decision: {decision.upper().replace('_', ' ')}\n"
            f"Iteration: {iteration}\n\n"
            f"{prompt}\n\n"
            f"After completing revision actions, call autolab_record, then follow the MANDATORY NEXT STEPS."
        )

    # --- Normal lab workflow ---
    profile = load_profile(project_directory, role)

    if role == "pi":
        prompt = build_pi_prompt(
            idea=idea,
            profile=profile,
            meeting_history=meeting_history,
            summaries=summaries,
            file_listings=file_listings,
            user_feedback=user_feedback,
            iteration=iteration,
        )
    else:
        prompt = build_trainee_prompt(
            idea=idea,
            profile=profile,
            meeting_history=meeting_history,
            summaries=summaries,
            file_listings=file_listings,
            user_feedback=user_feedback,
            iteration=iteration,
        )

    return (
        f"[AUTOLAB] {role.upper()} TURN -- Iteration {iteration}\n\n"
        f"{prompt}\n\n"
        f"After completing your actions as {role.upper()}, call autolab_record, then follow the MANDATORY NEXT STEPS."
    )


@mcp.tool()
async def autolab_record(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
    role: Annotated[str, Field(description="Role that just acted: 'pi' or 'trainee'")] = "",
    summary: Annotated[str, Field(description="Brief summary of what was done this turn")] = "",
    content: Annotated[str, Field(description="Full content of the turn (review, agenda, results, etc.)")] = "",
    status: Annotated[str, Field(description="Status: 'continue' or 'ready_for_review'")] = "continue",
    progress: Annotated[int, Field(description="PI overall project progress 0-100 (set only by PI)")] = -1,
) -> str:
    """Record a completed turn. Non-blocking â€” updates state and returns immediately.

    The monitoring UI (opened by autolab_init) auto-refreshes to show progress.
    User feedback is optional and picked up in the next autolab_next call.

    This tool:
    1. Saves the turn to meeting_log.md
    2. Updates state.json (advance iteration, flip role, store status)
    3. Compresses old meetings every 6 turns

    Call this AFTER completing your role actions (as PI or Trainee).

    CRITICAL LOOP PROTOCOL â€” you MUST follow the MANDATORY NEXT STEPS in the return message:
      - Normal turns: call lab_meeting, then autolab_next.
      - Submission/editorial: call autolab_editorial (blocks until editor acts).
      - NEVER stop. NEVER end your turn. The loop runs indefinitely.

    Args:
        project_directory: Path to the project directory
        role: Which role just completed: 'pi' or 'trainee'
        summary: Brief summary of actions taken
        content: Full structured content of the turn
        status: 'continue' to keep iterating, 'ready_for_review' when paper is done
        progress: PI's assessment of overall project progress 0-100 (only PI should set this)

    Returns:
        str: Instructions to continue the loop (MUST follow them)
    """
    from .lab.state import (
        append_meeting_log,
        compress_old_meetings,
        load_state,
        record_review,
        save_state,
        submit_manuscript,
        COMPRESS_EVERY,
    )

    project_directory = os.path.abspath(project_directory)

    # Validate role â€” allow pi, trainee, and reviewer_N
    valid_roles = role in ("pi", "trainee") or role.startswith("reviewer_")
    if not valid_roles:
        return "ERROR: role must be 'pi', 'trainee', or 'reviewer_N'"

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    iteration = state["iteration"]

    # 1. Append to meeting log
    append_meeting_log(project_directory, role, iteration, summary, content)

    # --- Reviewer turn handling ---
    if role.startswith("reviewer_"):
        # Extract recommendation from content (flexible matching)
        import re
        rec_match = re.search(
            r"(?:RECOMMENDATION|OVERALL\s+RECOMMENDATION|DECISION)[:\s]*([A-Za-z][A-Za-z _]*)",
            content, re.IGNORECASE,
        )
        conf_match = re.search(r"CONFIDENCE[^:]*:\s*(\d)", content, re.IGNORECASE)
        recommendation = rec_match.group(1).strip().lower().replace(" ", "_") if rec_match else "major_revision"
        confidence = int(conf_match.group(1)) if conf_match else 3

        review_data = {
            "recommendation": recommendation,
            "confidence": confidence,
            "report": content,
        }
        editorial = record_review(project_directory, role, review_data)
        remaining = [r["id"] for r in editorial.get("reviewers", [])
                     if r["id"] not in editorial.get("reviews", {})]

        if remaining:
            next_reviewer = remaining[0]
            # Find reviewer name for clarity
            rev_name = next((r.get("name", next_reviewer) for r in editorial.get("reviewers", [])
                           if r["id"] == next_reviewer), next_reviewer)
            return (
                f"Review by {role} recorded successfully.\n"
                f"Next reviewer: {rev_name} ({next_reviewer}).\n\n"
                f"---\n"
                f"MANDATORY NEXT STEPS:\n"
                f"1. Call autolab_next NOW â€” it will give you {rev_name}'s reviewer prompt.\n"
                f"2. Act as that reviewer and write the review.\n"
                f"3. Call autolab_record with role='{next_reviewer}'.\n"
                f"4. Then repeat: autolab_next for the next reviewer (or autolab_editorial if done).\n"
                f"NEVER stop. NEVER call lab_meeting between reviewer turns â€” go straight to autolab_next."
            )
        else:
            return (
                f"Review by {role} recorded. ALL REVIEWS ARE COMPLETE.\n\n"
                f"The Editor (user) will now make a final decision."
                + _EDITORIAL_INSTRUCTION
            )

    # --- PI submission handling ---
    if role == "pi" and status == "ready_for_review":
        submit_manuscript(project_directory, content)
        if progress >= 0:
            st = load_state(project_directory)
            st["progress"] = max(0, min(100, progress))
            save_state(project_directory, st)
        return (
            f"Manuscript submitted to Editor.\n"
            f"Cover letter and manuscript summary recorded.\n"
            f"The Editor (user) will now review via the monitoring UI."
            + _EDITORIAL_INSTRUCTION
        )

    # --- Normal turn handling ---
    next_role = "trainee" if role == "pi" else "pi"
    new_iteration = iteration + (1 if role == "trainee" else 0)
    state = load_state(project_directory)  # reload in case reviewer updated it
    state["iteration"] = new_iteration
    state["next_role"] = next_role
    state["status"] = status if status != "ready_for_review" else "active"
    # PI can set overall progress (0-100)
    if progress >= 0 and role == "pi":
        state["progress"] = max(0, min(100, progress))
    save_state(project_directory, state)

    # Compress old meetings periodically
    if new_iteration > 0 and new_iteration % COMPRESS_EVERY == 0:
        compress_old_meetings(project_directory)

    # Check if user left feedback via the web UI
    user_feedback = state.get("user_feedback", "").strip()

    if user_feedback:
        return (
            f"Turn recorded. Iteration {new_iteration}, next: {next_role.upper()}.\n\n"
            f"User feedback pending:\n{user_feedback}"
            + _LOOP_INSTRUCTION
        )

    return (
        f"Turn recorded. Iteration {new_iteration}, next: {next_role.upper()}."
        + _LOOP_INSTRUCTION
    )


@mcp.tool()
async def autolab_consult(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
    expert_name: Annotated[str, Field(description="Expert's name (e.g., 'Dr. Sarah Chen')")] = "",
    expert_role: Annotated[str, Field(description="Expert's specialty (e.g., 'Statistician', 'Immunologist')")] = "",
    expert_avatar: Annotated[str, Field(
        description="Avatar key for the UI sprite: reviewer, bioethicist, science_writer, grant_reviewer, "
                    "immunologist, oncologist, neuroscientist, geneticist, cell_biologist, microbiologist, "
                    "pathologist, pharmacologist, structural_bio, systems_biologist, epidemiologist, "
                    "statistician, bioinformatician, data_scientist, ml_engineer, comp_biologist, "
                    "clinician, radiologist, surgeon, chemist, physicist, engineer, psychologist, ecologist, generic"
    )] = "generic",
    question: Annotated[str, Field(description="The specific question or topic to consult about")] = "",
) -> str:
    """Invite a domain expert for a one-time consultation during a PI turn.

    The PI can call this to get advice from a specialist on a specific topic.
    The AI should then role-play as that expert and provide their insight,
    then continue the PI turn normally.

    This does NOT interrupt the loop â€” it's a synchronous consultation within
    the current PI turn. The expert appears in the monitoring UI sidebar.

    Args:
        project_directory: Path to the project directory
        expert_name: Name of the expert
        expert_role: Their specialty/domain
        expert_avatar: Avatar sprite key for the UI
        question: The specific question being asked

    Returns:
        str: Expert consultation prompt â€” the AI should role-play as the expert
    """
    from .lab.state import load_state, save_state

    project_directory = os.path.abspath(project_directory)

    if not expert_name or not expert_role or not question:
        return "ERROR: expert_name, expert_role, and question are all required."

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    # Add expert to the list (dedup by name)
    experts = state.get("experts", [])
    existing = next((e for e in experts if e["name"] == expert_name), None)
    if not existing:
        experts.append({
            "name": expert_name,
            "role": expert_role,
            "avatar": expert_avatar,
        })
        state["experts"] = experts
        save_state(project_directory, state)

    # Build the expert consultation prompt
    idea = ""
    try:
        from .lab.state import load_idea
        idea = load_idea(project_directory)
    except Exception:
        pass

    return (
        f"[CONSULTATION] Expert: {expert_name} ({expert_role})\n"
        f"{'=' * 50}\n\n"
        f"You are now briefly acting as **{expert_name}**, a specialist in **{expert_role}**.\n"
        f"The PI has asked you the following question:\n\n"
        f"> {question}\n\n"
        f"Project context (brief):\n{idea[:300]}{'...' if len(idea) > 300 else ''}\n\n"
        f"Provide a concise, expert response (2-4 paragraphs). Be direct and specific.\n"
        f"Include:\n"
        f"- Your expert opinion on the question\n"
        f"- Key considerations the PI should be aware of\n"
        f"- Any methodological recommendations\n"
        f"- Potential pitfalls or caveats in your domain\n\n"
        f"After providing the consultation, IMMEDIATELY return to acting as the PI.\n"
        f"Do NOT call autolab_record for this consultation â€” it's part of the PI's turn.\n"
        f"Continue with whatever the PI was doing before the consultation."
    )


@mcp.tool()
async def autolab_editorial(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
) -> str:
    """Wait for the Editor (user) to act on the submitted manuscript.

    Call this in two scenarios:
    1. After PI submits a manuscript (autolab_record with status='ready_for_review').
       Waits for the editor to either Desk Reject or Invite Reviewers.
    2. After all reviewer reports are complete.
       Waits for the editor to make a final decision (Accept/Minor/Major/Reject).

    This tool BLOCKS with NO TIMEOUT â€” it can wait minutes or hours.
    The editor acts via the monitoring UI (Editor's Desk overlay).

    Returns:
      - If reviewers invited: instructions to run the reviewer loop
        (call autolab_next for each reviewer, then autolab_editorial again)
      - If decision made (accept/minor/major/reject): the decision + instructions
        to call lab_meeting then autolab_next to continue PI/Trainee loop.
    """
    import asyncio

    from .lab.state import load_state

    project_directory = os.path.abspath(project_directory)

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    editorial = state.get("editorial", {})
    starting_phase = editorial.get("phase", "none")
    iteration = state.get("iteration", 0)
    current_status = state.get("status", "")
    next_role = state.get("next_role", "")

    debug_log(f"autolab_editorial: starting phase={starting_phase}, status={current_status}, next_role={next_role}")

    # â”€â”€ Immediate returns: if the state already advanced, don't poll â”€â”€

    # Already in reviewer phase â†’ return immediately with reviewer instructions
    if starting_phase in ("reviewers_invited", "under_review") and next_role.startswith("reviewer_"):
        reviewers = editorial.get("reviewers", [])
        reviewer_names = ", ".join(r.get("name", r["id"]) for r in reviewers)
        return (
            f"[EDITORIAL] Reviewers invited: {reviewer_names}\n\n"
            f"The editor has invited {len(reviewers)} reviewer(s) and reviews are in progress.\n"
            f"You must now play each reviewer role in sequence.\n\n"
            f"MANDATORY NEXT STEPS:\n"
            f"1. Call autolab_next â€” it will return the current reviewer's prompt.\n"
            f"2. Act as that reviewer, then call autolab_record with role='reviewer_N'.\n"
            f"3. Call autolab_next immediately for the next reviewer (do NOT call lab_meeting between reviewers).\n"
            f"4. Repeat until all reviews are done.\n"
            f"5. After the LAST review, call autolab_editorial to wait for the editor's final decision.\n"
            f"NEVER stop. Continue the loop."
        )

    # Already have a decision â†’ return immediately
    if starting_phase == "decision_made":
        decision = editorial.get("decision", "")
        feedback = editorial.get("decision_feedback", "")
        dec_display = decision.upper().replace("_", " ")
        return (
            f"[EDITORIAL] Decision: {dec_display}\n\n"
            f"Editor feedback: {feedback}\n\n"
            f"The editorial process for this round is complete.\n"
            f"Decision: {dec_display}"
            + _LOOP_INSTRUCTION
        )

    # Reviews already complete â†’ wait for decision only
    if starting_phase == "reviews_complete":
        debug_log("autolab_editorial: all reviews done, waiting for editor decision")

    # â”€â”€ Poll loop: wait for the editorial phase to advance â”€â”€

    poll_interval = 5  # seconds
    while True:
        await asyncio.sleep(poll_interval)

        try:
            state = load_state(project_directory)
        except Exception:
            continue  # Retry on transient read errors

        editorial = state.get("editorial", {})
        current_phase = editorial.get("phase", "none")
        current_status = state.get("status", "")
        next_role = state.get("next_role", "")

        # --- Case 1: We were waiting for editor after submission ---
        if starting_phase == "submitted":
            if current_phase in ("reviewers_invited", "under_review"):
                # Editor invited reviewers â†’ AI must now play reviewer roles
                reviewers = editorial.get("reviewers", [])
                reviewer_names = ", ".join(r.get("name", r["id"]) for r in reviewers)
                return (
                    f"[EDITORIAL] Reviewers invited: {reviewer_names}\n\n"
                    f"The editor has invited {len(reviewers)} reviewer(s).\n"
                    f"You must now play each reviewer role in sequence.\n\n"
                    f"MANDATORY NEXT STEPS:\n"
                    f"1. Call autolab_next â€” it will return the first reviewer's prompt.\n"
                    f"2. Act as that reviewer, then call autolab_record with role='reviewer_N'.\n"
                    f"3. Call autolab_next immediately for the next reviewer (do NOT call lab_meeting between reviewers).\n"
                    f"4. Repeat until all reviews are done.\n"
                    f"5. After the LAST review, call autolab_editorial to wait for the editor's final decision.\n"
                    f"NEVER stop. Continue the loop."
                )
            if current_phase == "decision_made":
                # Desk reject (editor decided without reviewers)
                decision = editorial.get("decision", "reject")
                feedback = editorial.get("decision_feedback", "")
                return (
                    f"[EDITORIAL] Decision: {decision.upper().replace('_', ' ')}\n\n"
                    f"The editor has desk-rejected the manuscript.\n"
                    f"Feedback: {feedback}\n"
                    + _LOOP_INSTRUCTION
                )

        # --- Case 2: We were waiting for final decision after reviews ---
        if starting_phase in ("reviews_complete", "under_review", "reviewers_invited"):
            if current_phase == "decision_made":
                decision = editorial.get("decision", "")
                feedback = editorial.get("decision_feedback", "")
                dec_display = decision.upper().replace("_", " ")
                return (
                    f"[EDITORIAL] Decision: {dec_display}\n\n"
                    f"Editor feedback: {feedback}\n\n"
                    f"The editorial process for this round is complete.\n"
                    f"Decision: {dec_display}"
                    + _LOOP_INSTRUCTION
                )
            # Reviewer role assigned while we're polling â†’ break out to let AI play reviewer
            if next_role.startswith("reviewer_") and current_phase in ("reviewers_invited", "under_review"):
                reviewers = editorial.get("reviewers", [])
                reviewer_names = ", ".join(r.get("name", r["id"]) for r in reviewers)
                return (
                    f"[EDITORIAL] Reviewer turn ready: {next_role}\n\n"
                    f"Reviewers: {reviewer_names}\n\n"
                    f"MANDATORY NEXT STEPS:\n"
                    f"1. Call autolab_next to get the reviewer prompt.\n"
                    f"2. Act as the reviewer, then call autolab_record with role='{next_role}'.\n"
                    f"3. Call autolab_next immediately for the next reviewer (skip lab_meeting between reviewers).\n"
                    f"4. After the LAST review, call autolab_editorial again.\n"
                    f"NEVER stop."
                )

        # --- Generic: any phase change from what we started with ---
        if current_phase != starting_phase:
            return (
                f"[EDITORIAL] Phase changed: {starting_phase} â†’ {current_phase}\n"
                f"Status: {current_status}\n"
                + _LOOP_INSTRUCTION
            )

        # Still waiting â€” continue polling
        # Gradually increase poll interval up to 15s to reduce disk I/O
        if poll_interval < 15:
            poll_interval = min(poll_interval + 1, 15)


@mcp.tool()
async def autolab_status(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
) -> str:
    """Get the current status of an Autonomous Lab project.

    Returns the current state, file listings, paper progress,
    and a summary of recent meetings.

    Args:
        project_directory: Path to the project directory

    Returns:
        str: Formatted status report
    """
    from .lab.state import (
        get_paper_progress,
        get_recent_meetings,
        load_state,
        scan_project_files,
    )

    project_directory = os.path.abspath(project_directory)

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    file_listings = scan_project_files(project_directory)
    paper_progress = get_paper_progress(project_directory)
    recent = get_recent_meetings(project_directory, n=2)

    # Format file counts
    file_counts = {k: len(v) for k, v in file_listings.items()}

    # Format paper progress
    paper_lines = []
    for section, info in paper_progress.items():
        if info["exists"] and info["words"] > 0:
            paper_lines.append(f"  {section}: {info['words']} words")
        elif info["exists"]:
            paper_lines.append(f"  {section}: placeholder only")
        else:
            paper_lines.append(f"  {section}: not created")

    report = (
        f"# Autonomous Lab Status\n\n"
        f"**Iteration:** {state['iteration']}\n"
        f"**Next role:** {state['next_role']}\n"
        f"**Status:** {state['status']}\n"
        f"**Last updated:** {state.get('last_updated', 'unknown')}\n\n"
        f"## File Counts\n"
        f"  data: {file_counts.get('data', 0)} files\n"
        f"  scripts: {file_counts.get('scripts', 0)} files\n"
        f"  figures: {file_counts.get('figures', 0)} files\n"
        f"  results: {file_counts.get('results', 0)} files\n"
        f"  paper: {file_counts.get('paper', 0)} files\n\n"
        f"## Paper Progress\n"
        + "\n".join(paper_lines)
        + "\n\n"
        f"## Recent Meetings\n\n{recent if recent.strip() else 'No meetings yet.'}\n"
    )

    if state.get("user_feedback"):
        report += f"\n## Pending User Feedback\n\n{state['user_feedback']}\n"

    return report


# ===== Biomni Integration MCP Tools =====

@mcp.tool()
async def autolab_biomni_status(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
) -> str:
    """Check Biomni integration status.

    Returns whether Biomni is installed, its version, whether it is enabled
    for this project, and datalake availability.

    Biomni (https://github.com/snap-stanford/Biomni) is an optional
    integration that provides 100+ biomedical tools and workflows.
    It is NOT bundled with Autonomous Lab â€” users must opt in.

    Args:
        project_directory: Path to the project directory

    Returns:
        str: JSON status report
    """
    from .integrations.biomni import get_status
    project_directory = os.path.abspath(project_directory)
    status = get_status(project_directory)
    return json.dumps(status, indent=2)


@mcp.tool()
async def autolab_biomni_install(
    from_source: Annotated[bool, Field(description="Install from GitHub (True) or PyPI (False)")] = True,
) -> str:
    """Install Biomni from GitHub or PyPI.

    This downloads and installs the Biomni package at runtime.
    Biomni is Apache 2.0 licensed, but some of its integrated tools
    may have more restrictive licenses â€” review before commercial use.

    NOTE: The full datalake (~11 GB) is downloaded on first agent use.
    Set skip_datalake=True in config to skip it.

    Args:
        from_source: If True, install latest from GitHub main. If False, use PyPI.

    Returns:
        str: Installation result
    """
    from .integrations.biomni import install_biomni
    result = install_biomni(from_source=from_source)
    if result["success"]:
        return f"SUCCESS: {result['message']}"
    else:
        return f"FAILED: {result['message']}"


@mcp.tool()
async def autolab_biomni_configure(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
    enabled: Annotated[bool, Field(description="Enable Biomni integration")] = True,
    data_path: Annotated[str, Field(description="Path to Biomni datalake")] = "./data",
    skip_datalake: Annotated[bool, Field(description="Skip downloading the 11GB datalake")] = False,
) -> str:
    """Configure Biomni integration for this project.

    Saves settings to .autolab/biomni_config.json.
    Biomni must be installed separately (use autolab_biomni_install).

    NOTE: Biomni is used as a tool/database library only â€” we do NOT
    instantiate the Biomni A1 agent or use their LLM pipeline.

    Args:
        project_directory: Path to the project directory
        enabled: Whether to enable Biomni for this project
        data_path: Path for Biomni's data/datalake
        skip_datalake: If True, skip the ~11GB datalake download

    Returns:
        str: Saved configuration
    """
    from .integrations.biomni import save_biomni_config
    project_directory = os.path.abspath(project_directory)
    cfg = save_biomni_config(
        project_directory,
        enabled=enabled,
        data_path=data_path,
        skip_datalake=skip_datalake,
    )
    return f"Biomni config saved:\n{json.dumps(cfg, indent=2)}"


@mcp.tool()
async def autolab_biomni_tools(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
) -> str:
    """List available Biomni tools and databases.

    Biomni provides curated biomedical tools (ADMET prediction,
    scRNA-seq analysis, CRISPR screen planning, etc.) and database
    connectors.  This lists what's available for import in scripts.

    NOTE: We use Biomni as a tool/database library, NOT as an agent.
    The Trainee imports and calls individual functions directly in
    their Python scripts.

    Returns:
        str: Available tools and databases, or installation instructions
    """
    from .integrations.biomni import (
        is_biomni_available,
        list_available_databases,
        list_available_tools,
    )
    if not is_biomni_available():
        return (
            "Biomni is not installed.\n"
            "Install with: autolab_biomni_install()\n"
            "See: https://github.com/snap-stanford/Biomni"
        )
    tools = list_available_tools()
    dbs = list_available_databases()

    lines = ["## Available Biomni Tools\n"]
    if tools:
        for t in tools:
            desc = f" â€” {t['description']}" if t.get('description') else ""
            lines.append(f"  - **{t['name']}** (`{t['module']}`){desc}")
    else:
        lines.append("  (no tools detected)")

    lines.append("\n## Available Biomni Databases\n")
    if dbs:
        for d in dbs:
            desc = f" â€” {d['description']}" if d.get('description') else ""
            lines.append(f"  - **{d['name']}** (`{d['module']}`){desc}")
    else:
        lines.append("  (no databases detected)")

    lines.append(
        "\n## Usage\n"
        "Import tools directly in scripts:\n"
        "```python\n"
        "from biomni.tools.<tool_name> import *\n"
        "```\n"
        "Do NOT instantiate the Biomni A1 agent."
    )
    return "\n".join(lines)


@mcp.tool()
async def autolab_biomni_env(
) -> str:
    """Check Biomni environment details.

    Shows which Biomni sub-packages are available, the active conda
    environment, datalake status, and Python path.

    Returns:
        str: Environment report
    """
    from .integrations.biomni import check_environment
    env = check_environment()
    return json.dumps(env, indent=2)


# ===== Main entry point =====
def main():
    """ä¸»è¦å…¥å£é»ï¼Œç”¨æ–¼å¥—ä»¶åŸ·è¡Œ
    æ”¶é›†ç”¨æˆ¶çš„äº’å‹•å›é¥‹ï¼Œæ”¯æ´æ–‡å­—å’Œåœ–ç‰‡
    æ­¤å·¥å…·ä½¿ç”¨ Web UI ä»‹é¢æ”¶é›†ç”¨æˆ¶å›é¥‹ï¼Œæ”¯æ´æ™ºèƒ½ç’°å¢ƒæª¢æ¸¬ã€‚

    ç”¨æˆ¶å¯ä»¥ï¼š
    1. åŸ·è¡Œå‘½ä»¤ä¾†é©—è­‰çµæœ
    2. æä¾›æ–‡å­—å›é¥‹
    3. ä¸Šå‚³åœ–ç‰‡ä½œç‚ºå›é¥‹
    4. æŸ¥çœ‹ AI çš„å·¥ä½œæ‘˜è¦

    èª¿è©¦æ¨¡å¼ï¼š
    - è¨­ç½®ç’°å¢ƒè®Šæ•¸ MCP_DEBUG=true å¯å•Ÿç”¨è©³ç´°èª¿è©¦è¼¸å‡º
    - ç”Ÿç”¢ç’°å¢ƒå»ºè­°é—œé–‰èª¿è©¦æ¨¡å¼ä»¥é¿å…è¼¸å‡ºå¹²æ“¾


    """
    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨èª¿è©¦æ¨¡å¼
    debug_enabled = os.getenv("MCP_DEBUG", "").lower() in ("true", "1", "yes", "on")

    if debug_enabled:
        debug_log("ğŸš€ å•Ÿå‹•äº’å‹•å¼å›é¥‹æ”¶é›† MCP æœå‹™å™¨")
        debug_log(f"   æœå‹™å™¨åç¨±: {SERVER_NAME}")
        debug_log(f"   ç‰ˆæœ¬: {__version__}")
        debug_log(f"   å¹³å°: {sys.platform}")
        debug_log(f"   ç·¨ç¢¼åˆå§‹åŒ–: {'æˆåŠŸ' if _encoding_initialized else 'å¤±æ•—'}")
        debug_log(f"   é ç«¯ç’°å¢ƒ: {is_remote_environment()}")
        debug_log(f"   WSL ç’°å¢ƒ: {is_wsl_environment()}")
        debug_log("   Interface: Web UI")
        debug_log("   ç­‰å¾…ä¾†è‡ª AI åŠ©æ‰‹çš„èª¿ç”¨...")
        debug_log("æº–å‚™å•Ÿå‹• MCP ä¼ºæœå™¨...")
        debug_log("èª¿ç”¨ mcp.run()...")

    try:
        # ä½¿ç”¨æ­£ç¢ºçš„ FastMCP API
        mcp.run()
    except KeyboardInterrupt:
        if debug_enabled:
            debug_log("æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£å¸¸é€€å‡º")
        sys.exit(0)
    except Exception as e:
        if debug_enabled:
            debug_log(f"MCP æœå‹™å™¨å•Ÿå‹•å¤±æ•—: {e}")
            import traceback

            debug_log(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
