#!/usr/bin/env python3
"""
MCP Feedback Enhanced ä¼ºæœå™¨ä¸»è¦æ¨¡çµ„

æ­¤æ¨¡çµ„æä¾› MCP (Model Context Protocol) çš„å¢å¼·å›é¥‹æ”¶é›†åŠŸèƒ½ï¼Œ
æ”¯æ´æ™ºèƒ½ç’°å¢ƒæª¢æ¸¬ï¼Œè‡ªå‹•ä½¿ç”¨ Web UI ä»‹é¢ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- MCP å·¥å…·å¯¦ç¾
- ä»‹é¢é¸æ“‡ï¼ˆWeb UIï¼‰
- ç’°å¢ƒæª¢æ¸¬ (SSH Remote, WSL, Local)
- åœ‹éš›åŒ–æ”¯æ´
- åœ–ç‰‡è™•ç†èˆ‡ä¸Šå‚³
- å‘½ä»¤åŸ·è¡Œèˆ‡çµæœå±•ç¤º
- å°ˆæ¡ˆç›®éŒ„ç®¡ç†

ä¸»è¦ MCP å·¥å…·ï¼š
- interactive_feedback: æ”¶é›†ç”¨æˆ¶äº’å‹•å›é¥‹
- get_system_info: ç²å–ç³»çµ±ç’°å¢ƒè³‡è¨Š

ä½œè€…: FÃ¡bio Ferreira (åŸä½œè€…)
å¢å¼·: Minidoracat (Web UI, åœ–ç‰‡æ”¯æ´, ç’°å¢ƒæª¢æ¸¬)
é‡æ§‹: æ¨¡å¡ŠåŒ–è¨­è¨ˆ
"""

import base64
import io
import json
import os
import sys
from typing import Annotated, Any

from fastmcp import FastMCP
from fastmcp.utilities.types import Image as MCPImage
from mcp.types import TextContent
from pydantic import Field

# å°å…¥çµ±ä¸€çš„èª¿è©¦åŠŸèƒ½
from .debug import server_debug_log as debug_log

# å°å…¥å¤šèªç³»æ”¯æ´
# å°å…¥éŒ¯èª¤è™•ç†æ¡†æ¶
from .utils.error_handler import ErrorHandler, ErrorType

# å°å…¥è³‡æºç®¡ç†å™¨
from .utils.resource_manager import create_temp_file


# ===== ç·¨ç¢¼åˆå§‹åŒ– =====
def init_encoding():
    """åˆå§‹åŒ–ç·¨ç¢¼è¨­ç½®ï¼Œç¢ºä¿æ­£ç¢ºè™•ç†ä¸­æ–‡å­—ç¬¦"""
    try:
        # Windows ç‰¹æ®Šè™•ç†
        if sys.platform == "win32":
            import msvcrt

            # è¨­ç½®ç‚ºäºŒé€²åˆ¶æ¨¡å¼
            msvcrt.setmode(sys.stdin.fileno(), os.O_BINARY)
            msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)

            # é‡æ–°åŒ…è£ç‚º UTF-8 æ–‡æœ¬æµï¼Œä¸¦ç¦ç”¨ç·©è¡
            # ä¿®å¾© union-attr éŒ¯èª¤ - å®‰å…¨ç²å– buffer æˆ– detach
            stdin_buffer = getattr(sys.stdin, "buffer", None)
            if stdin_buffer is None and hasattr(sys.stdin, "detach"):
                stdin_buffer = sys.stdin.detach()

            stdout_buffer = getattr(sys.stdout, "buffer", None)
            if stdout_buffer is None and hasattr(sys.stdout, "detach"):
                stdout_buffer = sys.stdout.detach()

            sys.stdin = io.TextIOWrapper(
                stdin_buffer, encoding="utf-8", errors="replace", newline=None
            )
            sys.stdout = io.TextIOWrapper(
                stdout_buffer,
                encoding="utf-8",
                errors="replace",
                newline="",
                write_through=True,  # é—œéµï¼šç¦ç”¨å¯«å…¥ç·©è¡
            )
        else:
            # é Windows ç³»çµ±çš„æ¨™æº–è¨­ç½®
            if hasattr(sys.stdout, "reconfigure"):
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stdin, "reconfigure"):
                sys.stdin.reconfigure(encoding="utf-8", errors="replace")

        # è¨­ç½® stderr ç·¨ç¢¼ï¼ˆç”¨æ–¼èª¿è©¦è¨Šæ¯ï¼‰
        if hasattr(sys.stderr, "reconfigure"):
            sys.stderr.reconfigure(encoding="utf-8", errors="replace")

        return True
    except Exception:
        # å¦‚æœç·¨ç¢¼è¨­ç½®å¤±æ•—ï¼Œå˜—è©¦åŸºæœ¬è¨­ç½®
        try:
            if hasattr(sys.stdout, "reconfigure"):
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stdin, "reconfigure"):
                sys.stdin.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stderr, "reconfigure"):
                sys.stderr.reconfigure(encoding="utf-8", errors="replace")
        except:
            pass
        return False


# åˆå§‹åŒ–ç·¨ç¢¼ï¼ˆåœ¨å°å…¥æ™‚å°±åŸ·è¡Œï¼‰
_encoding_initialized = init_encoding()

# ===== å¸¸æ•¸å®šç¾© =====
SERVER_NAME = "Autonomous Lab MCP"
SSH_ENV_VARS = ["SSH_CONNECTION", "SSH_CLIENT", "SSH_TTY"]
REMOTE_ENV_VARS = ["REMOTE_CONTAINERS", "CODESPACES"]


# åˆå§‹åŒ– MCP æœå‹™å™¨
from . import __version__


# ç¢ºä¿ log_level è¨­å®šç‚ºæ­£ç¢ºçš„å¤§å¯«æ ¼å¼
fastmcp_settings = {}

# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ä¸¦è¨­å®šæ­£ç¢ºçš„ log_level
env_log_level = os.getenv("FASTMCP_LOG_LEVEL", "").upper()
if env_log_level in ("DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"):
    fastmcp_settings["log_level"] = env_log_level
else:
    # é è¨­ä½¿ç”¨ INFO ç­‰ç´š
    fastmcp_settings["log_level"] = "INFO"

mcp: Any = FastMCP(SERVER_NAME)


# ===== å·¥å…·å‡½æ•¸ =====
def is_wsl_environment() -> bool:
    """
    æª¢æ¸¬æ˜¯å¦åœ¨ WSL (Windows Subsystem for Linux) ç’°å¢ƒä¸­é‹è¡Œ

    Returns:
        bool: True è¡¨ç¤º WSL ç’°å¢ƒï¼ŒFalse è¡¨ç¤ºå…¶ä»–ç’°å¢ƒ
    """
    try:
        # æª¢æŸ¥ /proc/version æ–‡ä»¶æ˜¯å¦åŒ…å« WSL æ¨™è­˜
        if os.path.exists("/proc/version"):
            with open("/proc/version") as f:
                version_info = f.read().lower()
                if "microsoft" in version_info or "wsl" in version_info:
                    debug_log("åµæ¸¬åˆ° WSL ç’°å¢ƒï¼ˆé€šé /proc/versionï¼‰")
                    return True

        # æª¢æŸ¥ WSL ç›¸é—œç’°å¢ƒè®Šæ•¸
        wsl_env_vars = ["WSL_DISTRO_NAME", "WSL_INTEROP", "WSLENV"]
        for env_var in wsl_env_vars:
            if os.getenv(env_var):
                debug_log(f"åµæ¸¬åˆ° WSL ç’°å¢ƒè®Šæ•¸: {env_var}")
                return True

        # æª¢æŸ¥æ˜¯å¦å­˜åœ¨ WSL ç‰¹æœ‰çš„è·¯å¾‘
        wsl_paths = ["/mnt/c", "/mnt/d", "/proc/sys/fs/binfmt_misc/WSLInterop"]
        for path in wsl_paths:
            if os.path.exists(path):
                debug_log(f"åµæ¸¬åˆ° WSL ç‰¹æœ‰è·¯å¾‘: {path}")
                return True

    except Exception as e:
        debug_log(f"WSL æª¢æ¸¬éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

    return False


def is_remote_environment() -> bool:
    """
    æª¢æ¸¬æ˜¯å¦åœ¨é ç«¯ç’°å¢ƒä¸­é‹è¡Œ

    Returns:
        bool: True è¡¨ç¤ºé ç«¯ç’°å¢ƒï¼ŒFalse è¡¨ç¤ºæœ¬åœ°ç’°å¢ƒ
    """
    # WSL ä¸æ‡‰è¢«è¦–ç‚ºé ç«¯ç’°å¢ƒï¼Œå› ç‚ºå®ƒå¯ä»¥è¨ªå• Windows ç€è¦½å™¨
    if is_wsl_environment():
        debug_log("WSL ç’°å¢ƒä¸è¢«è¦–ç‚ºé ç«¯ç’°å¢ƒ")
        return False

    # æª¢æŸ¥ SSH é€£ç·šæŒ‡æ¨™
    for env_var in SSH_ENV_VARS:
        if os.getenv(env_var):
            debug_log(f"åµæ¸¬åˆ° SSH ç’°å¢ƒè®Šæ•¸: {env_var}")
            return True

    # æª¢æŸ¥é ç«¯é–‹ç™¼ç’°å¢ƒ
    for env_var in REMOTE_ENV_VARS:
        if os.getenv(env_var):
            debug_log(f"åµæ¸¬åˆ°é ç«¯é–‹ç™¼ç’°å¢ƒ: {env_var}")
            return True

    # æª¢æŸ¥ Docker å®¹å™¨
    if os.path.exists("/.dockerenv"):
        debug_log("åµæ¸¬åˆ° Docker å®¹å™¨ç’°å¢ƒ")
        return True

    # Windows é ç«¯æ¡Œé¢æª¢æŸ¥
    if sys.platform == "win32":
        session_name = os.getenv("SESSIONNAME", "")
        if session_name and "RDP" in session_name:
            debug_log(f"åµæ¸¬åˆ° Windows é ç«¯æ¡Œé¢: {session_name}")
            return True

    # Linux ç„¡é¡¯ç¤ºç’°å¢ƒæª¢æŸ¥ï¼ˆä½†æ’é™¤ WSLï¼‰
    if (
        sys.platform.startswith("linux")
        and not os.getenv("DISPLAY")
        and not is_wsl_environment()
    ):
        debug_log("åµæ¸¬åˆ° Linux ç„¡é¡¯ç¤ºç’°å¢ƒ")
        return True

    return False


def save_feedback_to_file(feedback_data: dict, file_path: str | None = None) -> str:
    """
    å°‡å›é¥‹è³‡æ–™å„²å­˜åˆ° JSON æ–‡ä»¶

    Args:
        feedback_data: å›é¥‹è³‡æ–™å­—å…¸
        file_path: å„²å­˜è·¯å¾‘ï¼Œè‹¥ç‚º None å‰‡è‡ªå‹•ç”¢ç”Ÿè‡¨æ™‚æ–‡ä»¶

    Returns:
        str: å„²å­˜çš„æ–‡ä»¶è·¯å¾‘
    """
    if file_path is None:
        # ä½¿ç”¨è³‡æºç®¡ç†å™¨å‰µå»ºè‡¨æ™‚æ–‡ä»¶
        file_path = create_temp_file(suffix=".json", prefix="feedback_")

    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

    # è¤‡è£½æ•¸æ“šä»¥é¿å…ä¿®æ”¹åŸå§‹æ•¸æ“š
    json_data = feedback_data.copy()

    # è™•ç†åœ–ç‰‡æ•¸æ“šï¼šå°‡ bytes è½‰æ›ç‚º base64 å­—ç¬¦ä¸²ä»¥ä¾¿ JSON åºåˆ—åŒ–
    if "images" in json_data and isinstance(json_data["images"], list):
        processed_images = []
        for img in json_data["images"]:
            if isinstance(img, dict) and "data" in img:
                processed_img = img.copy()
                # å¦‚æœ data æ˜¯ bytesï¼Œè½‰æ›ç‚º base64 å­—ç¬¦ä¸²
                if isinstance(img["data"], bytes):
                    processed_img["data"] = base64.b64encode(img["data"]).decode(
                        "utf-8"
                    )
                    processed_img["data_type"] = "base64"
                processed_images.append(processed_img)
            else:
                processed_images.append(img)
        json_data["images"] = processed_images

    # å„²å­˜è³‡æ–™
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    debug_log(f"å›é¥‹è³‡æ–™å·²å„²å­˜è‡³: {file_path}")
    return file_path


def create_feedback_text(feedback_data: dict) -> str:
    """
    å»ºç«‹æ ¼å¼åŒ–çš„å›é¥‹æ–‡å­—

    Args:
        feedback_data: å›é¥‹è³‡æ–™å­—å…¸

    Returns:
        str: æ ¼å¼åŒ–å¾Œçš„å›é¥‹æ–‡å­—
    """
    text_parts = []

    # åŸºæœ¬å›é¥‹å…§å®¹
    if feedback_data.get("interactive_feedback"):
        text_parts.append(f"=== ç”¨æˆ¶å›é¥‹ ===\n{feedback_data['interactive_feedback']}")

    # å‘½ä»¤åŸ·è¡Œæ—¥èªŒ
    if feedback_data.get("command_logs"):
        text_parts.append(f"=== å‘½ä»¤åŸ·è¡Œæ—¥èªŒ ===\n{feedback_data['command_logs']}")

    # åœ–ç‰‡é™„ä»¶æ¦‚è¦
    if feedback_data.get("images"):
        images = feedback_data["images"]
        text_parts.append(f"=== åœ–ç‰‡é™„ä»¶æ¦‚è¦ ===\nç”¨æˆ¶æä¾›äº† {len(images)} å¼µåœ–ç‰‡ï¼š")

        for i, img in enumerate(images, 1):
            size = img.get("size", 0)
            name = img.get("name", "unknown")

            # æ™ºèƒ½å–®ä½é¡¯ç¤º
            if size < 1024:
                size_str = f"{size} B"
            elif size < 1024 * 1024:
                size_kb = size / 1024
                size_str = f"{size_kb:.1f} KB"
            else:
                size_mb = size / (1024 * 1024)
                size_str = f"{size_mb:.1f} MB"

            img_info = f"  {i}. {name} ({size_str})"

            # ç‚ºæé«˜å…¼å®¹æ€§ï¼Œæ·»åŠ  base64 é è¦½ä¿¡æ¯
            if img.get("data"):
                try:
                    if isinstance(img["data"], bytes):
                        img_base64 = base64.b64encode(img["data"]).decode("utf-8")
                    elif isinstance(img["data"], str):
                        img_base64 = img["data"]
                    else:
                        img_base64 = None

                    if img_base64:
                        # åªé¡¯ç¤ºå‰50å€‹å­—ç¬¦çš„é è¦½
                        preview = (
                            img_base64[:50] + "..."
                            if len(img_base64) > 50
                            else img_base64
                        )
                        img_info += f"\n     Base64 é è¦½: {preview}"
                        img_info += f"\n     å®Œæ•´ Base64 é•·åº¦: {len(img_base64)} å­—ç¬¦"

                        # å¦‚æœ AI åŠ©æ‰‹ä¸æ”¯æ´ MCP åœ–ç‰‡ï¼Œå¯ä»¥æä¾›å®Œæ•´ base64
                        debug_log(f"åœ–ç‰‡ {i} Base64 å·²æº–å‚™ï¼Œé•·åº¦: {len(img_base64)}")

                        # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨ Base64 è©³ç´°æ¨¡å¼ï¼ˆå¾ UI è¨­å®šä¸­ç²å–ï¼‰
                        include_full_base64 = feedback_data.get("settings", {}).get(
                            "enable_base64_detail", False
                        )

                        if include_full_base64:
                            # æ ¹æ“šæª”æ¡ˆåæ¨æ–· MIME é¡å‹
                            file_name = img.get("name", "image.png")
                            if file_name.lower().endswith((".jpg", ".jpeg")):
                                mime_type = "image/jpeg"
                            elif file_name.lower().endswith(".gif"):
                                mime_type = "image/gif"
                            elif file_name.lower().endswith(".webp"):
                                mime_type = "image/webp"
                            else:
                                mime_type = "image/png"

                            img_info += f"\n     å®Œæ•´ Base64: data:{mime_type};base64,{img_base64}"

                except Exception as e:
                    debug_log(f"åœ–ç‰‡ {i} Base64 è™•ç†å¤±æ•—: {e}")

            text_parts.append(img_info)

        # æ·»åŠ å…¼å®¹æ€§èªªæ˜
        text_parts.append(
            "\nğŸ’¡ æ³¨æ„ï¼šå¦‚æœ AI åŠ©æ‰‹ç„¡æ³•é¡¯ç¤ºåœ–ç‰‡ï¼Œåœ–ç‰‡æ•¸æ“šå·²åŒ…å«åœ¨ä¸Šè¿° Base64 ä¿¡æ¯ä¸­ã€‚"
        )

    return "\n\n".join(text_parts) if text_parts else "ç”¨æˆ¶æœªæä¾›ä»»ä½•å›é¥‹å…§å®¹ã€‚"


def process_images(images_data: list[dict]) -> list[MCPImage]:
    """
    è™•ç†åœ–ç‰‡è³‡æ–™ï¼Œè½‰æ›ç‚º MCP åœ–ç‰‡å°è±¡

    Args:
        images_data: åœ–ç‰‡è³‡æ–™åˆ—è¡¨

    Returns:
        List[MCPImage]: MCP åœ–ç‰‡å°è±¡åˆ—è¡¨
    """
    mcp_images = []

    for i, img in enumerate(images_data, 1):
        try:
            if not img.get("data"):
                debug_log(f"åœ–ç‰‡ {i} æ²’æœ‰è³‡æ–™ï¼Œè·³é")
                continue

            # æª¢æŸ¥æ•¸æ“šé¡å‹ä¸¦ç›¸æ‡‰è™•ç†
            if isinstance(img["data"], bytes):
                # å¦‚æœæ˜¯åŸå§‹ bytes æ•¸æ“šï¼Œç›´æ¥ä½¿ç”¨
                image_bytes = img["data"]
                debug_log(
                    f"åœ–ç‰‡ {i} ä½¿ç”¨åŸå§‹ bytes æ•¸æ“šï¼Œå¤§å°: {len(image_bytes)} bytes"
                )
            elif isinstance(img["data"], str):
                # å¦‚æœæ˜¯ base64 å­—ç¬¦ä¸²ï¼Œé€²è¡Œè§£ç¢¼
                image_bytes = base64.b64decode(img["data"])
                debug_log(f"åœ–ç‰‡ {i} å¾ base64 è§£ç¢¼ï¼Œå¤§å°: {len(image_bytes)} bytes")
            else:
                debug_log(f"åœ–ç‰‡ {i} æ•¸æ“šé¡å‹ä¸æ”¯æ´: {type(img['data'])}")
                continue

            if len(image_bytes) == 0:
                debug_log(f"åœ–ç‰‡ {i} æ•¸æ“šç‚ºç©ºï¼Œè·³é")
                continue

            # æ ¹æ“šæ–‡ä»¶åæ¨æ–·æ ¼å¼
            file_name = img.get("name", "image.png")
            if file_name.lower().endswith((".jpg", ".jpeg")):
                image_format = "jpeg"
            elif file_name.lower().endswith(".gif"):
                image_format = "gif"
            else:
                image_format = "png"  # é»˜èªä½¿ç”¨ PNG

            # å‰µå»º MCPImage å°è±¡
            mcp_image = MCPImage(data=image_bytes, format=image_format)
            mcp_images.append(mcp_image)

            debug_log(f"åœ–ç‰‡ {i} ({file_name}) è™•ç†æˆåŠŸï¼Œæ ¼å¼: {image_format}")

        except Exception as e:
            # ä½¿ç”¨çµ±ä¸€éŒ¯èª¤è™•ç†ï¼ˆä¸å½±éŸ¿ JSON RPCï¼‰
            error_id = ErrorHandler.log_error_with_context(
                e,
                context={"operation": "åœ–ç‰‡è™•ç†", "image_index": i},
                error_type=ErrorType.FILE_IO,
            )
            debug_log(f"åœ–ç‰‡ {i} è™•ç†å¤±æ•— [éŒ¯èª¤ID: {error_id}]: {e}")

    debug_log(f"å…±è™•ç† {len(mcp_images)} å¼µåœ–ç‰‡")
    return mcp_images


# ===== MCP å·¥å…·å®šç¾© =====
@mcp.tool()
async def interactive_feedback(
    project_directory: Annotated[str, Field(description="å°ˆæ¡ˆç›®éŒ„è·¯å¾‘")] = ".",
    summary: Annotated[
        str, Field(description="AI å·¥ä½œå®Œæˆçš„æ‘˜è¦èªªæ˜")
    ] = "æˆ‘å·²å®Œæˆäº†æ‚¨è«‹æ±‚çš„ä»»å‹™ã€‚",
    timeout: Annotated[int, Field(description="ç­‰å¾…ç”¨æˆ¶å›é¥‹çš„è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰")] = 600,
) -> list:
    """Interactive feedback collection tool for LLM agents.

    USAGE RULES:
    1. During any process, task, or conversation, whether asking, replying, or completing phased tasks, you must call this tool to ask for feedback.
    2. Unless receiving termination instructions, all steps must repeatedly call this tool.
    3. Whenever user feedback is received, if the feedback content is not empty, you must call this tool again and adjust behavior based on the feedback content.
    4. Only when the user explicitly indicates "end" or "no more interaction needed" can you stop calling this tool, and the process is considered complete.
    5. You should summarize what have done, and provide project directory through args to let user know what you have done to provide feedback for next step.

    Args:
        project_directory: Project directory path for context
        summary: Summary of AI work completed for user review
        timeout: Timeout in seconds for waiting user feedback (default: 600 seconds)

    Returns:
        list: List containing TextContent and MCPImage objects representing user feedback
    """
    # ç’°å¢ƒåµæ¸¬
    is_remote = is_remote_environment()
    is_wsl = is_wsl_environment()

    debug_log(f"ç’°å¢ƒåµæ¸¬çµæœ - é ç«¯: {is_remote}, WSL: {is_wsl}")
    debug_log("ä½¿ç”¨ä»‹é¢: Web UI")

    try:
        # ç¢ºä¿å°ˆæ¡ˆç›®éŒ„å­˜åœ¨
        if not os.path.exists(project_directory):
            project_directory = os.getcwd()
        project_directory = os.path.abspath(project_directory)

        # ä½¿ç”¨ Web æ¨¡å¼
        debug_log("å›é¥‹æ¨¡å¼: web")

        result = await launch_web_feedback_ui(project_directory, summary, timeout)

        # è™•ç†å–æ¶ˆæƒ…æ³
        if not result:
            return [TextContent(type="text", text="ç”¨æˆ¶å–æ¶ˆäº†å›é¥‹ã€‚")]

        # å„²å­˜è©³ç´°çµæœ
        save_feedback_to_file(result)

        # å»ºç«‹å›é¥‹é …ç›®åˆ—è¡¨
        feedback_items = []

        # æ·»åŠ æ–‡å­—å›é¥‹
        if (
            result.get("interactive_feedback")
            or result.get("command_logs")
            or result.get("images")
        ):
            feedback_text = create_feedback_text(result)
            feedback_items.append(TextContent(type="text", text=feedback_text))
            debug_log("æ–‡å­—å›é¥‹å·²æ·»åŠ ")

        # æ·»åŠ åœ–ç‰‡å›é¥‹
        if result.get("images"):
            mcp_images = process_images(result["images"])
            # ä¿®å¾© arg-type éŒ¯èª¤ - ç›´æ¥æ“´å±•åˆ—è¡¨
            feedback_items.extend(mcp_images)
            debug_log(f"å·²æ·»åŠ  {len(mcp_images)} å¼µåœ–ç‰‡")

        # ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹å›é¥‹é …ç›®
        if not feedback_items:
            feedback_items.append(
                TextContent(type="text", text="ç”¨æˆ¶æœªæä¾›ä»»ä½•å›é¥‹å…§å®¹ã€‚")
            )

        debug_log(f"å›é¥‹æ”¶é›†å®Œæˆï¼Œå…± {len(feedback_items)} å€‹é …ç›®")
        return feedback_items

    except Exception as e:
        # ä½¿ç”¨çµ±ä¸€éŒ¯èª¤è™•ç†ï¼Œä½†ä¸å½±éŸ¿ JSON RPC éŸ¿æ‡‰
        error_id = ErrorHandler.log_error_with_context(
            e,
            context={"operation": "å›é¥‹æ”¶é›†", "project_dir": project_directory},
            error_type=ErrorType.SYSTEM,
        )

        # ç”Ÿæˆç”¨æˆ¶å‹å¥½çš„éŒ¯èª¤ä¿¡æ¯
        user_error_msg = ErrorHandler.format_user_error(e, include_technical=False)
        debug_log(f"å›é¥‹æ”¶é›†éŒ¯èª¤ [éŒ¯èª¤ID: {error_id}]: {e!s}")

        return [TextContent(type="text", text=user_error_msg)]


async def launch_web_feedback_ui(project_dir: str, summary: str, timeout: int) -> dict:
    """
    å•Ÿå‹• Web UI æ”¶é›†å›é¥‹ï¼Œæ”¯æ´è‡ªè¨‚è¶…æ™‚æ™‚é–“

    Args:
        project_dir: å°ˆæ¡ˆç›®éŒ„è·¯å¾‘
        summary: AI å·¥ä½œæ‘˜è¦
        timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰

    Returns:
        dict: æ”¶é›†åˆ°çš„å›é¥‹è³‡æ–™
    """
    debug_log(f"å•Ÿå‹• Web UI ä»‹é¢ï¼Œè¶…æ™‚æ™‚é–“: {timeout} ç§’")

    try:
        # ä½¿ç”¨æ–°çš„ web æ¨¡çµ„
        from .web import launch_web_feedback_ui as web_launch

        # å‚³é timeout åƒæ•¸çµ¦ Web UI
        return await web_launch(project_dir, summary, timeout)
    except ImportError as e:
        # ä½¿ç”¨çµ±ä¸€éŒ¯èª¤è™•ç†
        error_id = ErrorHandler.log_error_with_context(
            e,
            context={"operation": "Web UI æ¨¡çµ„å°å…¥", "module": "web"},
            error_type=ErrorType.DEPENDENCY,
        )
        user_error_msg = ErrorHandler.format_user_error(
            e, ErrorType.DEPENDENCY, include_technical=False
        )
        debug_log(f"Web UI æ¨¡çµ„å°å…¥å¤±æ•— [éŒ¯èª¤ID: {error_id}]: {e}")

        return {
            "command_logs": "",
            "interactive_feedback": user_error_msg,
            "images": [],
        }


@mcp.tool()
def get_system_info() -> str:
    """
    ç²å–ç³»çµ±ç’°å¢ƒè³‡è¨Š

    Returns:
        str: JSON æ ¼å¼çš„ç³»çµ±è³‡è¨Š
    """
    is_remote = is_remote_environment()
    is_wsl = is_wsl_environment()

    system_info = {
        "å¹³å°": sys.platform,
        "Python ç‰ˆæœ¬": sys.version.split()[0],
        "WSL ç’°å¢ƒ": is_wsl,
        "é ç«¯ç’°å¢ƒ": is_remote,
        "ä»‹é¢é¡å‹": "Web UI",
        "ç’°å¢ƒè®Šæ•¸": {
            "SSH_CONNECTION": os.getenv("SSH_CONNECTION"),
            "SSH_CLIENT": os.getenv("SSH_CLIENT"),
            "DISPLAY": os.getenv("DISPLAY"),
            "VSCODE_INJECTION": os.getenv("VSCODE_INJECTION"),
            "SESSIONNAME": os.getenv("SESSIONNAME"),
            "WSL_DISTRO_NAME": os.getenv("WSL_DISTRO_NAME"),
            "WSL_INTEROP": os.getenv("WSL_INTEROP"),
            "WSLENV": os.getenv("WSLENV"),
        },
    }

    return json.dumps(system_info, ensure_ascii=False, indent=2)


# ===== Autonomous Lab MCP Tools =====

@mcp.tool()
async def autolab_init(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
    idea: Annotated[str, Field(description="Research project idea and context")] = "",
    title: Annotated[str, Field(description="Paper title")] = "TITLE",
) -> str:
    """Initialize an Autonomous Lab project.

    Creates .autolab/ directory with state, profiles, and meeting log.
    Creates paper/ directory with LaTeX template.
    Creates working directories: data/, scripts/, figures/, results/.
    Opens the game-style monitoring UI in the browser.

    Call this once at the start of a new research project.

    Args:
        project_directory: Path to the project directory
        idea: The research idea, context, and any preliminary data description
        title: Working title for the paper

    Returns:
        str: Confirmation message with instructions to call autolab_next
    """
    from .lab.latex_template import create_paper_structure
    from .lab.state import init_project

    project_directory = os.path.abspath(project_directory)

    if not idea.strip():
        return "ERROR: 'idea' parameter is required. Provide the research idea and context."

    try:
        state = init_project(project_directory, idea)
        create_paper_structure(project_directory, title)

        # Start the monitoring web UI (non-blocking)
        lab_url = ""
        try:
            import time

            from .web.main import get_web_ui_manager

            manager = get_web_ui_manager()
            manager.lab_project_dir = project_directory

            if manager.server_thread is None or not manager.server_thread.is_alive():
                manager.start_server()
                time.sleep(1)

            lab_url = f"{manager.get_server_url()}/lab"
            manager.open_browser(lab_url)
        except Exception as e:
            debug_log(f"Failed to start lab UI: {e}")
            lab_url = "(failed to start)"

        return (
            f"Autonomous Lab initialized in {project_directory}\n\n"
            f"Created:\n"
            f"  .autolab/ -- state, profiles, meeting log\n"
            f"  paper/ -- LaTeX template ({title})\n"
            f"  scripts/, figures/, results/ -- working directories\n\n"
            f"State: iteration={state['iteration']}, next_role={state['next_role']}\n"
            f"Monitoring UI: {lab_url}\n\n"
            f"Next step: Call autolab_next to get the PI's first prompt."
        )
    except Exception as e:
        return f"ERROR initializing project: {e}"


@mcp.tool()
async def autolab_next(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
) -> str:
    """Get the next role prompt for the Autonomous Lab session.

    Reads the current state to determine whose turn it is (PI or Trainee),
    assembles context (idea, profiles, meeting history, file listings),
    and returns a detailed role prompt.

    The AI should then follow the returned prompt exactly, acting as
    the specified role (PI or Trainee).

    Call flow: autolab_next -> (AI acts as role) -> autolab_record -> autolab_next -> ...

    Args:
        project_directory: Path to the project directory

    Returns:
        str: The role prompt to follow
    """
    from .lab.prompts import (
        build_pi_prompt,
        build_reviewer_prompt,
        build_revision_prompt,
        build_submission_prompt,
        build_trainee_prompt,
    )
    from .lab.state import (
        get_editorial,
        get_meeting_summaries,
        get_paper_progress,
        get_recent_meetings,
        load_idea,
        load_profile,
        load_state,
        scan_project_files,
    )

    project_directory = os.path.abspath(project_directory)

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    idea = load_idea(project_directory)
    role = state["next_role"]
    iteration = state["iteration"]
    user_feedback = state.get("user_feedback", "")
    meeting_history = get_recent_meetings(project_directory, n=3)
    summaries = get_meeting_summaries(project_directory)
    file_listings = scan_project_files(project_directory)
    editorial = state.get("editorial", {})

    # --- Editorial workflow routing ---

    # 1. PI is preparing submission (ready_for_review)
    if state.get("status") == "ready_for_review" and editorial.get("phase", "none") == "none":
        paper_progress = get_paper_progress(project_directory)
        prompt = build_submission_prompt(paper_progress, file_listings, meeting_history)
        return (
            f"[AUTOLAB] SUBMISSION MODE -- Iteration {iteration}\n\n"
            f"{prompt}"
        )

    # 2. Waiting for editor (submitted / reviews_complete)
    if state.get("status") in ("submitted_to_editor", "reviews_complete"):
        phase = editorial.get("phase", "none")
        if phase == "submitted":
            return (
                f"[AUTOLAB] AWAITING EDITOR -- Iteration {iteration}\n\n"
                f"The manuscript has been submitted to the Editor.\n"
                f"The Editor (user) will now decide: Desk Reject or Invite Reviewers.\n\n"
                f"Waiting for editorial action via the monitoring UI...\n"
                f"Call autolab_next again after the editor acts."
            )
        if phase == "reviews_complete":
            return (
                f"[AUTOLAB] AWAITING EDITORIAL DECISION -- Iteration {iteration}\n\n"
                f"All reviewer reports are in.\n"
                f"The Editor (user) will now decide: Accept / Minor Revision / Major Revision / Reject.\n\n"
                f"Waiting for editorial decision via the monitoring UI...\n"
                f"Call autolab_next again after the editor acts."
            )

    # 3. Reviewer turn
    if role.startswith("reviewer_"):
        paper_progress = get_paper_progress(project_directory)
        reviewers = editorial.get("reviewers", [])
        reviewer = next((r for r in reviewers if r["id"] == role), None)
        if not reviewer:
            return f"ERROR: Reviewer {role} not found in editorial state."

        cover_letter = editorial.get("cover_letter", "")
        round_num = editorial.get("round", 1)
        prompt = build_reviewer_prompt(
            reviewer=reviewer,
            paper_progress=paper_progress,
            file_listings=file_listings,
            cover_letter=cover_letter,
            round_number=round_num,
        )
        return (
            f"[AUTOLAB] REVIEWER TURN -- {reviewer['name']} ({reviewer['role']})\n"
            f"Iteration: {iteration}, Round: {round_num}\n\n"
            f"{prompt}"
        )

    # 4. PI handling revision
    if state.get("status") in ("revision_requested", "rejected"):
        profile = load_profile(project_directory, "pi")
        reviews = editorial.get("reviews", {})
        decision = editorial.get("decision", "")
        decision_fb = editorial.get("decision_feedback", "")
        round_num = editorial.get("round", 1)
        prompt = build_revision_prompt(
            idea=idea,
            profile=profile,
            reviews=reviews,
            editorial_decision=decision,
            editorial_feedback=decision_fb,
            meeting_history=meeting_history,
            file_listings=file_listings,
            round_number=round_num,
        )
        return (
            f"[AUTOLAB] REVISION MODE -- Round {round_num}\n"
            f"Decision: {decision.upper().replace('_', ' ')}\n"
            f"Iteration: {iteration}\n\n"
            f"{prompt}"
        )

    # --- Normal lab workflow ---
    profile = load_profile(project_directory, role)

    if role == "pi":
        prompt = build_pi_prompt(
            idea=idea,
            profile=profile,
            meeting_history=meeting_history,
            summaries=summaries,
            file_listings=file_listings,
            user_feedback=user_feedback,
            iteration=iteration,
        )
    else:
        prompt = build_trainee_prompt(
            idea=idea,
            profile=profile,
            meeting_history=meeting_history,
            summaries=summaries,
            file_listings=file_listings,
            user_feedback=user_feedback,
            iteration=iteration,
        )

    return (
        f"[AUTOLAB] {role.upper()} TURN -- Iteration {iteration}\n\n"
        f"{prompt}"
    )


@mcp.tool()
async def autolab_record(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
    role: Annotated[str, Field(description="Role that just acted: 'pi' or 'trainee'")] = "",
    summary: Annotated[str, Field(description="Brief summary of what was done this turn")] = "",
    content: Annotated[str, Field(description="Full content of the turn (review, agenda, results, etc.)")] = "",
    status: Annotated[str, Field(description="Status: 'continue' or 'ready_for_review'")] = "continue",
    progress: Annotated[int, Field(description="PI overall project progress 0-100 (set only by PI)")] = -1,
) -> str:
    """Record a completed turn. Non-blocking â€” updates state and returns immediately.

    The monitoring UI (opened by autolab_init) auto-refreshes to show progress.
    User feedback is optional and picked up in the next autolab_next call.

    This tool:
    1. Saves the turn to meeting_log.md
    2. Updates state.json (advance iteration, flip role, store status)
    3. Compresses old meetings every 6 turns

    Call this AFTER completing your role actions (as PI or Trainee).

    Args:
        project_directory: Path to the project directory
        role: Which role just completed: 'pi' or 'trainee'
        summary: Brief summary of actions taken
        content: Full structured content of the turn
        status: 'continue' to keep iterating, 'ready_for_review' when paper is done
        progress: PI's assessment of overall project progress 0-100 (only PI should set this)

    Returns:
        str: User feedback text, or empty string if auto-continued
    """
    from .lab.state import (
        append_meeting_log,
        compress_old_meetings,
        load_state,
        record_review,
        save_state,
        submit_manuscript,
        COMPRESS_EVERY,
    )

    project_directory = os.path.abspath(project_directory)

    # Validate role â€” allow pi, trainee, and reviewer_N
    valid_roles = role in ("pi", "trainee") or role.startswith("reviewer_")
    if not valid_roles:
        return "ERROR: role must be 'pi', 'trainee', or 'reviewer_N'"

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    iteration = state["iteration"]

    # 1. Append to meeting log
    append_meeting_log(project_directory, role, iteration, summary, content)

    # --- Reviewer turn handling ---
    if role.startswith("reviewer_"):
        # Extract recommendation from content
        import re
        rec_match = re.search(r"RECOMMENDATION:\s*(\w[\w\s]*)", content)
        conf_match = re.search(r"CONFIDENCE[^:]*:\s*(\d)", content)
        recommendation = rec_match.group(1).strip().lower().replace(" ", "_") if rec_match else "major_revision"
        confidence = int(conf_match.group(1)) if conf_match else 3

        review_data = {
            "recommendation": recommendation,
            "confidence": confidence,
            "report": content,
        }
        editorial = record_review(project_directory, role, review_data)
        remaining = [r["id"] for r in editorial.get("reviewers", [])
                     if r["id"] not in editorial.get("reviews", {})]

        if remaining:
            return (
                f"Review by {role} recorded.\n"
                f"Next reviewer: {remaining[0]}.\n"
                f"Call autolab_next to continue."
            )
        else:
            return (
                f"Review by {role} recorded. All reviews complete.\n"
                f"Waiting for editorial decision from the Editor (user).\n"
                f"Call autolab_next to check status."
            )

    # --- PI submission handling ---
    if role == "pi" and status == "ready_for_review":
        submit_manuscript(project_directory, content)
        if progress >= 0:
            st = load_state(project_directory)
            st["progress"] = max(0, min(100, progress))
            save_state(project_directory, st)
        return (
            f"Manuscript submitted to Editor.\n"
            f"Cover letter and manuscript summary recorded.\n"
            f"Waiting for editorial action via the monitoring UI.\n"
            f"Call autolab_next to check status."
        )

    # --- Normal turn handling ---
    next_role = "trainee" if role == "pi" else "pi"
    new_iteration = iteration + (1 if role == "trainee" else 0)
    state = load_state(project_directory)  # reload in case reviewer updated it
    state["iteration"] = new_iteration
    state["next_role"] = next_role
    state["status"] = status if status != "ready_for_review" else "active"
    # PI can set overall progress (0-100)
    if progress >= 0 and role == "pi":
        state["progress"] = max(0, min(100, progress))
    save_state(project_directory, state)

    # Compress old meetings periodically
    if new_iteration > 0 and new_iteration % COMPRESS_EVERY == 0:
        compress_old_meetings(project_directory)

    # Check if user left feedback via the web UI
    user_feedback = state.get("user_feedback", "").strip()

    if user_feedback:
        return (
            f"Turn recorded. Iteration {new_iteration}, next: {next_role.upper()}.\n\n"
            f"User feedback pending:\n{user_feedback}"
        )

    return (
        f"Turn recorded. Iteration {new_iteration}, next: {next_role.upper()}.\n"
        f"Call autolab_next to continue."
    )


@mcp.tool()
async def autolab_status(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
) -> str:
    """Get the current status of an Autonomous Lab project.

    Returns the current state, file listings, paper progress,
    and a summary of recent meetings.

    Args:
        project_directory: Path to the project directory

    Returns:
        str: Formatted status report
    """
    from .lab.state import (
        get_paper_progress,
        get_recent_meetings,
        load_state,
        scan_project_files,
    )

    project_directory = os.path.abspath(project_directory)

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    file_listings = scan_project_files(project_directory)
    paper_progress = get_paper_progress(project_directory)
    recent = get_recent_meetings(project_directory, n=2)

    # Format file counts
    file_counts = {k: len(v) for k, v in file_listings.items()}

    # Format paper progress
    paper_lines = []
    for section, info in paper_progress.items():
        if info["exists"] and info["words"] > 0:
            paper_lines.append(f"  {section}: {info['words']} words")
        elif info["exists"]:
            paper_lines.append(f"  {section}: placeholder only")
        else:
            paper_lines.append(f"  {section}: not created")

    report = (
        f"# Autonomous Lab Status\n\n"
        f"**Iteration:** {state['iteration']}\n"
        f"**Next role:** {state['next_role']}\n"
        f"**Status:** {state['status']}\n"
        f"**Last updated:** {state.get('last_updated', 'unknown')}\n\n"
        f"## File Counts\n"
        f"  data: {file_counts.get('data', 0)} files\n"
        f"  scripts: {file_counts.get('scripts', 0)} files\n"
        f"  figures: {file_counts.get('figures', 0)} files\n"
        f"  results: {file_counts.get('results', 0)} files\n"
        f"  paper: {file_counts.get('paper', 0)} files\n\n"
        f"## Paper Progress\n"
        + "\n".join(paper_lines)
        + "\n\n"
        f"## Recent Meetings\n\n{recent if recent.strip() else 'No meetings yet.'}\n"
    )

    if state.get("user_feedback"):
        report += f"\n## Pending User Feedback\n\n{state['user_feedback']}\n"

    return report


# ===== Biomni Integration MCP Tools =====

@mcp.tool()
async def autolab_biomni_status(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
) -> str:
    """Check Biomni integration status.

    Returns whether Biomni is installed, its version, whether it is enabled
    for this project, and datalake availability.

    Biomni (https://github.com/snap-stanford/Biomni) is an optional
    integration that provides 100+ biomedical tools and workflows.
    It is NOT bundled with Autonomous Lab â€” users must opt in.

    Args:
        project_directory: Path to the project directory

    Returns:
        str: JSON status report
    """
    from .integrations.biomni import get_status
    project_directory = os.path.abspath(project_directory)
    status = get_status(project_directory)
    return json.dumps(status, indent=2)


@mcp.tool()
async def autolab_biomni_install(
    from_source: Annotated[bool, Field(description="Install from GitHub (True) or PyPI (False)")] = True,
) -> str:
    """Install Biomni from GitHub or PyPI.

    This downloads and installs the Biomni package at runtime.
    Biomni is Apache 2.0 licensed, but some of its integrated tools
    may have more restrictive licenses â€” review before commercial use.

    NOTE: The full datalake (~11 GB) is downloaded on first agent use.
    Set skip_datalake=True in config to skip it.

    Args:
        from_source: If True, install latest from GitHub main. If False, use PyPI.

    Returns:
        str: Installation result
    """
    from .integrations.biomni import install_biomni
    result = install_biomni(from_source=from_source)
    if result["success"]:
        return f"SUCCESS: {result['message']}"
    else:
        return f"FAILED: {result['message']}"


@mcp.tool()
async def autolab_biomni_configure(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
    enabled: Annotated[bool, Field(description="Enable Biomni integration")] = True,
    data_path: Annotated[str, Field(description="Path to Biomni datalake")] = "./data",
    skip_datalake: Annotated[bool, Field(description="Skip downloading the 11GB datalake")] = False,
) -> str:
    """Configure Biomni integration for this project.

    Saves settings to .autolab/biomni_config.json.
    Biomni must be installed separately (use autolab_biomni_install).

    NOTE: Biomni is used as a tool/database library only â€” we do NOT
    instantiate the Biomni A1 agent or use their LLM pipeline.

    Args:
        project_directory: Path to the project directory
        enabled: Whether to enable Biomni for this project
        data_path: Path for Biomni's data/datalake
        skip_datalake: If True, skip the ~11GB datalake download

    Returns:
        str: Saved configuration
    """
    from .integrations.biomni import save_biomni_config
    project_directory = os.path.abspath(project_directory)
    cfg = save_biomni_config(
        project_directory,
        enabled=enabled,
        data_path=data_path,
        skip_datalake=skip_datalake,
    )
    return f"Biomni config saved:\n{json.dumps(cfg, indent=2)}"


@mcp.tool()
async def autolab_biomni_tools(
    project_directory: Annotated[str, Field(description="Project directory path")] = ".",
) -> str:
    """List available Biomni tools and databases.

    Biomni provides curated biomedical tools (ADMET prediction,
    scRNA-seq analysis, CRISPR screen planning, etc.) and database
    connectors.  This lists what's available for import in scripts.

    NOTE: We use Biomni as a tool/database library, NOT as an agent.
    The Trainee imports and calls individual functions directly in
    their Python scripts.

    Returns:
        str: Available tools and databases, or installation instructions
    """
    from .integrations.biomni import (
        is_biomni_available,
        list_available_databases,
        list_available_tools,
    )
    if not is_biomni_available():
        return (
            "Biomni is not installed.\n"
            "Install with: autolab_biomni_install()\n"
            "See: https://github.com/snap-stanford/Biomni"
        )
    tools = list_available_tools()
    dbs = list_available_databases()

    lines = ["## Available Biomni Tools\n"]
    if tools:
        for t in tools:
            desc = f" â€” {t['description']}" if t.get('description') else ""
            lines.append(f"  - **{t['name']}** (`{t['module']}`){desc}")
    else:
        lines.append("  (no tools detected)")

    lines.append("\n## Available Biomni Databases\n")
    if dbs:
        for d in dbs:
            desc = f" â€” {d['description']}" if d.get('description') else ""
            lines.append(f"  - **{d['name']}** (`{d['module']}`){desc}")
    else:
        lines.append("  (no databases detected)")

    lines.append(
        "\n## Usage\n"
        "Import tools directly in scripts:\n"
        "```python\n"
        "from biomni.tools.<tool_name> import *\n"
        "```\n"
        "Do NOT instantiate the Biomni A1 agent."
    )
    return "\n".join(lines)


@mcp.tool()
async def autolab_biomni_env(
) -> str:
    """Check Biomni environment details.

    Shows which Biomni sub-packages are available, the active conda
    environment, datalake status, and Python path.

    Returns:
        str: Environment report
    """
    from .integrations.biomni import check_environment
    env = check_environment()
    return json.dumps(env, indent=2)


# ===== Main entry point =====
def main():
    """ä¸»è¦å…¥å£é»ï¼Œç”¨æ–¼å¥—ä»¶åŸ·è¡Œ
    æ”¶é›†ç”¨æˆ¶çš„äº’å‹•å›é¥‹ï¼Œæ”¯æ´æ–‡å­—å’Œåœ–ç‰‡
    æ­¤å·¥å…·ä½¿ç”¨ Web UI ä»‹é¢æ”¶é›†ç”¨æˆ¶å›é¥‹ï¼Œæ”¯æ´æ™ºèƒ½ç’°å¢ƒæª¢æ¸¬ã€‚

    ç”¨æˆ¶å¯ä»¥ï¼š
    1. åŸ·è¡Œå‘½ä»¤ä¾†é©—è­‰çµæœ
    2. æä¾›æ–‡å­—å›é¥‹
    3. ä¸Šå‚³åœ–ç‰‡ä½œç‚ºå›é¥‹
    4. æŸ¥çœ‹ AI çš„å·¥ä½œæ‘˜è¦

    èª¿è©¦æ¨¡å¼ï¼š
    - è¨­ç½®ç’°å¢ƒè®Šæ•¸ MCP_DEBUG=true å¯å•Ÿç”¨è©³ç´°èª¿è©¦è¼¸å‡º
    - ç”Ÿç”¢ç’°å¢ƒå»ºè­°é—œé–‰èª¿è©¦æ¨¡å¼ä»¥é¿å…è¼¸å‡ºå¹²æ“¾


    """
    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨èª¿è©¦æ¨¡å¼
    debug_enabled = os.getenv("MCP_DEBUG", "").lower() in ("true", "1", "yes", "on")

    if debug_enabled:
        debug_log("ğŸš€ å•Ÿå‹•äº’å‹•å¼å›é¥‹æ”¶é›† MCP æœå‹™å™¨")
        debug_log(f"   æœå‹™å™¨åç¨±: {SERVER_NAME}")
        debug_log(f"   ç‰ˆæœ¬: {__version__}")
        debug_log(f"   å¹³å°: {sys.platform}")
        debug_log(f"   ç·¨ç¢¼åˆå§‹åŒ–: {'æˆåŠŸ' if _encoding_initialized else 'å¤±æ•—'}")
        debug_log(f"   é ç«¯ç’°å¢ƒ: {is_remote_environment()}")
        debug_log(f"   WSL ç’°å¢ƒ: {is_wsl_environment()}")
        debug_log("   Interface: Web UI")
        debug_log("   ç­‰å¾…ä¾†è‡ª AI åŠ©æ‰‹çš„èª¿ç”¨...")
        debug_log("æº–å‚™å•Ÿå‹• MCP ä¼ºæœå™¨...")
        debug_log("èª¿ç”¨ mcp.run()...")

    try:
        # ä½¿ç”¨æ­£ç¢ºçš„ FastMCP API
        mcp.run()
    except KeyboardInterrupt:
        if debug_enabled:
            debug_log("æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£å¸¸é€€å‡º")
        sys.exit(0)
    except Exception as e:
        if debug_enabled:
            debug_log(f"MCP æœå‹™å™¨å•Ÿå‹•å¤±æ•—: {e}")
            import traceback

            debug_log(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
