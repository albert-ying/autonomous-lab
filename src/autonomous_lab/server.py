#!/usr/bin/env python3
"""
MCP Feedback Enhanced ä¼ºæœå™¨ä¸»è¦æ¨¡çµ„

æ­¤æ¨¡çµ„æä¾› MCP (Model Context Protocol) çš„å¢å¼·å›é¥‹æ”¶é›†åŠŸèƒ½ï¼Œ
æ”¯æ´æ™ºèƒ½ç’°å¢ƒæª¢æ¸¬ï¼Œè‡ªå‹•ä½¿ç”¨ Web UI ä»‹é¢ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- MCP å·¥å…·å¯¦ç¾
- ä»‹é¢é¸æ“‡ï¼ˆWeb UIï¼‰
- ç’°å¢ƒæª¢æ¸¬ (SSH Remote, WSL, Local)
- åœ‹éš›åŒ–æ”¯æ´
- åœ–ç‰‡è™•ç†èˆ‡ä¸Šå‚³
- å‘½ä»¤åŸ·è¡Œèˆ‡çµæœå±•ç¤º
- å°ˆæ¡ˆç›®éŒ„ç®¡ç†

ä¸»è¦ MCP å·¥å…·ï¼š
- interactive_feedback: æ”¶é›†ç”¨æˆ¶äº’å‹•å›é¥‹
- get_system_info: ç²å–ç³»çµ±ç’°å¢ƒè³‡è¨Š

ä½œè€…: FÃ¡bio Ferreira (åŸä½œè€…)
å¢å¼·: Minidoracat (Web UI, åœ–ç‰‡æ”¯æ´, ç’°å¢ƒæª¢æ¸¬)
é‡æ§‹: æ¨¡å¡ŠåŒ–è¨­è¨ˆ
"""

import base64
import io
import json
import os
import sys
from typing import Annotated, Any

from fastmcp import FastMCP
from fastmcp.utilities.types import Image as MCPImage
from mcp.types import TextContent
from pydantic import Field

# å°å…¥çµ±ä¸€çš„èª¿è©¦åŠŸèƒ½
from .debug import server_debug_log as debug_log

# å°å…¥å¤šèªç³»æ”¯æ´
# å°å…¥éŒ¯èª¤è™•ç†æ¡†æ¶
from .utils.error_handler import ErrorHandler, ErrorType

# å°å…¥è³‡æºç®¡ç†å™¨
from .utils.resource_manager import create_temp_file


# ===== ç·¨ç¢¼åˆå§‹åŒ– =====
def init_encoding():
    """åˆå§‹åŒ–ç·¨ç¢¼è¨­ç½®ï¼Œç¢ºä¿æ­£ç¢ºè™•ç†ä¸­æ–‡å­—ç¬¦"""
    try:
        # Windows ç‰¹æ®Šè™•ç†
        if sys.platform == "win32":
            import msvcrt

            # è¨­ç½®ç‚ºäºŒé€²åˆ¶æ¨¡å¼
            msvcrt.setmode(sys.stdin.fileno(), os.O_BINARY)
            msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)

            # é‡æ–°åŒ…è£ç‚º UTF-8 æ–‡æœ¬æµï¼Œä¸¦ç¦ç”¨ç·©è¡
            # ä¿®å¾© union-attr éŒ¯èª¤ - å®‰å…¨ç²å– buffer æˆ– detach
            stdin_buffer = getattr(sys.stdin, "buffer", None)
            if stdin_buffer is None and hasattr(sys.stdin, "detach"):
                stdin_buffer = sys.stdin.detach()

            stdout_buffer = getattr(sys.stdout, "buffer", None)
            if stdout_buffer is None and hasattr(sys.stdout, "detach"):
                stdout_buffer = sys.stdout.detach()

            sys.stdin = io.TextIOWrapper(
                stdin_buffer, encoding="utf-8", errors="replace", newline=None
            )
            sys.stdout = io.TextIOWrapper(
                stdout_buffer,
                encoding="utf-8",
                errors="replace",
                newline="",
                write_through=True,  # é—œéµï¼šç¦ç”¨å¯«å…¥ç·©è¡
            )
        else:
            # é Windows ç³»çµ±çš„æ¨™æº–è¨­ç½®
            if hasattr(sys.stdout, "reconfigure"):
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stdin, "reconfigure"):
                sys.stdin.reconfigure(encoding="utf-8", errors="replace")

        # è¨­ç½® stderr ç·¨ç¢¼ï¼ˆç”¨æ–¼èª¿è©¦è¨Šæ¯ï¼‰
        if hasattr(sys.stderr, "reconfigure"):
            sys.stderr.reconfigure(encoding="utf-8", errors="replace")

        return True
    except Exception:
        # å¦‚æœç·¨ç¢¼è¨­ç½®å¤±æ•—ï¼Œå˜—è©¦åŸºæœ¬è¨­ç½®
        try:
            if hasattr(sys.stdout, "reconfigure"):
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stdin, "reconfigure"):
                sys.stdin.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stderr, "reconfigure"):
                sys.stderr.reconfigure(encoding="utf-8", errors="replace")
        except:
            pass
        return False


# åˆå§‹åŒ–ç·¨ç¢¼ï¼ˆåœ¨å°å…¥æ™‚å°±åŸ·è¡Œï¼‰
_encoding_initialized = init_encoding()

# ===== å¸¸æ•¸å®šç¾© =====
SERVER_NAME = "Autonomous Lab MCP"
SSH_ENV_VARS = ["SSH_CONNECTION", "SSH_CLIENT", "SSH_TTY"]
REMOTE_ENV_VARS = ["REMOTE_CONTAINERS", "CODESPACES"]


# åˆå§‹åŒ– MCP æœå‹™å™¨
from . import __version__


# ç¢ºä¿ log_level è¨­å®šç‚ºæ­£ç¢ºçš„å¤§å¯«æ ¼å¼
fastmcp_settings = {}

# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ä¸¦è¨­å®šæ­£ç¢ºçš„ log_level
env_log_level = os.getenv("FASTMCP_LOG_LEVEL", "").upper()
if env_log_level in ("DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"):
    fastmcp_settings["log_level"] = env_log_level
else:
    # é è¨­ä½¿ç”¨ INFO ç­‰ç´š
    fastmcp_settings["log_level"] = "INFO"

mcp: Any = FastMCP(SERVER_NAME)


# ===== å·¥å…·å‡½æ•¸ =====
def is_wsl_environment() -> bool:
    """
    æª¢æ¸¬æ˜¯å¦åœ¨ WSL (Windows Subsystem for Linux) ç’°å¢ƒä¸­é‹è¡Œ

    Returns:
        bool: True è¡¨ç¤º WSL ç’°å¢ƒï¼ŒFalse è¡¨ç¤ºå…¶ä»–ç’°å¢ƒ
    """
    try:
        # æª¢æŸ¥ /proc/version æ–‡ä»¶æ˜¯å¦åŒ…å« WSL æ¨™è­˜
        if os.path.exists("/proc/version"):
            with open("/proc/version") as f:
                version_info = f.read().lower()
                if "microsoft" in version_info or "wsl" in version_info:
                    debug_log("åµæ¸¬åˆ° WSL ç’°å¢ƒï¼ˆé€šé /proc/versionï¼‰")
                    return True

        # æª¢æŸ¥ WSL ç›¸é—œç’°å¢ƒè®Šæ•¸
        wsl_env_vars = ["WSL_DISTRO_NAME", "WSL_INTEROP", "WSLENV"]
        for env_var in wsl_env_vars:
            if os.getenv(env_var):
                debug_log(f"åµæ¸¬åˆ° WSL ç’°å¢ƒè®Šæ•¸: {env_var}")
                return True

        # æª¢æŸ¥æ˜¯å¦å­˜åœ¨ WSL ç‰¹æœ‰çš„è·¯å¾‘
        wsl_paths = ["/mnt/c", "/mnt/d", "/proc/sys/fs/binfmt_misc/WSLInterop"]
        for path in wsl_paths:
            if os.path.exists(path):
                debug_log(f"åµæ¸¬åˆ° WSL ç‰¹æœ‰è·¯å¾‘: {path}")
                return True

    except Exception as e:
        debug_log(f"WSL æª¢æ¸¬éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

    return False


def is_remote_environment() -> bool:
    """
    æª¢æ¸¬æ˜¯å¦åœ¨é ç«¯ç’°å¢ƒä¸­é‹è¡Œ

    Returns:
        bool: True è¡¨ç¤ºé ç«¯ç’°å¢ƒï¼ŒFalse è¡¨ç¤ºæœ¬åœ°ç’°å¢ƒ
    """
    # WSL ä¸æ‡‰è¢«è¦–ç‚ºé ç«¯ç’°å¢ƒï¼Œå› ç‚ºå®ƒå¯ä»¥è¨ªå• Windows ç€è¦½å™¨
    if is_wsl_environment():
        debug_log("WSL ç’°å¢ƒä¸è¢«è¦–ç‚ºé ç«¯ç’°å¢ƒ")
        return False

    # æª¢æŸ¥ SSH é€£ç·šæŒ‡æ¨™
    for env_var in SSH_ENV_VARS:
        if os.getenv(env_var):
            debug_log(f"åµæ¸¬åˆ° SSH ç’°å¢ƒè®Šæ•¸: {env_var}")
            return True

    # æª¢æŸ¥é ç«¯é–‹ç™¼ç’°å¢ƒ
    for env_var in REMOTE_ENV_VARS:
        if os.getenv(env_var):
            debug_log(f"åµæ¸¬åˆ°é ç«¯é–‹ç™¼ç’°å¢ƒ: {env_var}")
            return True

    # æª¢æŸ¥ Docker å®¹å™¨
    if os.path.exists("/.dockerenv"):
        debug_log("åµæ¸¬åˆ° Docker å®¹å™¨ç’°å¢ƒ")
        return True

    # Windows é ç«¯æ¡Œé¢æª¢æŸ¥
    if sys.platform == "win32":
        session_name = os.getenv("SESSIONNAME", "")
        if session_name and "RDP" in session_name:
            debug_log(f"åµæ¸¬åˆ° Windows é ç«¯æ¡Œé¢: {session_name}")
            return True

    # Linux ç„¡é¡¯ç¤ºç’°å¢ƒæª¢æŸ¥ï¼ˆä½†æ’é™¤ WSLï¼‰
    if (
        sys.platform.startswith("linux")
        and not os.getenv("DISPLAY")
        and not is_wsl_environment()
    ):
        debug_log("åµæ¸¬åˆ° Linux ç„¡é¡¯ç¤ºç’°å¢ƒ")
        return True

    return False


def save_feedback_to_file(feedback_data: dict, file_path: str | None = None) -> str:
    """
    å°‡å›é¥‹è³‡æ–™å„²å­˜åˆ° JSON æ–‡ä»¶

    Args:
        feedback_data: å›é¥‹è³‡æ–™å­—å…¸
        file_path: å„²å­˜è·¯å¾‘ï¼Œè‹¥ç‚º None å‰‡è‡ªå‹•ç”¢ç”Ÿè‡¨æ™‚æ–‡ä»¶

    Returns:
        str: å„²å­˜çš„æ–‡ä»¶è·¯å¾‘
    """
    if file_path is None:
        # ä½¿ç”¨è³‡æºç®¡ç†å™¨å‰µå»ºè‡¨æ™‚æ–‡ä»¶
        file_path = create_temp_file(suffix=".json", prefix="feedback_")

    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

    # è¤‡è£½æ•¸æ“šä»¥é¿å…ä¿®æ”¹åŸå§‹æ•¸æ“š
    json_data = feedback_data.copy()

    # è™•ç†åœ–ç‰‡æ•¸æ“šï¼šå°‡ bytes è½‰æ›ç‚º base64 å­—ç¬¦ä¸²ä»¥ä¾¿ JSON åºåˆ—åŒ–
    if "images" in json_data and isinstance(json_data["images"], list):
        processed_images = []
        for img in json_data["images"]:
            if isinstance(img, dict) and "data" in img:
                processed_img = img.copy()
                # å¦‚æœ data æ˜¯ bytesï¼Œè½‰æ›ç‚º base64 å­—ç¬¦ä¸²
                if isinstance(img["data"], bytes):
                    processed_img["data"] = base64.b64encode(img["data"]).decode(
                        "utf-8"
                    )
                    processed_img["data_type"] = "base64"
                processed_images.append(processed_img)
            else:
                processed_images.append(img)
        json_data["images"] = processed_images

    # å„²å­˜è³‡æ–™
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    debug_log(f"å›é¥‹è³‡æ–™å·²å„²å­˜è‡³: {file_path}")
    return file_path


def create_feedback_text(feedback_data: dict) -> str:
    """
    å»ºç«‹æ ¼å¼åŒ–çš„å›é¥‹æ–‡å­—

    Args:
        feedback_data: å›é¥‹è³‡æ–™å­—å…¸

    Returns:
        str: æ ¼å¼åŒ–å¾Œçš„å›é¥‹æ–‡å­—
    """
    text_parts = []

    # åŸºæœ¬å›é¥‹å…§å®¹
    if feedback_data.get("interactive_feedback"):
        text_parts.append(f"=== ç”¨æˆ¶å›é¥‹ ===\n{feedback_data['interactive_feedback']}")

    # å‘½ä»¤åŸ·è¡Œæ—¥èªŒ
    if feedback_data.get("command_logs"):
        text_parts.append(f"=== å‘½ä»¤åŸ·è¡Œæ—¥èªŒ ===\n{feedback_data['command_logs']}")

    # åœ–ç‰‡é™„ä»¶æ¦‚è¦
    if feedback_data.get("images"):
        images = feedback_data["images"]
        text_parts.append(f"=== åœ–ç‰‡é™„ä»¶æ¦‚è¦ ===\nç”¨æˆ¶æä¾›äº† {len(images)} å¼µåœ–ç‰‡ï¼š")

        for i, img in enumerate(images, 1):
            size = img.get("size", 0)
            name = img.get("name", "unknown")

            # æ™ºèƒ½å–®ä½é¡¯ç¤º
            if size < 1024:
                size_str = f"{size} B"
            elif size < 1024 * 1024:
                size_kb = size / 1024
                size_str = f"{size_kb:.1f} KB"
            else:
                size_mb = size / (1024 * 1024)
                size_str = f"{size_mb:.1f} MB"

            img_info = f"  {i}. {name} ({size_str})"

            # ç‚ºæé«˜å…¼å®¹æ€§ï¼Œæ·»åŠ  base64 é è¦½ä¿¡æ¯
            if img.get("data"):
                try:
                    if isinstance(img["data"], bytes):
                        img_base64 = base64.b64encode(img["data"]).decode("utf-8")
                    elif isinstance(img["data"], str):
                        img_base64 = img["data"]
                    else:
                        img_base64 = None

                    if img_base64:
                        # åªé¡¯ç¤ºå‰50å€‹å­—ç¬¦çš„é è¦½
                        preview = (
                            img_base64[:50] + "..."
                            if len(img_base64) > 50
                            else img_base64
                        )
                        img_info += f"\n     Base64 é è¦½: {preview}"
                        img_info += f"\n     å®Œæ•´ Base64 é•·åº¦: {len(img_base64)} å­—ç¬¦"

                        # å¦‚æœ AI åŠ©æ‰‹ä¸æ”¯æ´ MCP åœ–ç‰‡ï¼Œå¯ä»¥æä¾›å®Œæ•´ base64
                        debug_log(f"åœ–ç‰‡ {i} Base64 å·²æº–å‚™ï¼Œé•·åº¦: {len(img_base64)}")

                        # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨ Base64 è©³ç´°æ¨¡å¼ï¼ˆå¾ UI è¨­å®šä¸­ç²å–ï¼‰
                        include_full_base64 = feedback_data.get("settings", {}).get(
                            "enable_base64_detail", False
                        )

                        if include_full_base64:
                            # æ ¹æ“šæª”æ¡ˆåæ¨æ–· MIME é¡å‹
                            file_name = img.get("name", "image.png")
                            if file_name.lower().endswith((".jpg", ".jpeg")):
                                mime_type = "image/jpeg"
                            elif file_name.lower().endswith(".gif"):
                                mime_type = "image/gif"
                            elif file_name.lower().endswith(".webp"):
                                mime_type = "image/webp"
                            else:
                                mime_type = "image/png"

                            img_info += f"\n     å®Œæ•´ Base64: data:{mime_type};base64,{img_base64}"

                except Exception as e:
                    debug_log(f"åœ–ç‰‡ {i} Base64 è™•ç†å¤±æ•—: {e}")

            text_parts.append(img_info)

        # æ·»åŠ å…¼å®¹æ€§èªªæ˜
        text_parts.append(
            "\nğŸ’¡ æ³¨æ„ï¼šå¦‚æœ AI åŠ©æ‰‹ç„¡æ³•é¡¯ç¤ºåœ–ç‰‡ï¼Œåœ–ç‰‡æ•¸æ“šå·²åŒ…å«åœ¨ä¸Šè¿° Base64 ä¿¡æ¯ä¸­ã€‚"
        )

    return "\n\n".join(text_parts) if text_parts else "ç”¨æˆ¶æœªæä¾›ä»»ä½•å›é¥‹å…§å®¹ã€‚"


def process_images(images_data: list[dict]) -> list[MCPImage]:
    """
    è™•ç†åœ–ç‰‡è³‡æ–™ï¼Œè½‰æ›ç‚º MCP åœ–ç‰‡å°è±¡

    Args:
        images_data: åœ–ç‰‡è³‡æ–™åˆ—è¡¨

    Returns:
        List[MCPImage]: MCP åœ–ç‰‡å°è±¡åˆ—è¡¨
    """
    mcp_images = []

    for i, img in enumerate(images_data, 1):
        try:
            if not img.get("data"):
                debug_log(f"åœ–ç‰‡ {i} æ²’æœ‰è³‡æ–™ï¼Œè·³é")
                continue

            # æª¢æŸ¥æ•¸æ“šé¡å‹ä¸¦ç›¸æ‡‰è™•ç†
            if isinstance(img["data"], bytes):
                # å¦‚æœæ˜¯åŸå§‹ bytes æ•¸æ“šï¼Œç›´æ¥ä½¿ç”¨
                image_bytes = img["data"]
                debug_log(
                    f"åœ–ç‰‡ {i} ä½¿ç”¨åŸå§‹ bytes æ•¸æ“šï¼Œå¤§å°: {len(image_bytes)} bytes"
                )
            elif isinstance(img["data"], str):
                # å¦‚æœæ˜¯ base64 å­—ç¬¦ä¸²ï¼Œé€²è¡Œè§£ç¢¼
                image_bytes = base64.b64decode(img["data"])
                debug_log(f"åœ–ç‰‡ {i} å¾ base64 è§£ç¢¼ï¼Œå¤§å°: {len(image_bytes)} bytes")
            else:
                debug_log(f"åœ–ç‰‡ {i} æ•¸æ“šé¡å‹ä¸æ”¯æ´: {type(img['data'])}")
                continue

            if len(image_bytes) == 0:
                debug_log(f"åœ–ç‰‡ {i} æ•¸æ“šç‚ºç©ºï¼Œè·³é")
                continue

            # æ ¹æ“šæ–‡ä»¶åæ¨æ–·æ ¼å¼
            file_name = img.get("name", "image.png")
            if file_name.lower().endswith((".jpg", ".jpeg")):
                image_format = "jpeg"
            elif file_name.lower().endswith(".gif"):
                image_format = "gif"
            else:
                image_format = "png"  # é»˜èªä½¿ç”¨ PNG

            # å‰µå»º MCPImage å°è±¡
            mcp_image = MCPImage(data=image_bytes, format=image_format)
            mcp_images.append(mcp_image)

            debug_log(f"åœ–ç‰‡ {i} ({file_name}) è™•ç†æˆåŠŸï¼Œæ ¼å¼: {image_format}")

        except Exception as e:
            # ä½¿ç”¨çµ±ä¸€éŒ¯èª¤è™•ç†ï¼ˆä¸å½±éŸ¿ JSON RPCï¼‰
            error_id = ErrorHandler.log_error_with_context(
                e,
                context={"operation": "åœ–ç‰‡è™•ç†", "image_index": i},
                error_type=ErrorType.FILE_IO,
            )
            debug_log(f"åœ–ç‰‡ {i} è™•ç†å¤±æ•— [éŒ¯èª¤ID: {error_id}]: {e}")

    debug_log(f"å…±è™•ç† {len(mcp_images)} å¼µåœ–ç‰‡")
    return mcp_images


# ===== MCP å·¥å…·å®šç¾© =====
@mcp.tool()
async def lab_meeting(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
    summary: Annotated[
        str, Field(description="Summary of AI work completed for user review")
    ] = "Completed the requested task.",
    timeout: Annotated[
        int, Field(description="Pause duration in seconds (default: 15)")
    ] = 15,
) -> str:
    """Pause briefly between lab turns to show the monitoring UI and collect optional feedback.

    This opens/refreshes the game-style lab monitoring UI in the browser.
    It pauses for a short period (default 15s) during which the user can
    optionally type feedback through the lab UI's feedback panel.

    Call this after EVERY autolab_record call during normal lab turns.

    LOOP PROTOCOL (normal turns):
      autolab_next -> (act as role) -> autolab_record -> lab_meeting -> autolab_next -> ...

    DO NOT use this for editorial submission waits. Use autolab_editorial instead.

    After this tool returns, ALWAYS call autolab_next to continue.
    NEVER stop the loop.

    Args:
        project_directory: Project directory path
        summary: Summary of AI work completed (displayed in the status bar)
        timeout: How long to pause for user feedback (seconds, default 15)

    Returns:
        str: Any user feedback collected, or a message to continue the loop
    """
    import asyncio

    try:
        if not os.path.exists(project_directory):
            project_directory = os.getcwd()
        project_directory = os.path.abspath(project_directory)

        # Ensure the game-style lab UI is open
        _ensure_lab_ui(project_directory)

        from .lab.state import drain_feedback_queue, has_pending_feedback

        debug_log(f"lab_meeting: pausing {timeout}s for optional feedback")

        # Wait for the pause window, checking periodically for new feedback.
        # Messages accumulate in the queue â€” we don't clear anything yet.
        poll_interval = 3  # seconds
        elapsed = 0
        while elapsed < timeout:
            await asyncio.sleep(min(poll_interval, timeout - elapsed))
            elapsed += poll_interval
            try:
                if has_pending_feedback(project_directory):
                    # Give user a moment to finish typing follow-up messages
                    await asyncio.sleep(2)
                    break
            except Exception:
                continue

        # Drain ALL accumulated messages at once
        user_feedback = drain_feedback_queue(project_directory)

        if user_feedback:
            debug_log(f"lab_meeting: got user feedback ({len(user_feedback)} chars)")
            return (
                f"User feedback received:\n{user_feedback}\n\n"
                f"Incorporate this feedback into the next turn.\n"
                f"Continue the lab loop: call autolab_next now."
            )

        debug_log("lab_meeting: no feedback, continuing")
        return "No user feedback. Continue the lab loop: call autolab_next now."

    except Exception as e:
        debug_log(f"lab_meeting error: {e}")
        return f"Lab meeting error (non-fatal): {e}\n\nContinue the lab loop: call autolab_next now."


@mcp.tool()
def get_system_info() -> str:
    """
    ç²å–ç³»çµ±ç’°å¢ƒè³‡è¨Š

    Returns:
        str: JSON æ ¼å¼çš„ç³»çµ±è³‡è¨Š
    """
    is_remote = is_remote_environment()
    is_wsl = is_wsl_environment()

    system_info = {
        "å¹³å°": sys.platform,
        "Python ç‰ˆæœ¬": sys.version.split()[0],
        "WSL ç’°å¢ƒ": is_wsl,
        "é ç«¯ç’°å¢ƒ": is_remote,
        "ä»‹é¢é¡å‹": "Web UI",
        "ç’°å¢ƒè®Šæ•¸": {
            "SSH_CONNECTION": os.getenv("SSH_CONNECTION"),
            "SSH_CLIENT": os.getenv("SSH_CLIENT"),
            "DISPLAY": os.getenv("DISPLAY"),
            "VSCODE_INJECTION": os.getenv("VSCODE_INJECTION"),
            "SESSIONNAME": os.getenv("SESSIONNAME"),
            "WSL_DISTRO_NAME": os.getenv("WSL_DISTRO_NAME"),
            "WSL_INTEROP": os.getenv("WSL_INTEROP"),
            "WSLENV": os.getenv("WSLENV"),
        },
    }

    return json.dumps(system_info, ensure_ascii=False, indent=2)


# ===== Autonomous Lab MCP Tools =====

# Mandatory loop instruction for NORMAL turns (PI/Trainee/Reviewer)
_LOOP_INSTRUCTION = (
    "\n\n---\n"
    "MANDATORY NEXT STEPS (you MUST do both, in order):\n"
    "1. Call lab_meeting NOW with a brief summary of what just happened.\n"
    "2. After lab_meeting returns, call autolab_next to continue the lab session.\n"
    "NEVER stop or end your turn. The lab loop runs until the user explicitly says 'stop'."
)

# Editorial instruction for submission / waiting-for-editor scenarios
_EDITORIAL_INSTRUCTION = (
    "\n\n---\n"
    "MANDATORY NEXT STEP:\n"
    "Call autolab_editorial NOW. It will wait (with no timeout) for the Editor's action.\n"
    "When it returns, follow its instructions to continue.\n"
    "NEVER stop or end your turn."
)


_lab_browser_opened: set[str] = (
    set()
)  # tracks which project dirs already have a browser tab


def _ensure_lab_ui(project_directory: str, force_open: bool = False) -> None:
    """Ensure the monitoring web UI is running and points to this project.

    Only opens a new browser tab ONCE per project per MCP session.
    Subsequent calls just update lab_project_dir without spawning tabs.
    Set force_open=True to open the browser even if already opened (e.g., autolab_resume).

    If the web server thread died (crash, port conflict, etc.) this will
    restart it automatically.
    """
    try:
        import time
        import urllib.parse

        from .web.main import get_web_ui_manager

        manager = get_web_ui_manager()
        manager.lab_project_dir = project_directory

        # Start or restart the server if not running
        server_was_dead = (
            manager.server_thread is None or not manager.server_thread.is_alive()
        )
        if server_was_dead:
            debug_log("Lab UI server not running â€” (re)starting...")
            manager.start_server()
            # Wait with retries for the server to become healthy
            for attempt in range(5):
                time.sleep(1)
                try:
                    import urllib.request

                    health_url = f"{manager.get_server_url()}/api/autolab/state"
                    urllib.request.urlopen(health_url, timeout=2)
                    debug_log(f"Lab UI server healthy after {attempt + 1}s")
                    break
                except Exception:
                    if attempt == 4:
                        debug_log("Lab UI server failed health check after 5s")

        # Only open browser once per project (or on force_open, or after restart)
        if (
            project_directory not in _lab_browser_opened
            or force_open
            or server_was_dead
        ):
            encoded = urllib.parse.quote(project_directory, safe="")
            lab_url = f"{manager.get_server_url()}/lab?project={encoded}"
            manager.open_browser(lab_url)
            _lab_browser_opened.add(project_directory)
            debug_log(f"Lab UI browser opened for {project_directory}")
        else:
            debug_log("Lab UI already open, skipping browser launch")
    except Exception as e:
        debug_log(f"Lab UI ensure failed (non-fatal): {e}")


@mcp.tool()
async def autolab_init(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
    idea: Annotated[str, Field(description="Research project idea and context")] = "",
    title: Annotated[str, Field(description="Paper title")] = "TITLE",
    force_new: Annotated[
        bool, Field(description="Force fresh start even if project exists")
    ] = False,
    domain: Annotated[
        str,
        Field(
            description="Project domain: 'research' (default), 'software', 'consulting', 'legal', 'medical', or 'creative'. "
            "Controls role labels (e.g., PI/Trainee/Editor vs Tech Lead/Developer/Code Reviewer) "
            "and artifact names (Paper vs Pull Request vs Report)."
        ),
    ] = "research",
) -> str:
    """Initialize an Autonomous Lab project.

    Creates .autolab/ directory with state, profiles, and meeting log.
    Creates paper/ directory with LaTeX template.
    Creates working directories: data/, scripts/, figures/, results/.
    Opens the game-style monitoring UI in the browser.

    If an existing project is found, returns a warning and suggests
    calling autolab_resume instead. Set force_new=True to overwrite.

    Args:
        project_directory: Path to the project directory
        idea: The research idea, context, and any preliminary data description
        title: Working title for the paper
        force_new: If True, overwrite existing project. Default False.
        domain: Project domain controlling role labels and artifact names.

    Returns:
        str: Confirmation message with instructions to call autolab_next
    """
    from .lab.latex_template import create_paper_structure
    from .lab.state import init_project

    project_directory = os.path.abspath(project_directory)

    if not idea.strip():
        return "ERROR: 'idea' parameter is required. Provide the research idea and context."

    # Check if an existing project is present â€” suggest resume instead of overwriting
    autolab_dir = os.path.join(project_directory, ".autolab")
    state_file = os.path.join(autolab_dir, "state.json")
    if os.path.exists(state_file) and not force_new:
        try:
            with open(state_file, "r") as f:
                existing = json.load(f)
            iteration = existing.get("iteration", 0)
            status = existing.get("status", "active")
            role = existing.get("next_role", "pi")
            progress = existing.get("progress", 0)
            return (
                f"WARNING: An existing Autonomous Lab project was found in {project_directory}\n\n"
                f"  Iteration: {iteration}, Status: {status}, Next role: {role}, Progress: {progress}%\n\n"
                f"To RESUME the existing project, call autolab_resume(project_directory='{project_directory}').\n"
                f"To START FRESH (this will overwrite the existing project), call autolab_init again "
                f"with force_new=True.\n\n"
                f"Do NOT reinitialize unless the user explicitly asks for a fresh start."
            )
        except Exception:
            pass  # Corrupted state â€” safe to reinitialize

    # Validate domain
    valid_domains = (
        "research",
        "software",
        "consulting",
        "legal",
        "medical",
        "creative",
    )
    if domain not in valid_domains:
        return f"ERROR: Invalid domain '{domain}'. Must be one of: {', '.join(valid_domains)}"

    try:
        state = init_project(
            project_directory, idea, domain=domain,
            config={"title": title},
        )
        create_paper_structure(project_directory, title)

        # Start the monitoring web UI (non-blocking)
        lab_url = ""
        try:
            _ensure_lab_ui(project_directory)
            from .web.main import get_web_ui_manager

            lab_url = f"{get_web_ui_manager().get_server_url()}/lab"
        except Exception as e:
            debug_log(f"Failed to start lab UI: {e}")
            lab_url = "(failed to start)"

        from .lab.state import get_domain_config

        dcfg = get_domain_config(project_directory)

        return (
            f"Autonomous Lab initialized in {project_directory}\n\n"
            f"Domain: {domain} â€” roles: {dcfg['senior_label']}, {dcfg['junior_label']}, {dcfg['overseer_label']}\n"
            f"Artifact: {dcfg['artifact']}\n\n"
            f"Created:\n"
            f"  .autolab/ -- state, profiles, meeting log\n"
            f"  paper/ -- LaTeX template ({title})\n"
            f"  scripts/, figures/, results/ -- working directories\n\n"
            f"State: iteration={state['iteration']}, next_role={state['next_role']}\n"
            f"Monitoring UI: {lab_url}" + _LOOP_INSTRUCTION
        )
    except Exception as e:
        return f"ERROR initializing project: {e}"


@mcp.tool()
async def autolab_resume(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
) -> str:
    """Resume an interrupted Autonomous Lab session.

    Call this when the AI agent was disconnected, the process crashed,
    or a new conversation is started for an existing project.

    This tool:
    1. Loads the full saved state (iteration, role, status, progress, editorial)
    2. Reads the last several meeting log entries to reconstruct context
    3. Lists all existing files (scripts, figures, paper sections)
    4. Opens the monitoring UI
    5. Returns a comprehensive briefing so the AI can continue seamlessly

    After this returns, follow the MANDATORY NEXT STEPS to re-enter the loop.

    Args:
        project_directory: Path to the project directory with .autolab/

    Returns:
        str: Full recovery briefing + instructions to continue
    """
    from .lab.state import (
        get_domain_config,
        get_editorial,
        get_meeting_summaries,
        get_paper_progress,
        get_recent_meetings,
        load_idea,
        load_state,
        scan_project_files,
    )

    project_directory = os.path.abspath(project_directory)

    # Verify the project exists
    autolab_dir = os.path.join(project_directory, ".autolab")
    if not os.path.exists(autolab_dir):
        return (
            f"ERROR: No Autonomous Lab project found in {project_directory}\n"
            f"(.autolab/ directory does not exist)\n\n"
            f"To start a new project, call autolab_init instead."
        )

    try:
        state = load_state(project_directory)
    except Exception as e:
        return f"ERROR loading state: {e}"

    # Open the monitoring UI (force open since this is an explicit resume)
    _ensure_lab_ui(project_directory, force_open=True)

    dcfg = get_domain_config(project_directory)

    # Gather context
    idea = load_idea(project_directory)
    iteration = state.get("iteration", 0)
    role = state.get("next_role", "pi")
    status = state.get("status", "active")
    progress = state.get("progress", 0)
    experts = state.get("experts", [])
    editorial = state.get("editorial", {})
    created_at = state.get("created_at", "unknown")

    # Recent meeting history (more than normal â€” 5 entries for recovery context)
    recent_meetings = get_recent_meetings(project_directory, n=5)
    summaries = get_meeting_summaries(project_directory)
    file_listings = scan_project_files(project_directory)
    paper_progress = get_paper_progress(project_directory)

    # Build file listing summary
    file_summary_parts = []
    for category, files in file_listings.items():
        if files:
            file_summary_parts.append(
                f"  {category}/: {len(files)} file(s) â€” {', '.join(files[:5])}"
            )
    file_summary = (
        "\n".join(file_summary_parts) if file_summary_parts else "  (no files yet)"
    )

    # Build paper progress summary
    paper_parts = []
    for section, info in paper_progress.items():
        word_count = info.get("words", 0)
        paper_parts.append(
            f"  {section}: {word_count} words"
            if word_count > 0
            else f"  {section}: not written"
        )
    paper_summary = (
        "\n".join(paper_parts) if paper_parts else "  (no paper sections yet)"
    )

    # Build editorial status
    editorial_phase = editorial.get("phase", "none")
    editorial_summary = ""
    if editorial_phase != "none":
        editorial_summary = (
            f"\n\nEditorial Status:\n"
            f"  Phase: {editorial_phase}\n"
            f"  Round: {editorial.get('round', 1)}\n"
            f"  Decision: {editorial.get('decision', 'pending')}\n"
        )
        if editorial.get("reviewers"):
            rev_names = [r.get("name", "?") for r in editorial["reviewers"]]
            editorial_summary += f"  Reviewers: {', '.join(rev_names)}\n"
        if editorial.get("reviews"):
            done_reviews = list(editorial["reviews"].keys())
            editorial_summary += f"  Reviews completed: {', '.join(done_reviews)}\n"

    # Expert consultants
    consultant_label = dcfg.get("consultant_label", "Consultant")
    expert_summary = ""
    if experts:
        expert_summary = f"\n\nActive {consultant_label}s:\n"
        for ex in experts:
            expert_summary += f"  - {ex.get('name', '?')} ({ex.get('role', '?')})\n"

    # Map internal role id to display label
    if role == "pi":
        role_display = dcfg["senior_label"]
    elif role == "trainee":
        role_display = dcfg["junior_label"]
    else:
        role_display = role  # reviewer_N etc.

    # Determine the right instruction based on current state
    if status in ("submitted_to_editor", "reviews_complete"):
        next_instruction = _EDITORIAL_INSTRUCTION
    else:
        next_instruction = _LOOP_INSTRUCTION

    briefing = (
        f"[AUTOLAB] SESSION RECOVERED â€” Resuming from saved state\n"
        f"{'=' * 60}\n\n"
        f"Project: {project_directory}\n"
        f"Domain: {dcfg['senior_label']}/{dcfg['junior_label']}/{dcfg['overseer_label']}\n"
        f"Started: {created_at}\n"
        f"Iteration: {iteration}\n"
        f"Status: {status}\n"
        f"Progress: {progress}%\n"
        f"Next role: {role_display}\n"
        f"{editorial_summary}"
        f"{expert_summary}\n\n"
        f"Project Idea:\n{idea[:500]}{'...' if len(idea) > 500 else ''}\n\n"
        f"Files:\n{file_summary}\n\n"
        f"{dcfg['artifact']} Progress:\n{paper_summary}\n\n"
        f"Recent {dcfg['meeting_log_name']} (last 5 turns):\n"
        f"{'â”€' * 40}\n"
        f"{recent_meetings if recent_meetings.strip() else '(no meeting history yet)'}\n"
        f"{'â”€' * 40}\n\n"
        f"{dcfg['meeting_log_name']} Summaries:\n"
        f"{summaries[:1000] if summaries.strip() else '(none yet)'}\n\n"
        f"{'=' * 60}\n"
        f"Recovery complete. The monitoring UI should be open.\n"
        f"Continue the lab from where it left off."
        f"{next_instruction}"
    )

    return briefing


@mcp.tool()
async def autolab_next(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
) -> str:
    """Get the next role prompt for the Autonomous Lab session.

    Reads the current state to determine whose turn it is (PI or Trainee),
    assembles context (idea, profiles, meeting history, file listings),
    and returns a detailed role prompt.

    The AI should then follow the returned prompt exactly, acting as
    the specified role (PI or Trainee).

    MANDATORY LOOP PROTOCOL â€” the lab NEVER stops:

    Normal turns:
      autolab_next -> (act as role) -> autolab_record -> lab_meeting -> autolab_next -> ...

    Editorial submission:
      autolab_record(status='ready_for_review') -> autolab_editorial (blocks) -> follow instructions

    After EVERY autolab_record call, follow the MANDATORY NEXT STEPS in the return message.
    NEVER stop. NEVER end your turn.

    Args:
        project_directory: Path to the project directory

    Returns:
        str: The role prompt to follow (MUST act on it and continue the loop)
    """
    from .lab.prompts import (
        build_pi_prompt,
        build_reviewer_prompt,
        build_revision_prompt,
        build_submission_prompt,
        build_trainee_prompt,
    )
    from .lab.state import (
        get_domain_config,
        get_editorial,
        get_meeting_summaries,
        get_paper_progress,
        get_recent_meetings,
        load_idea,
        load_profile,
        load_state,
        scan_project_files,
    )

    project_directory = os.path.abspath(project_directory)

    # Ensure the monitoring UI is visible on every new turn
    _ensure_lab_ui(project_directory)

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    idea = load_idea(project_directory)
    role = state["next_role"]
    iteration = state["iteration"]
    dcfg = get_domain_config(project_directory)
    # Drain accumulated user feedback queue (combines all pending messages)
    from .lab.state import drain_feedback_queue

    user_feedback = drain_feedback_queue(project_directory)
    meeting_history = get_recent_meetings(project_directory, n=3)
    summaries = get_meeting_summaries(project_directory)
    file_listings = scan_project_files(project_directory)
    editorial = state.get("editorial", {})

    # --- Editorial workflow routing ---

    # 1. PI is preparing submission (ready_for_review)
    if (
        state.get("status") == "ready_for_review"
        and editorial.get("phase", "none") == "none"
    ):
        paper_progress = get_paper_progress(project_directory)
        prompt = build_submission_prompt(paper_progress, file_listings, meeting_history)
        return (
            f"[AUTOLAB] SUBMISSION MODE -- Iteration {iteration}\n\n"
            f"{prompt}\n\n"
            f"After completing your submission, call autolab_record, then follow the MANDATORY NEXT STEPS."
        )

    # 2. Waiting for editor (submitted / reviews_complete)
    if state.get("status") in ("submitted_to_editor", "reviews_complete"):
        phase = editorial.get("phase", "none")
        if phase == "submitted":
            return (
                f"[AUTOLAB] AWAITING {dcfg['overseer_label'].upper()} -- Iteration {iteration}\n\n"
                f"The {dcfg['artifact'].lower()} has been submitted to the {dcfg['overseer_label']}.\n"
                f"The {dcfg['overseer_label']} (user) will decide: Desk Reject or Invite {dcfg['reviewer_label']}s."
                + _EDITORIAL_INSTRUCTION
            )
        if phase == "reviews_complete":
            return (
                f"[AUTOLAB] AWAITING {dcfg['overseer_label'].upper()} DECISION -- Iteration {iteration}\n\n"
                f"All {dcfg['reviewer_label'].lower()} reports are in.\n"
                f"The {dcfg['overseer_label']} (user) will decide: {' / '.join(dcfg['decisions'])}."
                + _EDITORIAL_INSTRUCTION
            )
        # Catch-all for transitional editorial states
        return (
            f"[AUTOLAB] {dcfg['review_process'].upper()} IN PROGRESS -- Iteration {iteration}\n\n"
            f"Editorial phase: {phase}, Status: {state.get('status')}\n"
            f"The {dcfg['review_process'].lower()} is in progress."
            + _EDITORIAL_INSTRUCTION
        )

    # 3. Reviewer turn
    if role.startswith("reviewer_"):
        paper_progress = get_paper_progress(project_directory)
        reviewers = editorial.get("reviewers", [])
        reviewer = next((r for r in reviewers if r["id"] == role), None)
        if not reviewer:
            return f"ERROR: Reviewer {role} not found in editorial state."

        cover_letter = editorial.get("cover_letter", "")
        round_num = editorial.get("round", 1)
        prompt = build_reviewer_prompt(
            reviewer=reviewer,
            paper_progress=paper_progress,
            file_listings=file_listings,
            cover_letter=cover_letter,
            round_number=round_num,
        )
        return (
            f"[AUTOLAB] REVIEWER TURN -- {reviewer['name']} ({reviewer['role']})\n"
            f"Iteration: {iteration}, Round: {round_num}\n\n"
            f"{prompt}\n\n"
            f"After completing your review, call autolab_record, then follow the MANDATORY NEXT STEPS."
        )

    # 4. Artifact accepted â€” senior wraps up
    if state.get("status") == "accepted":
        return (
            f"[AUTOLAB] {dcfg['artifact'].upper()} ACCEPTED -- Iteration {iteration}\n\n"
            f"Congratulations! The {dcfg['artifact'].lower()} has been accepted by the {dcfg['overseer_label']}.\n"
            f"As {dcfg['senior_label']}, write a brief acknowledgment, finalize the {dcfg['artifact'].lower()}, and celebrate.\n"
            f"If there is nothing left to do, inform the user the project is complete."
            + _LOOP_INSTRUCTION
        )

    # 5. Senior handling revision
    if state.get("status") in ("revision_requested", "rejected"):
        profile = load_profile(project_directory, "pi")
        reviews = editorial.get("reviews", {})
        decision = editorial.get("decision", "")
        decision_fb = editorial.get("decision_feedback", "")
        round_num = editorial.get("round", 1)
        prompt = build_revision_prompt(
            idea=idea,
            profile=profile,
            reviews=reviews,
            editorial_decision=decision,
            editorial_feedback=decision_fb,
            meeting_history=meeting_history,
            file_listings=file_listings,
            round_number=round_num,
            domain_config=dcfg,
        )
        return (
            f"[AUTOLAB] REVISION MODE -- Round {round_num}\n"
            f"Decision: {decision.upper().replace('_', ' ')}\n"
            f"Iteration: {iteration}\n\n"
            f"{prompt}\n\n"
            f"After completing revision actions, call autolab_record, then follow the MANDATORY NEXT STEPS."
        )

    # --- Normal lab workflow ---
    profile = load_profile(project_directory, role)

    # Map internal role ids to display labels
    role_label = dcfg["senior_label"] if role == "pi" else dcfg["junior_label"]

    if role == "pi":
        prompt = build_pi_prompt(
            idea=idea,
            profile=profile,
            meeting_history=meeting_history,
            summaries=summaries,
            file_listings=file_listings,
            user_feedback=user_feedback,
            iteration=iteration,
            domain_config=dcfg,
        )
    else:
        prompt = build_trainee_prompt(
            idea=idea,
            profile=profile,
            meeting_history=meeting_history,
            summaries=summaries,
            file_listings=file_listings,
            user_feedback=user_feedback,
            iteration=iteration,
            domain_config=dcfg,
        )

    return (
        f"[AUTOLAB] {role_label.upper()} TURN -- Iteration {iteration}\n\n"
        f"{prompt}\n\n"
        f"After completing your actions as {role_label}, call autolab_record, then follow the MANDATORY NEXT STEPS."
    )


@mcp.tool()
async def autolab_record(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
    role: Annotated[
        str, Field(description="Role that just acted: 'pi' or 'trainee'")
    ] = "",
    summary: Annotated[
        str, Field(description="Brief summary of what was done this turn")
    ] = "",
    content: Annotated[
        str,
        Field(description="Full content of the turn (review, agenda, results, etc.)"),
    ] = "",
    status: Annotated[
        str, Field(description="Status: 'continue' or 'ready_for_review'")
    ] = "continue",
    progress: Annotated[
        int, Field(description="PI overall project progress 0-100 (set only by PI)")
    ] = -1,
) -> str:
    """Record a completed turn. Non-blocking â€” updates state and returns immediately.

    The monitoring UI (opened by autolab_init) auto-refreshes to show progress.
    User feedback is optional and picked up in the next autolab_next call.

    This tool:
    1. Saves the turn to meeting_log.md
    2. Updates state.json (advance iteration, flip role, store status)
    3. Compresses old meetings every 6 turns

    Call this AFTER completing your role actions (as PI or Trainee).

    CRITICAL LOOP PROTOCOL â€” you MUST follow the MANDATORY NEXT STEPS in the return message:
      - Normal turns: call lab_meeting, then autolab_next.
      - Submission/editorial: call autolab_editorial (blocks until editor acts).
      - NEVER stop. NEVER end your turn. The loop runs indefinitely.

    Args:
        project_directory: Path to the project directory
        role: Which role just completed: 'pi' or 'trainee'
        summary: Brief summary of actions taken
        content: Full structured content of the turn
        status: 'continue' to keep iterating, 'ready_for_review' when paper is done
        progress: PI's assessment of overall project progress 0-100 (only PI should set this)

    Returns:
        str: Instructions to continue the loop (MUST follow them)
    """
    from .lab.state import (
        append_meeting_log,
        compress_old_meetings,
        get_domain_config,
        load_state,
        record_review,
        save_state,
        submit_manuscript,
        COMPRESS_EVERY,
    )

    project_directory = os.path.abspath(project_directory)

    # Validate role â€” allow pi, trainee, and reviewer_N
    valid_roles = role in ("pi", "trainee") or role.startswith("reviewer_")
    if not valid_roles:
        return "ERROR: role must be 'pi', 'trainee', or 'reviewer_N'"

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    iteration = state["iteration"]
    dcfg = get_domain_config(project_directory)

    # 1. Append to meeting log
    append_meeting_log(project_directory, role, iteration, summary, content)

    # --- Reviewer turn handling ---
    if role.startswith("reviewer_"):
        # Extract recommendation from content (flexible matching)
        import re

        rec_match = re.search(
            r"(?:RECOMMENDATION|OVERALL\s+RECOMMENDATION|DECISION)[:\s]*([A-Za-z][A-Za-z _]*)",
            content,
            re.IGNORECASE,
        )
        conf_match = re.search(r"CONFIDENCE[^:]*:\s*(\d)", content, re.IGNORECASE)
        recommendation = (
            rec_match.group(1).strip().lower().replace(" ", "_")
            if rec_match
            else "major_revision"
        )
        confidence = int(conf_match.group(1)) if conf_match else 3

        review_data = {
            "recommendation": recommendation,
            "confidence": confidence,
            "report": content,
        }
        editorial = record_review(project_directory, role, review_data)
        remaining = [
            r["id"]
            for r in editorial.get("reviewers", [])
            if r["id"] not in editorial.get("reviews", {})
        ]

        if remaining:
            next_reviewer = remaining[0]
            rev_name = next(
                (
                    r.get("name", next_reviewer)
                    for r in editorial.get("reviewers", [])
                    if r["id"] == next_reviewer
                ),
                next_reviewer,
            )
            return (
                f"{dcfg['reviewer_label']} review by {role} recorded successfully.\n"
                f"Next {dcfg['reviewer_label'].lower()}: {rev_name} ({next_reviewer}).\n\n"
                f"---\n"
                f"MANDATORY NEXT STEPS:\n"
                f"1. Call autolab_next NOW â€” it will give you {rev_name}'s {dcfg['reviewer_label'].lower()} prompt.\n"
                f"2. Act as that {dcfg['reviewer_label'].lower()} and write the review.\n"
                f"3. Call autolab_record with role='{next_reviewer}'.\n"
                f"4. Then repeat: autolab_next for the next {dcfg['reviewer_label'].lower()} (or autolab_editorial if done).\n"
                f"NEVER stop. NEVER call lab_meeting between {dcfg['reviewer_label'].lower()} turns â€” go straight to autolab_next."
            )
        else:
            return (
                f"{dcfg['reviewer_label']} review by {role} recorded. ALL REVIEWS ARE COMPLETE.\n\n"
                f"The {dcfg['overseer_label']} (user) will now make a final decision."
                + _EDITORIAL_INSTRUCTION
            )

    # --- Senior submission handling ---
    if role == "pi" and status == "ready_for_review":
        submit_manuscript(project_directory, content)
        if progress >= 0:
            st = load_state(project_directory)
            st["progress"] = max(0, min(100, progress))
            save_state(project_directory, st)
        return (
            f"{dcfg['artifact']} submitted to {dcfg['overseer_label']}.\n"
            f"Cover letter and {dcfg['artifact'].lower()} summary recorded.\n"
            f"The {dcfg['overseer_label']} (user) will now review via the monitoring UI."
            + _EDITORIAL_INSTRUCTION
        )

    # --- Normal turn handling ---
    next_role = "trainee" if role == "pi" else "pi"
    next_role_label = dcfg["junior_label"] if role == "pi" else dcfg["senior_label"]
    new_iteration = iteration + (1 if role == "trainee" else 0)
    state = load_state(project_directory)  # reload in case reviewer updated it
    state["iteration"] = new_iteration
    state["next_role"] = next_role
    state["status"] = status if status != "ready_for_review" else "active"
    # Senior can set overall progress (0-100)
    if progress >= 0 and role == "pi":
        state["progress"] = max(0, min(100, progress))
    save_state(project_directory, state)

    # Compress old meetings periodically
    if new_iteration > 0 and new_iteration % COMPRESS_EVERY == 0:
        compress_old_meetings(project_directory)

    # Check if user left feedback via the web UI (peek, don't drain â€”
    # the feedback will be properly drained in autolab_next or lab_meeting)
    from .lab.state import has_pending_feedback

    pending_fb = has_pending_feedback(project_directory)

    if pending_fb:
        return (
            f"Turn recorded. Iteration {new_iteration}, next: {next_role_label.upper()}.\n\n"
            f"User feedback is queued â€” it will be included in the next autolab_next call."
            + _LOOP_INSTRUCTION
        )

    return (
        f"Turn recorded. Iteration {new_iteration}, next: {next_role_label.upper()}."
        + _LOOP_INSTRUCTION
    )


@mcp.tool()
async def autolab_consult(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
    expert_name: Annotated[
        str, Field(description="Expert's name (e.g., 'Dr. Sarah Chen')")
    ] = "",
    expert_role: Annotated[
        str,
        Field(description="Expert's specialty (e.g., 'Statistician', 'Immunologist')"),
    ] = "",
    expert_avatar: Annotated[
        str,
        Field(
            description="Avatar key for the UI sprite: reviewer, bioethicist, science_writer, grant_reviewer, "
            "immunologist, oncologist, neuroscientist, geneticist, cell_biologist, microbiologist, "
            "pathologist, pharmacologist, structural_bio, systems_biologist, epidemiologist, "
            "statistician, bioinformatician, data_scientist, ml_engineer, comp_biologist, "
            "clinician, radiologist, surgeon, chemist, physicist, engineer, psychologist, ecologist, generic"
        ),
    ] = "generic",
    question: Annotated[
        str, Field(description="The specific question or topic to consult about")
    ] = "",
    character_repo: Annotated[
        str,
        Field(
            description="Optional GitHub repo of a marketplace character to use as the expert "
            "(e.g., 'albert-ying/autolab-char-biostatistician'). The character's skills "
            "and personality will define the expert's domain knowledge. "
            "If not provided, uses built-in domain knowledge."
        ),
    ] = "",
) -> str:
    """Invite a domain expert for a one-time consultation during a PI turn.

    The PI can call this to get advice from a specialist on a specific topic.
    The AI should then role-play as that expert and provide their insight,
    then continue the PI turn normally.

    This does NOT interrupt the loop â€” it's a synchronous consultation within
    the current PI turn. The expert appears in the monitoring UI sidebar.

    Args:
        project_directory: Path to the project directory
        expert_name: Name of the expert
        expert_role: Their specialty/domain
        expert_avatar: Avatar sprite key for the UI
        question: The specific question being asked

    Returns:
        str: Expert consultation prompt â€” the AI should role-play as the expert
    """
    from .lab.state import get_domain_config, load_state, save_state

    project_directory = os.path.abspath(project_directory)

    if not expert_name or not expert_role or not question:
        return "ERROR: expert_name, expert_role, and question are all required."

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    dcfg = get_domain_config(project_directory)
    senior = dcfg["senior_label"]
    consultant = dcfg.get("consultant_label", "Consultant")

    # Add expert to the list (dedup by name)
    experts = state.get("experts", [])
    existing = next((e for e in experts if e["name"] == expert_name), None)
    if not existing:
        experts.append(
            {
                "name": expert_name,
                "role": expert_role,
                "avatar": expert_avatar,
            }
        )
        state["experts"] = experts
        save_state(project_directory, state)

    # Build the expert consultation prompt with domain-specific knowledge
    idea = ""
    try:
        from .lab.state import load_idea

        idea = load_idea(project_directory)
    except Exception:
        pass

    # Look up domain-specific resources
    from .lab.prompts import CONSULTANT_DOMAINS, CONSULTANT_GENERIC

    # Load domain knowledge: marketplace character > built-in domains > generic
    domain_knowledge = ""
    character_skills_content = ""

    if character_repo:
        # Try to fetch character.yaml + skills from marketplace repo
        import urllib.request
        import json as _json

        clean_repo = character_repo.strip().strip("/")
        for branch in ("master", "main"):
            try:
                char_url = (
                    f"https://raw.githubusercontent.com/{clean_repo}"
                    f"/{branch}/character.yaml"
                )
                req = urllib.request.Request(char_url, headers={
                    "User-Agent": "AutonomousLab/0.5",
                })
                with urllib.request.urlopen(req, timeout=10) as resp:
                    import yaml as _yaml
                    char_data = _yaml.safe_load(resp.read().decode("utf-8"))

                # Use character's expertise and personality as domain knowledge
                char_expertise = char_data.get("expertise", "")
                char_personality = char_data.get("personality", [])
                char_skills = char_data.get("skills", [])

                # Override expert name/role from character if not provided
                if not expert_name or expert_name == "":
                    expert_name = char_data.get("name", expert_name)
                if not expert_role or expert_role == "":
                    expert_role = char_data.get("title", expert_role)

                domain_knowledge = (
                    f"Expertise: {char_expertise}\n"
                    f"Skills: {', '.join(char_skills)}\n"
                    f"Personality: {'; '.join(char_personality)}\n"
                )

                # Try to fetch SKILL.md files for richer context
                for skill_name in char_skills[:3]:  # Cap at 3 to limit context
                    try:
                        skill_url = (
                            f"https://raw.githubusercontent.com/{clean_repo}"
                            f"/{branch}/skills/{skill_name}/SKILL.md"
                        )
                        req2 = urllib.request.Request(skill_url, headers={
                            "User-Agent": "AutonomousLab/0.5",
                        })
                        with urllib.request.urlopen(req2, timeout=5) as resp2:
                            skill_content = resp2.read().decode("utf-8")
                            character_skills_content += (
                                f"\n### Skill: {skill_name}\n{skill_content[:2000]}\n"
                            )
                    except Exception:
                        pass
                break
            except Exception:
                continue

    if not domain_knowledge:
        # Fall back to built-in domain lookup
        avatar_key = expert_avatar.lower().replace(" ", "_")
        role_key = expert_role.lower().replace(" ", "_")
        domain_knowledge = (
            CONSULTANT_DOMAINS.get(avatar_key)
            or CONSULTANT_DOMAINS.get(role_key)
            or CONSULTANT_GENERIC
        )

    return (
        f"[{consultant.upper()}] Expert: {expert_name} ({expert_role})\n"
        f"{'=' * 50}\n\n"
        f"You are now briefly acting as **{expert_name}**, a specialist in **{expert_role}**.\n\n"
        f"**Your domain knowledge and standards:**\n{domain_knowledge}\n\n"
        + (f"**Your specialized skills:**\n{character_skills_content}\n\n"
           if character_skills_content else "")
        + f"The {senior} has asked you the following question:\n\n"
        f"> {question}\n\n"
        f"Project context (brief):\n{idea[:500]}{'...' if len(idea) > 500 else ''}\n\n"
        f"Provide a concise, expert response (2-4 paragraphs). Be direct and specific.\n"
        f"Ground your advice in the specific frameworks, guidelines, and standards listed above.\n"
        f"Include:\n"
        f"- Your expert opinion on the question, citing relevant standards or guidelines\n"
        f"- Key considerations the {senior} should be aware of\n"
        f"- Specific methodological recommendations with justification\n"
        f"- Potential pitfalls or caveats in your domain\n\n"
        f"After providing the consultation, IMMEDIATELY return to acting as the {senior}.\n"
        f"The {senior} should evaluate the {consultant.lower()}'s advice and decide what to adopt.\n"
        f"Do NOT call autolab_record for this consultation â€” it's part of the {senior}'s turn.\n"
        f"Continue with whatever the {senior} was doing before the consultation."
    )


@mcp.tool()
async def autolab_cite(
    action: Annotated[
        str,
        Field(
            description="Action: 'search' (find papers by topic), 'doi' (get BibTeX from DOI), "
            "'validate' (check references.bib for errors)"
        ),
    ] = "search",
    query: Annotated[
        str,
        Field(
            description="For 'search': topic description. For 'doi': the DOI string. "
            "For 'validate': path to .bib file (default: paper/references.bib)"
        ),
    ] = "",
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
    count: Annotated[
        int, Field(description="Number of results for search (max 10)")
    ] = 5,
    from_year: Annotated[
        int, Field(description="Only papers from this year onward (0 = no filter)")
    ] = 0,
) -> str:
    """Look up, search, and validate citations using CrossRef API.

    This tool ensures that ALL citations in the paper are real, verified papers
    with correct metadata. Use it for:

    1. **search**: Find real papers on a topic â†’ returns verified BibTeX entries
       ready to paste into references.bib.
    2. **doi**: Convert a known DOI to a properly formatted BibTeX entry.
    3. **validate**: Check an existing .bib file â€” verify DOIs resolve, find
       entries missing DOIs, and suggest corrections.

    CRITICAL RULE FOR CITATIONS:
    - NEVER invent or fabricate citation entries. ALWAYS use this tool to get
      verified BibTeX from CrossRef.
    - EVERY \cite{key} in the paper MUST have a corresponding entry in
      references.bib that was retrieved via this tool.
    - When writing a paper section, first search for relevant papers, add
      their BibTeX to references.bib, THEN cite them in the text.

    Args:
        action: 'search', 'doi', or 'validate'
        query: Topic description, DOI string, or .bib file path
        project_directory: Project directory path
        count: Number of search results (max 10, default 5)
        from_year: Filter papers from this year onward (0 = no filter)

    Returns:
        BibTeX entries (search/doi) or validation report (validate)
    """
    from .lab.citations import doi_to_bibtex, search_papers, validate_bibtex_file

    project_directory = os.path.abspath(project_directory)

    try:
        if action == "doi":
            if not query:
                return "ERROR: 'query' must contain a DOI string (e.g., '10.1038/s41586-021-03819-2')"
            bibtex = doi_to_bibtex(query)
            return (
                f"Verified BibTeX entry from CrossRef:\n\n{bibtex}\n\n"
                f"Add this to paper/references.bib, then use the citation key in your LaTeX."
            )

        elif action == "search":
            if not query:
                return "ERROR: 'query' must describe what you need to cite (e.g., 'CRISPR efficiency in human cells')"
            year_filter = from_year if from_year > 0 else None
            results = search_papers(
                query, rows=min(count, 10), filter_from_year=year_filter
            )
            if not results or (len(results) == 1 and "error" in results[0]):
                return f"No papers found for: {query}\nTry broader search terms or remove the year filter."

            output_parts = [f"Found {len(results)} verified paper(s) for: {query}\n"]
            for i, r in enumerate(results, 1):
                if "error" in r:
                    continue
                output_parts.append(
                    f"--- Paper {i} ---\n"
                    f"Title: {r['title']}\n"
                    f"Authors: {r['authors']}\n"
                    f"Year: {r['year']} | Journal: {r['journal']}\n"
                    f"DOI: {r['doi']}\n\n"
                    f"{r['bibtex']}\n"
                )
            output_parts.append(
                "\nAdd the relevant entries to paper/references.bib.\n"
                "Use the citation keys (e.g., \\cite{Smith2024keyword}) in your LaTeX text."
            )
            return "\n".join(output_parts)

        elif action == "validate":
            bib_path = (
                query
                if query
                else os.path.join(project_directory, "paper", "references.bib")
            )
            if not os.path.exists(bib_path):
                return f"ERROR: BibTeX file not found: {bib_path}"
            report = validate_bibtex_file(bib_path)
            lines = [
                f"Citation Validation Report for {bib_path}",
                f"{'=' * 50}",
                f"Total entries: {report['total']}",
                f"Valid (DOI verified or match found): {report['valid']}",
                f"Errors: {len(report['errors'])}",
                f"Warnings: {len(report['warnings'])}",
            ]
            if report["errors"]:
                lines.append("\n--- ERRORS (must fix) ---")
                for err in report["errors"]:
                    lines.append(f"  [{err['key']}] {err['error']}")
            if report["warnings"]:
                lines.append("\n--- WARNINGS (review) ---")
                for w in report["warnings"]:
                    lines.append(f"  [{w['key']}] {w['warning']}")
                    if "suggested_doi" in w:
                        lines.append(f"    Suggested DOI: {w['suggested_doi']}")
            if report["corrected_entries"]:
                lines.append("\n--- CORRECTED ENTRIES (verified via CrossRef) ---")
                for c in report["corrected_entries"][:5]:  # Show max 5
                    lines.append(f"\n  Key: {c['key']}")
                    lines.append(f"  {c['corrected_bibtex']}")
            return "\n".join(lines)

        else:
            return (
                f"ERROR: Unknown action '{action}'. Use 'search', 'doi', or 'validate'."
            )

    except Exception as e:
        return f"Citation tool error: {e}\n\nYou can still manually look up papers at https://doi.org/ or https://scholar.google.com/"


@mcp.tool()
async def autolab_editorial(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
) -> str:
    """Wait for the Editor (user) to act on the submitted manuscript.

    Call this in two scenarios:
    1. After PI submits a manuscript (autolab_record with status='ready_for_review').
       Waits for the editor to either Desk Reject or Invite Reviewers.
    2. After all reviewer reports are complete.
       Waits for the editor to make a final decision (Accept/Minor/Major/Reject).

    This tool BLOCKS until the editor acts OR the configured timeout expires
    (default 30 min, configurable via editor_timeout_minutes in config.yaml,
    set to 0 to wait forever). When timeout expires, returns an AI Editor
    prompt â€” you must then act as editor and call autolab_editor_act.

    Returns:
      - If reviewers invited: instructions to run the reviewer loop
        (call autolab_next for each reviewer, then autolab_editorial again)
      - If decision made (accept/minor/major/reject): the decision + instructions
        to call lab_meeting then autolab_next to continue PI/Trainee loop.
      - If timeout: AI Editor prompt + instructions to call autolab_editor_act.
    """
    import asyncio

    from .lab.state import load_state, get_domain_config, get_editor_timeout_seconds

    project_directory = os.path.abspath(project_directory)

    # Ensure the web UI is running â€” the user needs it to make editorial decisions
    _ensure_lab_ui(project_directory)

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    editorial = state.get("editorial", {})
    starting_phase = editorial.get("phase", "none")
    iteration = state.get("iteration", 0)
    current_status = state.get("status", "")
    next_role = state.get("next_role", "")
    dcfg = get_domain_config(project_directory)

    debug_log(
        f"autolab_editorial: starting phase={starting_phase}, status={current_status}, next_role={next_role}"
    )

    # â”€â”€ Immediate returns: if the state already advanced, don't poll â”€â”€

    # Already in reviewer phase â†’ return immediately with reviewer instructions
    if starting_phase in ("reviewers_invited", "under_review") and next_role.startswith(
        "reviewer_"
    ):
        reviewers = editorial.get("reviewers", [])
        reviewer_names = ", ".join(r.get("name", r["id"]) for r in reviewers)
        rl = dcfg["reviewer_label"].lower()
        return (
            f"[EDITORIAL] {dcfg['reviewer_label']}s invited: {reviewer_names}\n\n"
            f"The {dcfg['overseer_label'].lower()} has invited {len(reviewers)} {rl}(s) and reviews are in progress.\n"
            f"You must now play each {rl} role in sequence.\n\n"
            f"MANDATORY NEXT STEPS:\n"
            f"1. Call autolab_next â€” it will return the current {rl}'s prompt.\n"
            f"2. Act as that {rl}, then call autolab_record with role='reviewer_N'.\n"
            f"3. Call autolab_next immediately for the next {rl} (do NOT call lab_meeting between {rl}s).\n"
            f"4. Repeat until all reviews are done.\n"
            f"5. After the LAST review, call autolab_editorial to wait for the {dcfg['overseer_label'].lower()}'s final decision.\n"
            f"NEVER stop. Continue the loop."
        )

    # Already have a decision â†’ return immediately
    if starting_phase == "decision_made":
        decision = editorial.get("decision", "")
        feedback = editorial.get("decision_feedback", "")
        dec_display = decision.upper().replace("_", " ")
        return (
            f"[EDITORIAL] Decision: {dec_display}\n\n"
            f"{dcfg['overseer_label']} feedback: {feedback}\n\n"
            f"The {dcfg['review_process'].lower()} for this round is complete.\n"
            f"Decision: {dec_display}" + _LOOP_INSTRUCTION
        )

    # Reviews already complete â†’ wait for decision only
    if starting_phase == "reviews_complete":
        debug_log("autolab_editorial: all reviews done, waiting for editor decision")

    # â”€â”€ Timeout configuration â”€â”€
    timeout_seconds = get_editor_timeout_seconds(project_directory)
    elapsed_seconds = 0
    debug_log(f"autolab_editorial: timeout={timeout_seconds}s (0=disabled)")

    # â”€â”€ Poll loop: wait for the editorial phase to advance â”€â”€

    poll_interval = 5  # seconds
    _health_check_counter = 0
    while True:
        await asyncio.sleep(poll_interval)
        elapsed_seconds += poll_interval
        _health_check_counter += 1

        # Periodic health check â€” restart the web server if it died (every ~60s)
        if _health_check_counter % 12 == 0:
            try:
                _ensure_lab_ui(project_directory)
            except Exception:
                pass  # non-fatal

        try:
            state = load_state(project_directory)
        except Exception:
            continue  # Retry on transient read errors

        editorial = state.get("editorial", {})
        current_phase = editorial.get("phase", "none")
        current_status = state.get("status", "")
        next_role = state.get("next_role", "")

        # --- Case 1: We were waiting for overseer after submission ---
        if starting_phase == "submitted":
            if current_phase in ("reviewers_invited", "under_review"):
                # Overseer invited reviewers â†’ AI must now play reviewer roles
                reviewers = editorial.get("reviewers", [])
                reviewer_names = ", ".join(r.get("name", r["id"]) for r in reviewers)
                rl = dcfg["reviewer_label"].lower()
                return (
                    f"[EDITORIAL] {dcfg['reviewer_label']}s invited: {reviewer_names}\n\n"
                    f"The {dcfg['overseer_label'].lower()} has invited {len(reviewers)} {rl}(s).\n"
                    f"You must now play each {rl} role in sequence.\n\n"
                    f"MANDATORY NEXT STEPS:\n"
                    f"1. Call autolab_next â€” it will return the first {rl}'s prompt.\n"
                    f"2. Act as that {rl}, then call autolab_record with role='reviewer_N'.\n"
                    f"3. Call autolab_next immediately for the next {rl} (do NOT call lab_meeting between {rl}s).\n"
                    f"4. Repeat until all reviews are done.\n"
                    f"5. After the LAST review, call autolab_editorial to wait for the {dcfg['overseer_label'].lower()}'s final decision.\n"
                    f"NEVER stop. Continue the loop."
                )
            if current_phase == "decision_made":
                # Desk reject (overseer decided without reviewers)
                decision = editorial.get("decision", "reject")
                feedback = editorial.get("decision_feedback", "")
                return (
                    f"[EDITORIAL] Decision: {decision.upper().replace('_', ' ')}\n\n"
                    f"The {dcfg['overseer_label'].lower()} has desk-rejected the {dcfg['artifact'].lower()}.\n"
                    f"Feedback: {feedback}\n" + _LOOP_INSTRUCTION
                )

        # --- Case 2: We were waiting for final decision after reviews ---
        if starting_phase in ("reviews_complete", "under_review", "reviewers_invited"):
            if current_phase == "decision_made":
                decision = editorial.get("decision", "")
                feedback = editorial.get("decision_feedback", "")
                dec_display = decision.upper().replace("_", " ")
                return (
                    f"[EDITORIAL] Decision: {dec_display}\n\n"
                    f"{dcfg['overseer_label']} feedback: {feedback}\n\n"
                    f"The {dcfg['review_process'].lower()} for this round is complete.\n"
                    f"Decision: {dec_display}" + _LOOP_INSTRUCTION
                )
            # Reviewer role assigned while we're polling â†’ break out to let AI play reviewer
            if next_role.startswith("reviewer_") and current_phase in (
                "reviewers_invited",
                "under_review",
            ):
                reviewers = editorial.get("reviewers", [])
                reviewer_names = ", ".join(r.get("name", r["id"]) for r in reviewers)
                rl = dcfg["reviewer_label"].lower()
                return (
                    f"[EDITORIAL] {dcfg['reviewer_label']} turn ready: {next_role}\n\n"
                    f"{dcfg['reviewer_label']}s: {reviewer_names}\n\n"
                    f"MANDATORY NEXT STEPS:\n"
                    f"1. Call autolab_next to get the {rl} prompt.\n"
                    f"2. Act as the {rl}, then call autolab_record with role='{next_role}'.\n"
                    f"3. Call autolab_next immediately for the next {rl} (skip lab_meeting between {rl}s).\n"
                    f"4. After the LAST review, call autolab_editorial again.\n"
                    f"NEVER stop."
                )

        # --- Generic: any phase change from what we started with ---
        if current_phase != starting_phase:
            return (
                f"[EDITORIAL] Phase changed: {starting_phase} â†’ {current_phase}\n"
                f"Status: {current_status}\n" + _LOOP_INSTRUCTION
            )

        # â”€â”€ Timeout check: if enabled and expired, switch to AI editor â”€â”€
        if timeout_seconds > 0 and elapsed_seconds >= timeout_seconds:
            debug_log(
                f"autolab_editorial: editor timeout after {elapsed_seconds}s â€” "
                f"switching to AI editor for phase={current_phase}"
            )
            return _build_ai_editor_return(
                project_directory, editorial, current_phase, elapsed_seconds
            )

        # Still waiting â€” continue polling
        # Gradually increase poll interval up to 15s to reduce disk I/O
        if poll_interval < 15:
            poll_interval = min(poll_interval + 1, 15)


def _build_ai_editor_return(
    project_directory: str,
    editorial: dict,
    phase: str,
    elapsed_seconds: int,
) -> str:
    """Construct the return value when editor timeout expires."""
    from .lab.prompts import build_ai_editor_prompt
    from .lab.state import get_domain_config, scan_project_files

    dcfg = get_domain_config(project_directory)
    file_listings = scan_project_files(project_directory)
    cover_letter = editorial.get("cover_letter", "")
    reviews = editorial.get("reviews", {})

    editor_prompt = build_ai_editor_prompt(
        editorial=editorial,
        phase=phase,
        cover_letter=cover_letter,
        reviews=reviews,
        file_listings=file_listings,
    )

    mins = elapsed_seconds // 60
    rl = dcfg["reviewer_label"].lower()
    return (
        f"[EDITORIAL â€” AI {dcfg['overseer_label'].upper()} ACTIVATED]\n\n"
        f"The human {dcfg['overseer_label'].lower()} did not respond within {mins} minutes.\n"
        f"You must now act as the {dcfg['overseer_label']} and make a decision.\n\n"
        f"{editor_prompt}\n\n"
        f"MANDATORY NEXT STEPS after making your decision:\n"
        f"1. Call `autolab_editor_act` with your action and feedback.\n"
        f"   - For desk reject: action='desk_reject', feedback='...'\n"
        f"   - For invite {rl}s: action='invite_reviewers', feedback='...', "
        f'reviewers=[{{"name":"Dr. X","role":"Specialty","avatar":"sprite_key"}}]\n'
        f"   - For final decision: action='accept'|'minor_revision'|'major_revision'|'reject', feedback='...'\n"
        f"2. Then follow the MANDATORY NEXT STEPS returned by autolab_editor_act.\n"
        f"NEVER stop. Continue the loop."
    )


@mcp.tool()
async def autolab_editor_act(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
    action: Annotated[
        str,
        Field(
            description="Editorial action: 'desk_reject', 'invite_reviewers', "
            "'accept', 'minor_revision', 'major_revision', 'reject'"
        ),
    ] = "",
    feedback: Annotated[
        str, Field(description="Editor's feedback to the authors")
    ] = "",
    reviewers: Annotated[
        str,
        Field(
            description="JSON array of reviewer dicts for invite_reviewers, "
            'e.g. [{"name":"Dr. X","role":"Statistician","avatar":"statistician"}]. '
            "Ignored for other actions."
        ),
    ] = "[]",
) -> str:
    """Execute an editorial decision made by the AI editor (after timeout).

    Called by the AI after autolab_editorial returns an AI-editor prompt
    due to the human editor not responding within the configured timeout.

    Actions:
    - desk_reject: reject without review, feedback required
    - invite_reviewers: send to peer review, reviewers JSON required
    - accept / minor_revision / major_revision / reject: final decision after reviews

    Returns instructions to continue the loop.
    """
    from .lab.state import (
        get_domain_config,
        load_state,
        record_editorial_decision,
        invite_reviewers as _invite_reviewers,
    )

    project_directory = os.path.abspath(project_directory)

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    dcfg = get_domain_config(project_directory)
    action = action.strip().lower()
    feedback = (
        feedback.strip()
        or f"(No feedback provided by AI {dcfg['overseer_label'].lower()})"
    )

    debug_log(f"autolab_editor_act: action={action}")

    if action == "desk_reject":
        record_editorial_decision(project_directory, "reject", feedback)
        return (
            f"[AI {dcfg['overseer_label'].upper()}] {dcfg['artifact']} desk-rejected.\n\n"
            f"Feedback sent to {dcfg['senior_label']}: {feedback[:200]}...\n"
            + _LOOP_INSTRUCTION
        )

    elif action == "invite_reviewers":
        # Parse reviewer list
        import json as _json

        try:
            reviewer_list = _json.loads(reviewers)
        except (ValueError, TypeError):
            reviewer_list = []

        if not reviewer_list or not isinstance(reviewer_list, list):
            # Fallback: generate 3 default reviewers
            reviewer_list = [
                {
                    "id": "reviewer_1",
                    "name": "Dr. Methods",
                    "role": "Statistician",
                    "avatar": "statistician",
                },
                {
                    "id": "reviewer_2",
                    "name": "Dr. Domain",
                    "role": "Domain Expert",
                    "avatar": "reviewer",
                },
                {
                    "id": "reviewer_3",
                    "name": "Dr. Scope",
                    "role": "Generalist",
                    "avatar": "generic",
                },
            ]

        # Ensure each reviewer has an id
        for i, r in enumerate(reviewer_list):
            if "id" not in r:
                r["id"] = f"reviewer_{i + 1}"

        _invite_reviewers(project_directory, reviewer_list)
        reviewer_names = ", ".join(r.get("name", r["id"]) for r in reviewer_list)
        rl = dcfg["reviewer_label"].lower()
        return (
            f"[AI {dcfg['overseer_label'].upper()}] {dcfg['reviewer_label']}s invited: {reviewer_names}\n\n"
            f"{dcfg['overseer_label']} note: {feedback[:200]}\n\n"
            f"You must now play each {rl} role in sequence.\n\n"
            f"MANDATORY NEXT STEPS:\n"
            f"1. Call autolab_next â€” it will return the first {rl}'s prompt.\n"
            f"2. Act as that {rl}, then call autolab_record with role='reviewer_N'.\n"
            f"3. Call autolab_next immediately for the next {rl} (skip lab_meeting between {rl}s).\n"
            f"4. After the LAST review, call autolab_editorial again.\n"
            f"NEVER stop."
        )

    elif action in ("accept", "minor_revision", "major_revision", "reject"):
        record_editorial_decision(project_directory, action, feedback)
        dec_display = action.upper().replace("_", " ")
        return (
            f"[AI {dcfg['overseer_label'].upper()}] Decision: {dec_display}\n\n"
            f"Feedback: {feedback[:200]}...\n" + _LOOP_INSTRUCTION
        )

    else:
        return (
            f"ERROR: Unknown action '{action}'. Must be one of: "
            f"desk_reject, invite_reviewers, accept, minor_revision, major_revision, reject.\n"
            f"Call autolab_editor_act again with a valid action."
        )


@mcp.tool()
async def autolab_status(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
) -> str:
    """Get the current status of an Autonomous Lab project.

    Returns the current state, file listings, paper progress,
    and a summary of recent meetings.

    Args:
        project_directory: Path to the project directory

    Returns:
        str: Formatted status report
    """
    from .lab.state import (
        get_domain_config,
        get_paper_progress,
        get_recent_meetings,
        load_state,
        scan_project_files,
    )

    project_directory = os.path.abspath(project_directory)

    try:
        state = load_state(project_directory)
    except FileNotFoundError as e:
        return f"ERROR: {e}"

    dcfg = get_domain_config(project_directory)
    file_listings = scan_project_files(project_directory)
    paper_progress = get_paper_progress(project_directory)
    recent = get_recent_meetings(project_directory, n=2)

    # Map internal role id to display label
    next_role_id = state["next_role"]
    if next_role_id == "pi":
        next_role_display = dcfg["senior_label"]
    elif next_role_id == "trainee":
        next_role_display = dcfg["junior_label"]
    else:
        next_role_display = next_role_id  # reviewer_N etc.

    # Format file counts
    file_counts = {k: len(v) for k, v in file_listings.items()}

    # Format paper progress
    paper_lines = []
    for section, info in paper_progress.items():
        if info["exists"] and info["words"] > 0:
            paper_lines.append(f"  {section}: {info['words']} words")
        elif info["exists"]:
            paper_lines.append(f"  {section}: placeholder only")
        else:
            paper_lines.append(f"  {section}: not created")

    report = (
        f"# Autonomous Lab Status\n\n"
        f"**Domain:** {dcfg.get('senior_label', 'PI')}/{dcfg.get('junior_label', 'Trainee')}/{dcfg.get('overseer_label', 'Editor')}\n"
        f"**Iteration:** {state['iteration']}\n"
        f"**Next role:** {next_role_display}\n"
        f"**Status:** {state['status']}\n"
        f"**Last updated:** {state.get('last_updated', 'unknown')}\n\n"
        f"## File Counts\n"
        f"  data: {file_counts.get('data', 0)} files\n"
        f"  scripts: {file_counts.get('scripts', 0)} files\n"
        f"  figures: {file_counts.get('figures', 0)} files\n"
        f"  results: {file_counts.get('results', 0)} files\n"
        f"  paper: {file_counts.get('paper', 0)} files\n\n"
        f"## {dcfg['artifact']} Progress\n" + "\n".join(paper_lines) + "\n\n"
        f"## Recent {dcfg['meeting_log_name']}\n\n{recent if recent.strip() else 'No meetings yet.'}\n"
    )

    # Show queued feedback count (don't drain it â€” just peek)
    queue = state.get("feedback_queue", [])
    if not isinstance(queue, list):
        queue = [queue] if queue else []
    if queue:
        report += f"\n## Pending User Feedback ({len(queue)} message(s) queued)\n\n"
        for i, msg in enumerate(queue, 1):
            report += f"  {i}. {msg[:100]}{'...' if len(msg) > 100 else ''}\n"

    return report


# ===== Biomedical Toolkit Integration =====
# Wraps the optional Biomni package (snap-stanford/Biomni) behind
# generic tool names so the Autonomous Lab API stays vendor-neutral.


@mcp.tool()
async def autolab_biotools_status(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
) -> str:
    """Check whether the optional biomedical toolkit is installed and enabled.

    The biomedical toolkit provides 100+ curated tools and databases for
    life-science research (ADMET prediction, scRNA-seq analysis, CRISPR
    screen planning, pathway enrichment, etc.).

    It is NOT bundled with Autonomous Lab â€” users opt in by calling
    autolab_biotools_install. Once installed, the Trainee can import
    individual functions directly in Python scripts.

    Returns JSON with: installed (bool), version, enabled, datalake path.
    """
    from .integrations.biomni import get_status

    project_directory = os.path.abspath(project_directory)
    status = get_status(project_directory)
    return json.dumps(status, indent=2)


@mcp.tool()
async def autolab_biotools_install(
    from_source: Annotated[
        bool,
        Field(
            description="Install latest from GitHub (True) or stable from PyPI (False)"
        ),
    ] = True,
) -> str:
    """Install the biomedical toolkit (one-time setup).

    Downloads and installs the package at runtime.
    Apache 2.0 licensed core; some integrated tools may carry
    more restrictive licenses â€” review before commercial use.

    The full datalake (~11 GB) downloads on first use.
    Set skip_datalake=True via autolab_biotools_configure to skip it.
    """
    from .integrations.biomni import install_biomni

    result = install_biomni(from_source=from_source)
    if result["success"]:
        return f"SUCCESS: {result['message']}"
    else:
        return f"FAILED: {result['message']}"


@mcp.tool()
async def autolab_biotools_configure(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
    enabled: Annotated[
        bool, Field(description="Enable biomedical toolkit for this project")
    ] = True,
    data_path: Annotated[
        str, Field(description="Path for toolkit datalake")
    ] = "./data",
    skip_datalake: Annotated[
        bool, Field(description="Skip downloading the 11GB datalake")
    ] = False,
) -> str:
    """Configure the biomedical toolkit for this project.

    Saves settings to .autolab/biotools_config.json.
    The toolkit must be installed first (autolab_biotools_install).

    NOTE: The toolkit is used as a library only â€” we do NOT instantiate
    any external LLM agent or pipeline.
    """
    from .integrations.biomni import save_biomni_config

    project_directory = os.path.abspath(project_directory)
    cfg = save_biomni_config(
        project_directory,
        enabled=enabled,
        data_path=data_path,
        skip_datalake=skip_datalake,
    )
    return f"Biotools config saved:\n{json.dumps(cfg, indent=2)}"


@mcp.tool()
async def autolab_biotools_list(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
) -> str:
    """List available biomedical tools and databases.

    Returns curated tools (ADMET prediction, scRNA-seq analysis,
    CRISPR screen planning, etc.) and database connectors that the
    Trainee can import directly in Python scripts.

    Use this when the PI or Trainee needs to discover what analysis
    capabilities are available beyond standard Python packages.
    """
    from .integrations.biomni import (
        is_biomni_available,
        list_available_databases,
        list_available_tools,
    )

    if not is_biomni_available():
        return (
            "Biomedical toolkit is not installed.\n"
            "Install with: autolab_biotools_install()\n"
        )
    tools = list_available_tools()
    dbs = list_available_databases()

    lines = ["## Available Biomedical Tools\n"]
    if tools:
        for t in tools:
            desc = f" â€” {t['description']}" if t.get("description") else ""
            lines.append(f"  - **{t['name']}** (`{t['module']}`){desc}")
    else:
        lines.append("  (no tools detected)")

    lines.append("\n## Available Databases\n")
    if dbs:
        for d in dbs:
            desc = f" â€” {d['description']}" if d.get("description") else ""
            lines.append(f"  - **{d['name']}** (`{d['module']}`){desc}")
    else:
        lines.append("  (no databases detected)")

    lines.append(
        "\n## Usage\n"
        "Import tools directly in analysis scripts:\n"
        "```python\n"
        "from biomni.tools.<tool_name> import *\n"
        "```"
    )
    return "\n".join(lines)


@mcp.tool()
async def autolab_biotools_env() -> str:
    """Check biomedical toolkit environment details.

    Shows available sub-packages, active Python environment,
    datalake status, and paths.
    """
    from .integrations.biomni import check_environment

    env = check_environment()
    return json.dumps(env, indent=2)


# ===== Character Creation =====

# Available avatar sprite keys (must match EXPERT_DEFS in sprites.js)
_AVATAR_KEYS = [
    "reviewer",
    "bioethicist",
    "science_writer",
    "grant_reviewer",
    "immunologist",
    "oncologist",
    "neuroscientist",
    "geneticist",
    "cell_biologist",
    "microbiologist",
    "pathologist",
    "pharmacologist",
    "structural_bio",
    "systems_biologist",
    "epidemiologist",
    "statistician",
    "bioinformatician",
    "data_scientist",
    "ml_engineer",
    "comp_biologist",
    "clinician",
    "radiologist",
    "surgeon",
    "chemist",
    "physicist",
    "engineer",
    "psychologist",
    "ecologist",
    "generic",
]

# Built-in skill catalog for reference (users can also create their own)
_SKILL_CATALOG = {
    "Bioinformatics & Omics": [
        "scanpy",
        "scvi-tools",
        "pydeseq2",
        "pysam",
        "deeptools",
        "anndata",
        "cellxgene-census",
        "geniml",
        "gtars",
        "arboreto",
    ],
    "Machine Learning & AI": [
        "scikit-learn",
        "pytorch-lightning",
        "transformers",
        "accelerate",
        "shap",
        "torch_geometric",
        "deepchem",
        "torchdrug",
        "umap-learn",
        "stable-baselines3",
    ],
    "Statistics & Modeling": [
        "statistical-analysis",
        "statsmodels",
        "pymc",
        "scikit-survival",
        "sympy",
        "pymoo",
    ],
    "Chemistry & Drug Discovery": [
        "rdkit",
        "datamol",
        "medchem",
        "molfeat",
        "pytdc",
        "diffdock",
        "rowan",
        "matchms",
    ],
    "Visualization": [
        "scientific-visualization",
        "matplotlib",
        "seaborn",
        "plotly",
    ],
    "Writing & Communication": [
        "scientific-writing",
        "literature-review",
        "peer-review",
        "scientific-brainstorming",
        "hypothesis-generation",
        "venue-templates",
        "latex-posters",
        "scientific-slides",
    ],
    "Clinical & Databases": [
        "clinical-reports",
        "clinicaltrials-database",
        "clinvar-database",
        "pubmed-database",
        "geo-database",
        "opentargets-database",
        "string-database",
        "kegg-database",
        "reactome-database",
        "uniprot-database",
        "pdb-database",
        "pubchem-database",
        "chembl-database",
        "ensembl-database",
    ],
    "Infrastructure & MLOps": [
        "weights-and-biases",
        "mlflow",
        "tensorboard",
        "modal",
        "vllm",
        "sglang",
    ],
    "Data Processing": [
        "polars",
        "dask",
        "vaex",
        "zarr-python",
        "geopandas",
        "exploratory-data-analysis",
    ],
}

# Flatten catalog skills for reference (not for validation â€” users can add custom skills)
_KNOWN_SKILLS = sorted({s for skills in _SKILL_CATALOG.values() for s in skills})


@mcp.tool()
async def autolab_create_character(
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
    name: Annotated[
        str, Field(description="Character's display name (e.g., 'Dr. Maria Chen')")
    ] = "",
    role: Annotated[
        str, Field(description="Role type: 'pi', 'trainee', or 'collaborator'")
    ] = "",
    title: Annotated[
        str, Field(description="Professional title (e.g., 'Computational Biology PI')")
    ] = "",
    expertise: Annotated[
        str,
        Field(
            description="Areas of expertise (e.g., 'Single-cell genomics, machine learning')"
        ),
    ] = "",
    goal: Annotated[str, Field(description="Character's research goal")] = "",
    skills: Annotated[
        str,
        Field(
            description="Comma-separated skill names "
            "(e.g., 'scanpy,scvi-tools,scientific-writing,my-custom-tool'). "
            "Can be built-in Cursor skills or your own custom SKILL.md names. "
            "Use list_skills=True to browse the built-in catalog."
        ),
    ] = "",
    personality: Annotated[
        str,
        Field(
            description="Pipe-separated personality traits "
            "(e.g., 'Rigorous: demands statistical reproducibility|"
            "Visionary: spots novel research directions')"
        ),
    ] = "",
    avatar: Annotated[
        str,
        Field(
            description="Avatar sprite key for the game UI: reviewer, bioethicist, "
            "science_writer, grant_reviewer, immunologist, oncologist, "
            "neuroscientist, geneticist, cell_biologist, microbiologist, "
            "pathologist, pharmacologist, structural_bio, systems_biologist, "
            "epidemiologist, statistician, bioinformatician, data_scientist, "
            "ml_engineer, comp_biologist, clinician, radiologist, surgeon, "
            "chemist, physicist, engineer, psychologist, ecologist, generic"
        ),
    ] = "generic",
    deploy_as: Annotated[
        str,
        Field(
            description="Deploy locally as 'pi' or 'trainee' profile "
            "(saves to .autolab/profiles/). Leave empty to only generate the YAML."
        ),
    ] = "",
    github_repo: Annotated[
        str,
        Field(
            description="Optional GitHub repo path for marketplace listing "
            "(e.g., 'username/autolab-char-name')"
        ),
    ] = "",
    list_skills: Annotated[
        bool,
        Field(
            description="If True, ignore other params and return the built-in skill catalog "
            "(you can also use any custom skill name not listed here)"
        ),
    ] = False,
    list_avatars: Annotated[
        bool,
        Field(
            description="If True, ignore other params and return all available avatar keys"
        ),
    ] = False,
) -> str:
    """Create a character profile for the Autonomous Lab research team.

    A character is a YAML persona backed by Cursor agent skills that controls
    how the AI behaves during research sessions. Characters can be:
    - Deployed locally as a PI or Trainee profile
    - Published to GitHub for the marketplace
    - Used as consultants or reviewers during editorial review

    Each character is a **self-contained folder** with this structure:

        autolab-char-<name>/
        â”œâ”€â”€ character.yaml          # Profile (name, role, skills, personality)
        â”œâ”€â”€ README.md               # Description for marketplace
        â””â”€â”€ skills/                 # Skill definitions (SKILL.md files)
            â”œâ”€â”€ <skill-name>/
            â”‚   â””â”€â”€ SKILL.md        # Detailed instructions for this skill
            â””â”€â”€ ...

    Use list_skills=True or list_avatars=True to discover options first.

    Workflow:
    1. Call with list_skills=True to see available skills by domain.
    2. Call with all fields filled to generate the character folder.
    3. Optionally set github_repo to initialize a GitHub repo and push.

    Args:
        project_directory: Project directory path
        name: Character display name
        role: pi, trainee, or collaborator
        title: Professional title
        expertise: Areas of expertise
        goal: Research goal
        skills: Comma-separated Cursor skill names
        personality: Pipe-separated personality traits (Trait: description)
        avatar: Sprite key for the game UI
        deploy_as: Deploy as 'pi' or 'trainee' profile locally
        github_repo: GitHub repo for marketplace listing
        list_skills: Return skill catalog instead of creating
        list_avatars: Return avatar list instead of creating

    Returns:
        str: Generated YAML, deployment confirmation, or catalog listing
    """
    import yaml as _yaml
    from pathlib import Path

    # â”€â”€ Discovery modes â”€â”€

    if list_skills:
        lines = ["# Built-in Cursor Skills (Reference Catalog)\n"]
        lines.append(
            f"Total: {len(_KNOWN_SKILLS)} built-in skills across "
            f"{len(_SKILL_CATALOG)} domains\n"
        )
        for domain, skill_list in _SKILL_CATALOG.items():
            lines.append(f"\n## {domain}")
            for s in skill_list:
                lines.append(f"  - {s}")
        lines.append(
            "\n---\n"
            "## Custom Skills\n"
            "You are NOT limited to this catalog. You can use ANY custom skill name.\n"
            "Custom skills just need a SKILL.md file in ~/.cursor/skills/<name>/SKILL.md.\n"
            "Example: skills='scanpy,my-protein-docking-tool,lab-specific-pipeline'\n"
            "\n## Tips\n"
            "- Choose 3-6 skills per character for focus\n"
            "- Group related tools into a single skill (e.g., 'ngs-pipeline' "
            "covering pysam + deeptools + samtools)\n"
            "- PI characters should include a writing skill\n"
            "- Trainee characters should include hands-on analysis tools\n"
        )
        return "\n".join(lines)

    if list_avatars:
        lines = ["# Available Avatar Sprite Keys\n"]
        lines.append("These control the pixel-art character in the game UI:\n")
        for a in _AVATAR_KEYS:
            lines.append(f"  - {a}")
        lines.append(
            "\n---\n" "Pass your chosen avatar key:\n" "  avatar='neuroscientist'"
        )
        return "\n".join(lines)

    # â”€â”€ Validation â”€â”€

    errors = []
    if not name:
        errors.append("'name' is required (e.g., 'Dr. Maria Chen')")
    if role not in ("pi", "trainee", "collaborator"):
        errors.append("'role' must be 'pi', 'trainee', or 'collaborator'")
    if not title:
        errors.append("'title' is required (e.g., 'Computational Biology PI')")
    if not expertise:
        errors.append("'expertise' is required")
    if not goal:
        errors.append("'goal' is required")
    if not skills:
        errors.append("'skills' is required (comma-separated skill names)")
    if not personality:
        errors.append(
            "'personality' is required (pipe-separated 'Trait: description' entries)"
        )
    if avatar not in _AVATAR_KEYS:
        errors.append(
            f"'avatar' must be one of: {', '.join(_AVATAR_KEYS[:5])}... "
            "(use list_avatars=True)"
        )
    if deploy_as and deploy_as not in ("pi", "trainee"):
        errors.append("'deploy_as' must be 'pi' or 'trainee' (or empty)")

    if errors:
        return (
            "ERROR: Missing or invalid fields:\n"
            + "\n".join(f"  - {e}" for e in errors)
            + "\n\nUse list_skills=True or list_avatars=True to discover options."
        )

    # â”€â”€ Parse inputs â”€â”€

    skill_list = [s.strip() for s in skills.split(",") if s.strip()]
    personality_list = [p.strip() for p in personality.split("|") if p.strip()]

    # â”€â”€ Build character folder â”€â”€

    project_dir = Path(os.path.abspath(project_directory))
    char_slug = name.lower().replace(" ", "-").replace(".", "").replace("'", "")
    folder_name = f"autolab-char-{char_slug}"
    char_dir = project_dir / folder_name
    char_dir.mkdir(parents=True, exist_ok=True)

    # -- character.yaml --
    character_data = {
        "name": name,
        "role": role,
        "avatar": avatar,
        "title": title,
        "expertise": expertise,
        "goal": goal,
        "skills": skill_list,
        "personality": personality_list,
    }

    yaml_body = _yaml.dump(
        character_data,
        default_flow_style=False,
        allow_unicode=True,
        sort_keys=False,
    )
    char_yaml_path = char_dir / "character.yaml"
    char_yaml_path.write_text(yaml_body, encoding="utf-8")

    # -- skills/ directory with SKILL.md skeletons --
    skills_dir = char_dir / "skills"
    skills_dir.mkdir(exist_ok=True)

    for skill_name in skill_list:
        skill_folder = skills_dir / skill_name
        skill_folder.mkdir(exist_ok=True)
        skill_md_path = skill_folder / "SKILL.md"
        if not skill_md_path.exists():
            skill_md_path.write_text(
                f"---\n"
                f"name: {skill_name}\n"
                f"description: TODO â€” describe what this skill does\n"
                f"metadata:\n"
                f"  skill-author: {name}\n"
                f"---\n\n"
                f"# {skill_name.replace('-', ' ').title()}\n\n"
                f"## When to use\n\n"
                f"- TODO: describe when the AI should apply this skill\n\n"
                f"## Standard workflow\n\n"
                f"```python\n"
                f"# TODO: add code examples and instructions\n"
                f"```\n\n"
                f"## Key decisions\n\n"
                f"- TODO: document important choices and defaults\n",
                encoding="utf-8",
            )

    # -- README.md --
    skills_tree = "\n".join(
        f"    â”œâ”€â”€ {s}/\n    â”‚   â””â”€â”€ SKILL.md" for s in skill_list[:-1]
    )
    if skill_list:
        skills_tree += f"\n    â””â”€â”€ {skill_list[-1]}/\n        â””â”€â”€ SKILL.md"

    readme_content = (
        f"# {name} â€” {title}\n\n"
        f"Character for [Autonomous Lab](https://autolab.kejunying.com). "
        f"{expertise.rstrip('.')}.\n\n"
        f"## Structure\n\n"
        f"```\n"
        f"{folder_name}/\n"
        f"â”œâ”€â”€ character.yaml\n"
        f"â”œâ”€â”€ README.md\n"
        f"â””â”€â”€ skills/\n"
        f"{skills_tree}\n"
        f"```\n\n"
        f"## Install\n\n"
        f"```shell\n"
        f"git clone https://github.com/USERNAME/{folder_name} "
        f".autolab/characters/{char_slug}\n"
        f"```\n\n"
        f"## License\n\n"
        f"Apache 2.0\n"
    )
    readme_path = char_dir / "README.md"
    readme_path.write_text(readme_content, encoding="utf-8")

    # â”€â”€ Output summary â”€â”€

    output_lines = [
        f"Character folder created: {char_dir}/\n",
        f"  {folder_name}/",
        f"  â”œâ”€â”€ character.yaml",
        f"  â”œâ”€â”€ README.md",
        f"  â””â”€â”€ skills/",
    ]
    for s in skill_list:
        output_lines.append(f"      â””â”€â”€ {s}/SKILL.md")

    output_lines.append(f"\n--- character.yaml ---\n{yaml_body}--- End YAML ---\n")

    # â”€â”€ IMPORTANT: remind to fill in SKILL.md files â”€â”€
    output_lines.append(
        "NEXT STEP: Fill in each skills/<name>/SKILL.md with detailed instructions.\n"
        "Each SKILL.md should contain:\n"
        "  - When to use the skill\n"
        "  - Standard workflow with code examples\n"
        "  - Key decisions and defaults\n"
        "The AI reads these instructions when acting as this character.\n"
    )

    # â”€â”€ Deploy locally â”€â”€

    if deploy_as:
        autolab_dir = project_dir / ".autolab" / "profiles"
        autolab_dir.mkdir(parents=True, exist_ok=True)
        target = autolab_dir / f"{deploy_as}.yaml"
        target.write_text(yaml_body, encoding="utf-8")
        output_lines.append(
            f"Deployed as {deploy_as} profile: {target}\n"
            f"This character is now active. Run autolab_next to use it."
        )

    if len(skill_list) > 8:
        output_lines.append(
            "\nWARNING: More than 8 skills can dilute focus. "
            "Consider grouping related tools into fewer composite skills."
        )

    # â”€â”€ GitHub publishing â”€â”€

    if github_repo:
        clean_repo = github_repo.strip().strip("/")
        output_lines.append(
            f"\n--- GitHub Publishing ---\n"
            f"To publish this character to the marketplace, run these commands "
            f"inside the character folder ({char_dir}):\n\n"
            f"  cd {char_dir}\n"
            f"  git init\n"
            f"  git add .\n"
            f"  git commit -m 'Character: {name}'\n"
            f"  gh repo create {clean_repo} --public --source=. --push\n\n"
            f"Then submit to the marketplace:\n"
            f"  Open an issue at https://github.com/albert-ying/autonomous-lab/issues/new\n"
            f"  Title: New Character: {name}\n"
            f"  Body: https://github.com/{clean_repo}\n"
        )
    else:
        output_lines.append(
            "\nTo publish to the marketplace, call again with "
            "github_repo='username/autolab-char-name', or run:\n"
            f"  cd {char_dir} && git init && git add . && "
            f"git commit -m 'Character: {name}'\n"
            f"  gh repo create username/{folder_name} --public --source=. --push"
        )

    return "\n".join(output_lines)


# â”€â”€ Marketplace registry URL â”€â”€
_REGISTRY_URL = (
    "https://albert-ying.github.io/autonomous-lab/characters/index.json"
)


@mcp.tool()
async def autolab_acquire_skill(
    skill_name: Annotated[
        str,
        Field(
            description="The skill to acquire (e.g., 'scanpy', 'survival-analysis', "
            "'pytorch-lightning'). The tool first searches the character marketplace "
            "for an existing character that has this skill. If found, it downloads "
            "the SKILL.md. If not, it returns instructions to train the skill."
        ),
    ] = "",
    project_directory: Annotated[
        str, Field(description="Project directory path")
    ] = ".",
) -> str:
    """Search the character marketplace for a skill and acquire it.

    When the PI or Trainee encounters a skill gap during the loop, call this
    tool BEFORE attempting to build it from scratch. It checks the published
    character registry for any character that already has the needed skill.

    If a matching skill is found:
      - Downloads the SKILL.md from the character's GitHub repo
      - Saves it to .autolab/acquired_skills/<skill>/SKILL.md
      - Returns the skill content so you can use it immediately

    If no match is found:
      - Returns instructions to create a new character with the skill
      - Or to manually write the SKILL.md

    This enables in-loop skill acquisition: the research never stops because
    of a missing capability.

    Args:
        skill_name: Name of the skill to find
        project_directory: Project directory path

    Returns:
        str: The SKILL.md content if found, or instructions to create it
    """
    import json
    import urllib.parse
    import urllib.request
    from pathlib import Path

    if not skill_name:
        return "ERROR: skill_name is required (e.g., 'scanpy', 'survival-analysis')"

    skill_name = skill_name.strip().lower()
    project_dir = Path(os.path.abspath(project_directory))
    acquired_dir = project_dir / ".autolab" / "acquired_skills" / skill_name
    skill_md_path = acquired_dir / "SKILL.md"

    # Check if already acquired
    if skill_md_path.exists():
        content = skill_md_path.read_text(encoding="utf-8")
        return (
            f"Skill '{skill_name}' already acquired locally.\n\n"
            f"--- SKILL.md ---\n{content}\n--- End ---\n\n"
            f"Read and follow the instructions above."
        )

    output = [f"Searching marketplace for skill: {skill_name}...\n"]

    # Step 1: Fetch the character registry
    matches = []
    try:
        req = urllib.request.Request(_REGISTRY_URL, headers={
            "User-Agent": "AutonomousLab/0.5",
            "Accept": "application/json",
        })
        with urllib.request.urlopen(req, timeout=10) as resp:
            registry = json.loads(resp.read().decode("utf-8"))

        for char in registry.get("characters", []):
            char_skills = [s.lower() for s in char.get("skills", [])]
            # Exact match or substring match
            if skill_name in char_skills or any(
                skill_name in s or s in skill_name for s in char_skills
            ):
                matches.append(char)
    except Exception as e:
        output.append(f"Registry fetch failed ({e}), trying GitHub search...\n")

    # Step 2: Try GitHub search as fallback
    if not matches:
        try:
            query = urllib.parse.quote(
                f"autolab-char {skill_name} in:readme,name"
            )
            gh_url = (
                f"https://api.github.com/search/repositories"
                f"?q={query}+topic:autolab-character&per_page=5"
            )
            req = urllib.request.Request(gh_url, headers={
                "User-Agent": "AutonomousLab/0.5",
                "Accept": "application/vnd.github+json",
            })
            with urllib.request.urlopen(req, timeout=10) as resp:
                gh_data = json.loads(resp.read().decode("utf-8"))
            for repo in gh_data.get("items", []):
                matches.append({
                    "name": repo.get("name", ""),
                    "github": repo.get("full_name", ""),
                    "title": repo.get("description", ""),
                    "stars": repo.get("stargazers_count", 0),
                    "skills": [],  # Unknown until we fetch character.yaml
                })
        except Exception:
            pass

    # Step 3: If found, try to download the SKILL.md
    if matches:
        best = max(matches, key=lambda c: c.get("stars", 0))
        github_repo = best.get("github", "")
        output.append(
            f"Found matching character: {best.get('name', github_repo)}\n"
            f"  Repo: https://github.com/{github_repo}\n"
            f"  Stars: {best.get('stars', '?')}\n"
        )

        # Try to download the specific SKILL.md
        skill_content = None
        for try_name in [skill_name, skill_name.replace("-", "_")]:
            try:
                raw_url = (
                    f"https://raw.githubusercontent.com/{github_repo}"
                    f"/master/skills/{try_name}/SKILL.md"
                )
                req = urllib.request.Request(raw_url, headers={
                    "User-Agent": "AutonomousLab/0.5",
                })
                with urllib.request.urlopen(req, timeout=10) as resp:
                    skill_content = resp.read().decode("utf-8")
                break
            except Exception:
                # Try main branch
                try:
                    raw_url = (
                        f"https://raw.githubusercontent.com/{github_repo}"
                        f"/main/skills/{try_name}/SKILL.md"
                    )
                    req = urllib.request.Request(raw_url, headers={
                        "User-Agent": "AutonomousLab/0.5",
                    })
                    with urllib.request.urlopen(req, timeout=10) as resp:
                        skill_content = resp.read().decode("utf-8")
                    break
                except Exception:
                    continue

        if skill_content:
            # Save locally
            acquired_dir.mkdir(parents=True, exist_ok=True)
            skill_md_path.write_text(skill_content, encoding="utf-8")

            output.append(
                f"Downloaded SKILL.md â†’ {skill_md_path}\n\n"
                f"--- SKILL.md ---\n{skill_content}\n--- End ---\n\n"
                f"Skill acquired. Read and follow the instructions above."
            )
            return "\n".join(output)
        else:
            # Character found but skill folder not matching
            output.append(
                f"Character exists but skill '{skill_name}' not found as a "
                f"separate SKILL.md in that repo.\n"
                f"The character's skills: {best.get('skills', [])}\n"
                f"You can still clone the full character:\n"
                f"  git clone https://github.com/{github_repo} "
                f".autolab/characters/{best.get('id', 'char')}\n"
            )

    # Step 4: Not found â€” provide instructions to create
    if not matches:
        output.append(f"No marketplace character found with skill '{skill_name}'.\n")

    output.append(
        f"\n--- Create This Skill ---\n"
        f"Option 1: Use the Character Builder UI at /character-builder\n"
        f"Option 2: Call autolab_create_character with skills='{skill_name}'\n"
        f"Option 3: Manually create the skill:\n"
        f"  mkdir -p .autolab/acquired_skills/{skill_name}\n"
        f"  # Write a SKILL.md with: When to use, Standard workflow, Key decisions\n"
        f"\nAfter creating, the skill will be available for all future sessions."
    )
    return "\n".join(output)


# ===== Main entry point =====
def main():
    """ä¸»è¦å…¥å£é»ï¼Œç”¨æ–¼å¥—ä»¶åŸ·è¡Œ
    æ”¶é›†ç”¨æˆ¶çš„äº’å‹•å›é¥‹ï¼Œæ”¯æ´æ–‡å­—å’Œåœ–ç‰‡
    æ­¤å·¥å…·ä½¿ç”¨ Web UI ä»‹é¢æ”¶é›†ç”¨æˆ¶å›é¥‹ï¼Œæ”¯æ´æ™ºèƒ½ç’°å¢ƒæª¢æ¸¬ã€‚

    ç”¨æˆ¶å¯ä»¥ï¼š
    1. åŸ·è¡Œå‘½ä»¤ä¾†é©—è­‰çµæœ
    2. æä¾›æ–‡å­—å›é¥‹
    3. ä¸Šå‚³åœ–ç‰‡ä½œç‚ºå›é¥‹
    4. æŸ¥çœ‹ AI çš„å·¥ä½œæ‘˜è¦

    èª¿è©¦æ¨¡å¼ï¼š
    - è¨­ç½®ç’°å¢ƒè®Šæ•¸ MCP_DEBUG=true å¯å•Ÿç”¨è©³ç´°èª¿è©¦è¼¸å‡º
    - ç”Ÿç”¢ç’°å¢ƒå»ºè­°é—œé–‰èª¿è©¦æ¨¡å¼ä»¥é¿å…è¼¸å‡ºå¹²æ“¾


    """
    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨èª¿è©¦æ¨¡å¼
    debug_enabled = os.getenv("MCP_DEBUG", "").lower() in ("true", "1", "yes", "on")

    if debug_enabled:
        debug_log("ğŸš€ å•Ÿå‹•äº’å‹•å¼å›é¥‹æ”¶é›† MCP æœå‹™å™¨")
        debug_log(f"   æœå‹™å™¨åç¨±: {SERVER_NAME}")
        debug_log(f"   ç‰ˆæœ¬: {__version__}")
        debug_log(f"   å¹³å°: {sys.platform}")
        debug_log(f"   ç·¨ç¢¼åˆå§‹åŒ–: {'æˆåŠŸ' if _encoding_initialized else 'å¤±æ•—'}")
        debug_log(f"   é ç«¯ç’°å¢ƒ: {is_remote_environment()}")
        debug_log(f"   WSL ç’°å¢ƒ: {is_wsl_environment()}")
        debug_log("   Interface: Web UI")
        debug_log("   ç­‰å¾…ä¾†è‡ª AI åŠ©æ‰‹çš„èª¿ç”¨...")
        debug_log("æº–å‚™å•Ÿå‹• MCP ä¼ºæœå™¨...")
        debug_log("èª¿ç”¨ mcp.run()...")

    try:
        # ä½¿ç”¨æ­£ç¢ºçš„ FastMCP API
        mcp.run()
    except KeyboardInterrupt:
        if debug_enabled:
            debug_log("æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£å¸¸é€€å‡º")
        sys.exit(0)
    except Exception as e:
        if debug_enabled:
            debug_log(f"MCP æœå‹™å™¨å•Ÿå‹•å¤±æ•—: {e}")
            import traceback

            debug_log(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
